{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ml-git \u00b6 ml-git is a tool which provides a Distributed Version Control system to enable efficient dataset management. Like its name emphasizes, it is inspired in git concepts and workflows, ml-git enables the following operations: Manage a repository of different datasets, labels and models. Distribute these ML artifacts between members of a team or across organizations. Apply the right data governance and security models to their artifacts. How to install \u00b6 Prerequisites: Git Python 3.7+ With pip: pip install git+git://github.com/HPInc/ml-git.git Source code: Download ml-git from repository and execute commands below: Windows: cd ml-git/ python3.7 setup.py install Linux: cd ml-git/ sudo python3.7 setup.py install How to configure \u00b6 1 - As ml-git leverages git to manage ML entities metadata, it is necessary to configure user name and email address: $ git config --global user.name \"Your User\" $ git config --global user.email \"your_email@example.com\" 2 - Storage: Ml-git needs a configured storage to store data from managed artifacts. Please take a look at the ml-git architecture and internals documentation to better understand how ml-git works internally with data. To configure the storage see documentation about supported stores and how to configure each one. 3 - Ml-git project: An ml-git project is an initialized directory that will contain a configuration file to be used by ml-git in managing entities. To configure it you can use the basic steps to configure the project described in first project documentation. Usage \u00b6 $ ml-git --help Usage: ml-git [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. Commands: clone clone a ml-git repository ML_GIT_REPOSITORY_URL dataset management of datasets within this ml-git repository labels management of labels sets within this ml-git repository model management of models within this ml-git repository repository management of this ml-git repository Basic commands \u00b6 ml-git clone <repository-url> $ mkdir my-project $ cd my-project $ ml-git clone https://github.com/user/ml_git_configuration_file_example.git If you prefer not to create the directory: $ ml-git clone https://github.com/user/ml_git_configuration_file_example.git --folder=my-project If you prefer keep git tracking files in the project: $ mkdir my-project $ cd my-project $ ml-git clone https://github.com/user/ml_git_configuration_file_example.git --track ml-git <ml-entity> create This command will help you to start a new project, it creates your project artifact metadata: $ ml-git dataset create --category=computer-vision --category=images --bucket-name=your_bucket --import=../import-path --mutability=strict dataset-ex Demonstration video: ml-git <ml-entity> status Show changes in project workspace: $ ml-git dataset status dataset-ex Demonstration video: ml-git <ml-entity> add Add new files to index: $ ml-git dataset add dataset-ex To increment version: $ ml-git dataset add dataset-ex --bumpversion Add an specific file: $ ml-git dataset add dataset-ex data/file_name.ex Demonstration video: ml-git <ml-entity> commit Consolidate added files in the index to repository: $ ml-git dataset commit dataset-ex Demonstration video: ml-git <ml-entity> push Upload metadata to remote repository and send chunks to store: $ ml-git dataset push dataset-ex Demonstration video: ml-git <ml-entity> checkout Change workspace and metadata to versioned ml-entity tag: $ ml-git dataset checkout computer-vision__images__dataset-ex__1 Demonstration video: More about commands in documentation How to contribute \u00b6 Your contributions are always welcome! Clone repository and create a new branch Make changes and test Submit Pull Request with comprehensive description of changes Another way to contribute with the community is creating an issue to track your ideas, doubts, enhancements, tasks, or bugs found. If an issue with the same topic already exists, discuss on the issue. Links \u00b6 ML-Git API documentation - Find the commands that are available in our api, usage examples and more. Working with tabular data - Find suggestions on how to use ml-git with tabular data. ml-git data specialization plugins - Dynamically link third-party packages to add specialized behaviors for the data type.","title":"Introduction"},{"location":"#ml-git","text":"ml-git is a tool which provides a Distributed Version Control system to enable efficient dataset management. Like its name emphasizes, it is inspired in git concepts and workflows, ml-git enables the following operations: Manage a repository of different datasets, labels and models. Distribute these ML artifacts between members of a team or across organizations. Apply the right data governance and security models to their artifacts.","title":"ml-git"},{"location":"#how-to-install","text":"Prerequisites: Git Python 3.7+ With pip: pip install git+git://github.com/HPInc/ml-git.git Source code: Download ml-git from repository and execute commands below: Windows: cd ml-git/ python3.7 setup.py install Linux: cd ml-git/ sudo python3.7 setup.py install","title":"How to install"},{"location":"#how-to-configure","text":"1 - As ml-git leverages git to manage ML entities metadata, it is necessary to configure user name and email address: $ git config --global user.name \"Your User\" $ git config --global user.email \"your_email@example.com\" 2 - Storage: Ml-git needs a configured storage to store data from managed artifacts. Please take a look at the ml-git architecture and internals documentation to better understand how ml-git works internally with data. To configure the storage see documentation about supported stores and how to configure each one. 3 - Ml-git project: An ml-git project is an initialized directory that will contain a configuration file to be used by ml-git in managing entities. To configure it you can use the basic steps to configure the project described in first project documentation.","title":"How to configure"},{"location":"#usage","text":"$ ml-git --help Usage: ml-git [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. Commands: clone clone a ml-git repository ML_GIT_REPOSITORY_URL dataset management of datasets within this ml-git repository labels management of labels sets within this ml-git repository model management of models within this ml-git repository repository management of this ml-git repository","title":"Usage"},{"location":"#basic-commands","text":"ml-git clone <repository-url> $ mkdir my-project $ cd my-project $ ml-git clone https://github.com/user/ml_git_configuration_file_example.git If you prefer not to create the directory: $ ml-git clone https://github.com/user/ml_git_configuration_file_example.git --folder=my-project If you prefer keep git tracking files in the project: $ mkdir my-project $ cd my-project $ ml-git clone https://github.com/user/ml_git_configuration_file_example.git --track ml-git <ml-entity> create This command will help you to start a new project, it creates your project artifact metadata: $ ml-git dataset create --category=computer-vision --category=images --bucket-name=your_bucket --import=../import-path --mutability=strict dataset-ex Demonstration video: ml-git <ml-entity> status Show changes in project workspace: $ ml-git dataset status dataset-ex Demonstration video: ml-git <ml-entity> add Add new files to index: $ ml-git dataset add dataset-ex To increment version: $ ml-git dataset add dataset-ex --bumpversion Add an specific file: $ ml-git dataset add dataset-ex data/file_name.ex Demonstration video: ml-git <ml-entity> commit Consolidate added files in the index to repository: $ ml-git dataset commit dataset-ex Demonstration video: ml-git <ml-entity> push Upload metadata to remote repository and send chunks to store: $ ml-git dataset push dataset-ex Demonstration video: ml-git <ml-entity> checkout Change workspace and metadata to versioned ml-entity tag: $ ml-git dataset checkout computer-vision__images__dataset-ex__1 Demonstration video: More about commands in documentation","title":"Basic commands"},{"location":"#how-to-contribute","text":"Your contributions are always welcome! Clone repository and create a new branch Make changes and test Submit Pull Request with comprehensive description of changes Another way to contribute with the community is creating an issue to track your ideas, doubts, enhancements, tasks, or bugs found. If an issue with the same topic already exists, discuss on the issue.","title":"How to contribute"},{"location":"#links","text":"ML-Git API documentation - Find the commands that are available in our api, usage examples and more. Working with tabular data - Find suggestions on how to use ml-git with tabular data. ml-git data specialization plugins - Dynamically link third-party packages to add specialized behaviors for the data type.","title":"Links"},{"location":"azure_configurations/","text":"Azure container configuration \u00b6 Ml-git allows the user to choose to have their data stored in an Azure Blob Storage that provides massively scalable storage for unstructured data like images, videos, or documents. This section explains how to configure the settings that ml-git uses to interact with your Azure container. To establish the connection between ml-git and Azure services you will need a connection string which can be found on the Azure portal. See the image below: With this connection string in hand, you can configure your environment in two ways (this order is the one used by ml-git to get your credentials): Environment Variable Azure CLI 1. Environment Variable \u00b6 You can add the connection string to your system's set of variables. Ml-git will look for the variable AZURE_STORAGE_CONNECTION_STRING . To add the system variable, run the following command: Windows : setx AZURE_STORAGE_CONNECTION_STRING \"<yourconnectionstring>\" Linux or macOS : export AZURE_STORAGE_CONNECTION_STRING=\"<yourconnectionstring>\" 2. Azure CLI \u00b6 The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources. The Azure CLI is available across Azure services and is designed to get you working quickly with Azure, with an emphasis on automation. Azure CLI uses a file to store the configurations that are used by its services. To add settings to the file, simply run the following command: $ az configure If tou prefer, you can create a configuration file through the console. The configuration file itself is located at $AZURE_CONFIG_DIR/config . The default value of AZURE_CONFIG_DIR is $HOME/.azure on Linux and macOS, and %USERPROFILE%\\.azure on Windows. From the home directory (UserProfile) execute: $ mkdir .azure You need to create the config file with the connection string value: $ echo \" [storage] connection_string = \"<yourconnectionstring>\" \" > .azure/config","title":"Azure"},{"location":"azure_configurations/#azure-container-configuration","text":"Ml-git allows the user to choose to have their data stored in an Azure Blob Storage that provides massively scalable storage for unstructured data like images, videos, or documents. This section explains how to configure the settings that ml-git uses to interact with your Azure container. To establish the connection between ml-git and Azure services you will need a connection string which can be found on the Azure portal. See the image below: With this connection string in hand, you can configure your environment in two ways (this order is the one used by ml-git to get your credentials): Environment Variable Azure CLI","title":"Azure container configuration"},{"location":"azure_configurations/#1-environment-variable","text":"You can add the connection string to your system's set of variables. Ml-git will look for the variable AZURE_STORAGE_CONNECTION_STRING . To add the system variable, run the following command: Windows : setx AZURE_STORAGE_CONNECTION_STRING \"<yourconnectionstring>\" Linux or macOS : export AZURE_STORAGE_CONNECTION_STRING=\"<yourconnectionstring>\"","title":" 1. Environment Variable "},{"location":"azure_configurations/#2-azure-cli","text":"The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources. The Azure CLI is available across Azure services and is designed to get you working quickly with Azure, with an emphasis on automation. Azure CLI uses a file to store the configurations that are used by its services. To add settings to the file, simply run the following command: $ az configure If tou prefer, you can create a configuration file through the console. The configuration file itself is located at $AZURE_CONFIG_DIR/config . The default value of AZURE_CONFIG_DIR is $HOME/.azure on Linux and macOS, and %USERPROFILE%\\.azure on Windows. From the home directory (UserProfile) execute: $ mkdir .azure You need to create the config file with the connection string value: $ echo \" [storage] connection_string = \"<yourconnectionstring>\" \" > .azure/config","title":" 2. Azure CLI "},{"location":"centralized_cache_and_objects/","text":"Centralized cache \u00b6 Centralized cache is a configuration mode that allows cached files to be shared between multiple users on the same machine, reducing the total cost of disk space. Currently, this feature works only in Linux and derivative machines. :warning: Caution: We encourage the use of centralized cache just with mutability set as strict . It is necessary to deactivate the feature fs.protected_hardlinks , because ML-Git uses hardlink to share cache files. Be aware that changing this setting is a risky operation, as malicious people can exploit this (see the extract below). Do this only if you really need to use the Centralized Cache feature. Remember to revert this change if you will stop to use Centralized Cache. Please read this extract from kernel.org about protected_hardlinks setting: protected_hardlinks: A long-standing class of security issues is the hardlink-based time-of-check-time-of-use race, most commonly seen in world-writable directories like /tmp. The common method of exploitation of this flaw is to cross privilege boundaries when following a given hardlink (i.e. a root process follows a hardlink created by another user). Additionally, on systems without separated partitions, this stops unauthorized users from \"pinning\" vulnerable setuid/setgid files against being upgraded by the administrator, or linking to special files. When set to \"0\", hardlink creation behavior is unrestricted. When set to \"1\" hardlinks cannot be created by users if they do not already own the source file, or do not have read/write access to it. This protection is based on the restrictions in Openwall and grsecurity. Changing fs.protected_hardlinks: Execute in terminal: sudo gedit /etc/sysctl.conf Search for: #fs.protected_hardlinks = 0 and uncomment (remove \u2018#\u2019). If you didn't find it, add a line with #fs.protected_hardlinks = 0 to this file Then execute: sudo sysctl -p Requirements: \u00b6 Machine's root user (administrator). Configurations steps: \u00b6 1 - Create a common directory for each entity with read and write permission for all users: sudo mkdir -p /srv/mlgit/cache/dataset sudo mkdir -p /srv/mlgit/cache/labels sudo mkdir -p /srv/mlgit/cache/model Change permissions: sudo chmod -R a+rwX /srv/mlgit/cache/dataset sudo chmod -R a+rwX /srv/mlgit/cache/labels sudo chmod -R a+rwX /srv/mlgit/cache/model 2 - With the project ml-git initialized change .ml-git/config.yaml : dataset: git: '' cache_path: 'Cache path directory created on step 1 for dataset entity' labels: git: '' cache_path: 'Path directory created on step 1 for labels entity' model: git: '' cache_path: 'Path directory created on step 1 for model entity' store: s3: mlgit-datasets: aws-credentials: profile: default region: us-east-1 Centralized objects \u00b6 Centralized objects is a configuration that allow to user share ml-git\u2019s data between machine\u2019s users, avoiding downloading times. Requirements: \u00b6 Machine's root user (administrator). Configurations steps: \u00b6 1 - Create a common directory for each entity with read and write permission for all users: Windows: \u00b6 mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\dataset mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\labels mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\model or mkgit \\a C:\\ProgramData\\mlgit\\objects\\dataset mkgit \\a C:\\ProgramData\\mlgit\\objects\\labels mkgit \\a C:\\ProgramData\\mlgit\\objects\\model Linux and derivatives: \u00b6 sudo mkdir -p /srv/mlgit/objects/dataset sudo mkdir -p /srv/mlgit/objects/labels sudo mkdir -p /srv/mlgit/objects/model Change permissions: sudo chmod -R a+rwX /srv/mlgit/objects/dataset sudo chmod -R a+rwX /srv/mlgit/objects/labels sudo chmod -R a+rwX /srv/mlgit/objects/model 2 - With the project ml-git initialized change .ml-git/config.yaml : dataset: git: '' objects_path: 'Path directory created on step 1 for dataset entity' labels: git: '' objects_path: 'Path directory created on step 1 for labels entity' model: git: '' objects_path: 'Path directory created on step 1 for model entity' store: s3: mlgit-datasets: aws-credentials: profile: default region: us-east-1","title":"Cache and Objects"},{"location":"centralized_cache_and_objects/#centralized-cache","text":"Centralized cache is a configuration mode that allows cached files to be shared between multiple users on the same machine, reducing the total cost of disk space. Currently, this feature works only in Linux and derivative machines. :warning: Caution: We encourage the use of centralized cache just with mutability set as strict . It is necessary to deactivate the feature fs.protected_hardlinks , because ML-Git uses hardlink to share cache files. Be aware that changing this setting is a risky operation, as malicious people can exploit this (see the extract below). Do this only if you really need to use the Centralized Cache feature. Remember to revert this change if you will stop to use Centralized Cache. Please read this extract from kernel.org about protected_hardlinks setting: protected_hardlinks: A long-standing class of security issues is the hardlink-based time-of-check-time-of-use race, most commonly seen in world-writable directories like /tmp. The common method of exploitation of this flaw is to cross privilege boundaries when following a given hardlink (i.e. a root process follows a hardlink created by another user). Additionally, on systems without separated partitions, this stops unauthorized users from \"pinning\" vulnerable setuid/setgid files against being upgraded by the administrator, or linking to special files. When set to \"0\", hardlink creation behavior is unrestricted. When set to \"1\" hardlinks cannot be created by users if they do not already own the source file, or do not have read/write access to it. This protection is based on the restrictions in Openwall and grsecurity. Changing fs.protected_hardlinks: Execute in terminal: sudo gedit /etc/sysctl.conf Search for: #fs.protected_hardlinks = 0 and uncomment (remove \u2018#\u2019). If you didn't find it, add a line with #fs.protected_hardlinks = 0 to this file Then execute: sudo sysctl -p","title":"Centralized cache"},{"location":"centralized_cache_and_objects/#requirements","text":"Machine's root user (administrator).","title":"Requirements:"},{"location":"centralized_cache_and_objects/#configurations-steps","text":"1 - Create a common directory for each entity with read and write permission for all users: sudo mkdir -p /srv/mlgit/cache/dataset sudo mkdir -p /srv/mlgit/cache/labels sudo mkdir -p /srv/mlgit/cache/model Change permissions: sudo chmod -R a+rwX /srv/mlgit/cache/dataset sudo chmod -R a+rwX /srv/mlgit/cache/labels sudo chmod -R a+rwX /srv/mlgit/cache/model 2 - With the project ml-git initialized change .ml-git/config.yaml : dataset: git: '' cache_path: 'Cache path directory created on step 1 for dataset entity' labels: git: '' cache_path: 'Path directory created on step 1 for labels entity' model: git: '' cache_path: 'Path directory created on step 1 for model entity' store: s3: mlgit-datasets: aws-credentials: profile: default region: us-east-1","title":"Configurations steps:"},{"location":"centralized_cache_and_objects/#centralized-objects","text":"Centralized objects is a configuration that allow to user share ml-git\u2019s data between machine\u2019s users, avoiding downloading times.","title":"Centralized objects"},{"location":"centralized_cache_and_objects/#requirements_1","text":"Machine's root user (administrator).","title":"Requirements:"},{"location":"centralized_cache_and_objects/#configurations-steps_1","text":"1 - Create a common directory for each entity with read and write permission for all users:","title":"Configurations steps:"},{"location":"centralized_cache_and_objects/#windows","text":"mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\dataset mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\labels mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\model or mkgit \\a C:\\ProgramData\\mlgit\\objects\\dataset mkgit \\a C:\\ProgramData\\mlgit\\objects\\labels mkgit \\a C:\\ProgramData\\mlgit\\objects\\model","title":"Windows:"},{"location":"centralized_cache_and_objects/#linux-and-derivatives","text":"sudo mkdir -p /srv/mlgit/objects/dataset sudo mkdir -p /srv/mlgit/objects/labels sudo mkdir -p /srv/mlgit/objects/model Change permissions: sudo chmod -R a+rwX /srv/mlgit/objects/dataset sudo chmod -R a+rwX /srv/mlgit/objects/labels sudo chmod -R a+rwX /srv/mlgit/objects/model 2 - With the project ml-git initialized change .ml-git/config.yaml : dataset: git: '' objects_path: 'Path directory created on step 1 for dataset entity' labels: git: '' objects_path: 'Path directory created on step 1 for labels entity' model: git: '' objects_path: 'Path directory created on step 1 for model entity' store: s3: mlgit-datasets: aws-credentials: profile: default region: us-east-1","title":"Linux and derivatives:"},{"location":"developer_info/","text":"Info for ML-Git Developers \u00b6 Requirements : Python 3.7 Pipenv Git Docker (required only for Integration Tests execution) Setting tests environment \u00b6 Install Docker: Windows Linux The Integration Tests script starts a MinIO container on your local machine (port 9000) to be used as store during tests execution. [Optional] Install and configure Make to run tests easily: Windows Linux Configure git: git config --global user.name \"First Name and Last Name\" git config --global user.email \"your_name@example.com\" Unit Tests \u00b6 Running unit tests \u00b6 Using Make : \u00b6 Execute on terminal: cd ml-git make test.unit Without Make : \u00b6 Linux: Execute on terminal: cd ml-git sh ./scripts/run_unit_tests.sh Windows: Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_unit_tests . bat Integration Tests \u00b6 Running Integration Tests \u00b6 Using Make : \u00b6 Execute on terminal: cd ml-git make test.integration Without Make: \u00b6 Linux: Execute on terminal: cd ml-git sh ./scripts/run_integration_tests.sh Windows: Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_integration_tests . bat Google Drive Integration test: \u00b6 To run google drive integration test you need to: 1. Create directory tests/integration/credentials-json Put your credentials file with name credentials.json in the folder you created in step 1 Example of credentials.json: {\"installed\":{\"client_id\":\"fake_client_id \",\"project_id\":\"project\",\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"fake_client_secret \",\"redirect_uris\":[\"urn:ietf:wg:oauth:2.0:oob\",\"http://localhost\"]}} Create a folder with name mlgit/test-folder in your GDrive Create files mlgit/B and mlgit/test-folder/A with any content, make sure that files aren't Google Files. You should have the following structure in your drive: YourDrive | \u251c\u2500\u2500 mlgit \u2502 \u251c\u2500\u2500 B \u2502 \u2514\u2500\u2500 test-folder \u2502 \u2514\u2500\u2500 A Create tests/integration/gdrive-files-links.json with shared links of mlgit/B and mlgit/test-folder . Example of gdrive-files-links.json: { \"test-folder\": \"https://drive.google.com/drive/folders/1MvWrQtPVDuJ5-XB82dMwRI8XflBZ?usp=sharing\", \"B\": \"https://drive.google.com/file/d/1uy6Kao8byRqTPv-Plw8tuhITyh5N1Uua/view?usp=sharing\" } The Google Drive Integration Tests are set to not run by default (as they require extra setup, as mentioned earlier). To include the integration tests for Google Drive store during an integration tests run, you should execute: Using Make : \u00b6 Execute on terminal: cd ml-git make test.integration.gdrive Without Make: \u00b6 Linux: Execute on terminal: cd ml-git sh ./scripts/run_integration_tests.sh --gdrive Windows: Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_integration_tests . bat - -gdrive Executing a single test file: \u00b6 To execute a specific integration tests file, execute the run_integration_tests script accordingly with your operating system and pass the test file path relative to integration tests folder (tests/integration/). See the below examples running test_01_init.py located at ml-git/tests/integration/test_01_init.py : Linux: cd ml-git sh ./scripts/run_integration_tests.sh test_01_init.py Windows: cd ml-git .\\ scripts \\ run_integration_tests . bat test_01_init . py","title":"Info for ML-Git Developers"},{"location":"developer_info/#info-for-ml-git-developers","text":"Requirements : Python 3.7 Pipenv Git Docker (required only for Integration Tests execution)","title":"Info for ML-Git Developers"},{"location":"developer_info/#setting-tests-environment","text":"Install Docker: Windows Linux The Integration Tests script starts a MinIO container on your local machine (port 9000) to be used as store during tests execution. [Optional] Install and configure Make to run tests easily: Windows Linux Configure git: git config --global user.name \"First Name and Last Name\" git config --global user.email \"your_name@example.com\"","title":"Setting tests environment"},{"location":"developer_info/#unit-tests","text":"","title":"Unit Tests"},{"location":"developer_info/#running-unit-tests","text":"","title":"Running unit tests"},{"location":"developer_info/#using-make","text":"Execute on terminal: cd ml-git make test.unit","title":"Using Make:"},{"location":"developer_info/#without-make","text":"Linux: Execute on terminal: cd ml-git sh ./scripts/run_unit_tests.sh Windows: Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_unit_tests . bat","title":"Without Make:"},{"location":"developer_info/#integration-tests","text":"","title":"Integration Tests"},{"location":"developer_info/#running-integration-tests","text":"","title":"Running Integration Tests"},{"location":"developer_info/#using-make_1","text":"Execute on terminal: cd ml-git make test.integration","title":"Using Make:"},{"location":"developer_info/#without-make_1","text":"Linux: Execute on terminal: cd ml-git sh ./scripts/run_integration_tests.sh Windows: Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_integration_tests . bat","title":"Without Make:"},{"location":"developer_info/#google-drive-integration-test","text":"To run google drive integration test you need to: 1. Create directory tests/integration/credentials-json Put your credentials file with name credentials.json in the folder you created in step 1 Example of credentials.json: {\"installed\":{\"client_id\":\"fake_client_id \",\"project_id\":\"project\",\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"fake_client_secret \",\"redirect_uris\":[\"urn:ietf:wg:oauth:2.0:oob\",\"http://localhost\"]}} Create a folder with name mlgit/test-folder in your GDrive Create files mlgit/B and mlgit/test-folder/A with any content, make sure that files aren't Google Files. You should have the following structure in your drive: YourDrive | \u251c\u2500\u2500 mlgit \u2502 \u251c\u2500\u2500 B \u2502 \u2514\u2500\u2500 test-folder \u2502 \u2514\u2500\u2500 A Create tests/integration/gdrive-files-links.json with shared links of mlgit/B and mlgit/test-folder . Example of gdrive-files-links.json: { \"test-folder\": \"https://drive.google.com/drive/folders/1MvWrQtPVDuJ5-XB82dMwRI8XflBZ?usp=sharing\", \"B\": \"https://drive.google.com/file/d/1uy6Kao8byRqTPv-Plw8tuhITyh5N1Uua/view?usp=sharing\" } The Google Drive Integration Tests are set to not run by default (as they require extra setup, as mentioned earlier). To include the integration tests for Google Drive store during an integration tests run, you should execute:","title":"Google Drive Integration test:"},{"location":"developer_info/#using-make_2","text":"Execute on terminal: cd ml-git make test.integration.gdrive","title":"Using Make:"},{"location":"developer_info/#without-make_2","text":"Linux: Execute on terminal: cd ml-git sh ./scripts/run_integration_tests.sh --gdrive Windows: Execute on Powershell or CMD: cd ml-git .\\ scripts \\ run_integration_tests . bat - -gdrive","title":"Without Make:"},{"location":"developer_info/#executing-a-single-test-file","text":"To execute a specific integration tests file, execute the run_integration_tests script accordingly with your operating system and pass the test file path relative to integration tests folder (tests/integration/). See the below examples running test_01_init.py located at ml-git/tests/integration/test_01_init.py : Linux: cd ml-git sh ./scripts/run_integration_tests.sh test_01_init.py Windows: cd ml-git .\\ scripts \\ run_integration_tests . bat test_01_init . py","title":"Executing a single test file:"},{"location":"downloadable_environment/","text":"Downloadable environment \u00b6 About \u00b6 This image enables new users to get started with ml-git in a lightweight Linux-based image without worrying about configurations. The image also include a git repository with a predefined dataset and a minio instance populated with the dataset's data. How to use: \u00b6 Ensure that you have Docker installed. Inside root of ml-git directory build the image locally with the following command: make docker.build or docker build -t mlgit_docker_env -f docker/Dockerfile . Run the Docker container to launch the built image: make docker.run or docker run -it -p 8888:8888 --name mlgit_env mlgit_docker_env Port 8888 will be used to start the jupyter notebook web service. Using the ml-git with environment (inside docker container): \u00b6 The container has a ml-git project initialized inside directory workspace, the content of versioned tag is an image from mnist database . You can execute the command checkout directly to tag: ml-git dataset checkout handwritten__digits__mnist__1 Summary of files in image: \u00b6 local_server.git (local git repository, used to store metadafiles). data (directory used by the bucket to store project data). init.sh (script that run basic command to use ml-git). minio (minio executable). local_ml_git_config_server.git (local git repositoy with configuration files, used by ml-git clone). ml-git (source code of ml-git). workspace (initialized ml-git project).","title":"Docker Environment"},{"location":"downloadable_environment/#downloadable-environment","text":"","title":"Downloadable environment"},{"location":"downloadable_environment/#about","text":"This image enables new users to get started with ml-git in a lightweight Linux-based image without worrying about configurations. The image also include a git repository with a predefined dataset and a minio instance populated with the dataset's data.","title":"About"},{"location":"downloadable_environment/#how-to-use","text":"Ensure that you have Docker installed. Inside root of ml-git directory build the image locally with the following command: make docker.build or docker build -t mlgit_docker_env -f docker/Dockerfile . Run the Docker container to launch the built image: make docker.run or docker run -it -p 8888:8888 --name mlgit_env mlgit_docker_env Port 8888 will be used to start the jupyter notebook web service.","title":"How to use:"},{"location":"downloadable_environment/#using-the-ml-git-with-environment-inside-docker-container","text":"The container has a ml-git project initialized inside directory workspace, the content of versioned tag is an image from mnist database . You can execute the command checkout directly to tag: ml-git dataset checkout handwritten__digits__mnist__1","title":"Using the ml-git with environment (inside docker container):"},{"location":"downloadable_environment/#summary-of-files-in-image","text":"local_server.git (local git repository, used to store metadafiles). data (directory used by the bucket to store project data). init.sh (script that run basic command to use ml-git). minio (minio executable). local_ml_git_config_server.git (local git repositoy with configuration files, used by ml-git clone). ml-git (source code of ml-git). workspace (initialized ml-git project).","title":"Summary of files in image:"},{"location":"first_project/","text":"Your 1st ML artefacts under ML-Git management \u00b6 We will divide this quick howto into 6 main sections: ML-Git repository configuration / intialization This section explains how to initialize and configure a repository for ML-Git, considering the scenarios of the store be an S3 or a MinIO. Uploading a dataset Having a repository initialized, this section explains how to create and upload a dataset to the store. Adding data to a dataset This section explains how to add new data to an entity already versioned by ML-Git. Uploading labels associated to a dataset This section describes how to upload a set of labels by associating the dataset to which these labels refer. Downloading a dataset This section describes how to download a versioned data set using ML-Git. Checking data integrity This section explains how to check the integrity of the metadata repository. At the end of each section there is a video to demonstrate the ML-Git usage. Initial configuration of ML-Git \u00b6 Make sure you have created your own git repository (more information) for dataset metadata and a S3 bucket or a MinIO server for the dataset actual data. If you haven't created it yet, you can use the resources initialization script which aims to facilitate the creation of resources (buckets and repositories). After that, create a ML-Git project. To do this, use the following commands (note that 'mlgit-project' is the project name used as example): $ mkdir mlgit-project && cd mlgit-project (or clone an existing repo from Github or Github Enterprise) $ ml-git repository init Now, we need to configure our project with the remote configurations. This section is divided into two parts according to the storage: Setting up a ml-git project with S3 and Setting up a ml-git project with MinIO . After configuring the project with the bucket, the remote ones, the credentials that will be used, and the other configurations that were performed in this section, a good practice is to make the version of the .ml-git folder that was generated in a git repository. That way in future projects or if you want to share with someone you can use the command ml-git clone to import the project's settings, without having to configure it for each new project. Setting up a ML-Git project with S3 \u00b6 In addition to creating the bucket in S3, it is necessary to configure the settings that the ML-Git uses to interact with your bucket, see how to configure a S3 bucket for more details. For a basic ML-Git repository, you need to add a remote repository for metadata and a S3 bucket configuration. $ ml-git repository remote dataset add git@github.com:example/your-mlgit-datasets.git $ ml-git repository storage add mlgit-datasets --credentials=mlgit Last but not least, initialize the metadata repository. $ ml-git dataset init Setting up a ML-Git project with MinIO \u00b6 Same as for S3, in addition to creating the MinIO server, it is necessary to configure the settings that the ML-Git uses to interact with your bucket, see how to configure a MinIO for this. For a basic ML-Git repository, you need to add a remote repository for metadata and the MinIO bucket configuration. $ ml-git dataset remote add git@github.com:example/your-mlgit-datasets.git $ ml-git storage add mlgit-datasets --credentials=mlgit --endpoint-url=<minio-endpoint-url> After that initialize the metadata repository. $ ml-git dataset init Setting up ML-Git project with MinIO: Why ML-Git uses git? \u00b6 The ML-Git uses git to versioning project's metadata. See below versioned metadata: .spec , is the specification file that contains informations like version number, artefact name, entity type (dataset, label, model), categories (tree struct that caracterize an entity). MANIFEST.yaml , is responsible to map artefact's files. The files are mapped by hashes, that are the references used to perform operations in local files, and download/upload operations in Stores (AWS|MinIO). You can find more information about metadata here . All configurations are stored in .ml-git/config.yaml and you can look at configuration state at any time with the following command: $ ml-git repository config config: {'batch_size': 20, 'cache_path': '', 'dataset': {'git': 'git@github.com:example/your-mlgit-datasets.git'}, 'index_path': '', 'labels': {'git': ''}, 'metadata_path': '', 'mlgit_conf': 'config.yaml', 'mlgit_path': '.ml-git', 'model': {'git': ''}, 'object_path': '', 'push_threads_count': 10, 'refs_path': '', 'store': {'s3': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'}, 'region': 'us-east-1'}}, 's3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'mlgit'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 'verbose': 'info'} Uploading a dataset \u00b6 To create and upload a dataset to a storage, you must be in an already initialized project, if necessary read section 1 to initialize and configure a project. ML-Git expects any dataset to be specified under dataset/ directory of your project and it expects a specification file with the name of the dataset. To create this specification file for a new entity you must run the following command: $ ml-git dataset create imagenet8 --category=computer-vision --category=images --mutability=strict --storage-type=s3h --bucket-name=mlgit-datasets --version=1 After that a file must have been created in dataset/imagenet8/imagenet8.spec and should look like this: dataset: categories: - computer-vision - images manifest: store: s3h://mlgit-datasets mutability: strict name: imagenet8 version: 1 There are 5 main items in the spec file: name : it's the dataset name version : the version should be a positive integer, incremented each time a new version is pushed into ML-Git. You can use the --bumpversion as an argument to do the automatic increment when you add more files to a dataset. categories : describes a tree structure to characterize the dataset category. This information is used by ML-Git to create a directory structure in the git repository managing the metadata. manifest : describes the data storage in which the data is actually stored. In the above example, a S3 bucket named mlgit-datasets . The AWS credential profile name and AWS region should be found in the ML-Git config file. mutability : describes the mutability option that your project has. The mutability options are \"strict\", \"flexible\" and \"mutable\", after selecting one of these options, you cannot change that. If you want to know more about each type of mutability and how it works, please take a look at mutability documentation . The items listed above are mandatory in the spec. An important point to note is if the user wishes, it is possible to add new items that will be versioned with the spec. The example below presents a spec with the entity's owner information to be versioned. Those information were put under metadata field just for purpose of organization. dataset: categories: - computer-vision - images mutability: strict manifest: store: s3h://mlgit-datasets name: imagenet8 version: 1 metadata: owner: name: <your-name-here> email: <your-email-here> After creating the dataset spec file, you can create a README.md to create a web page describing your dataset, adding references and any other useful information. Then, you can put the data of that dataset under the directory. Below, you will see the tree of imagenet8 directory and file structure: imagenet8/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_1 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_2 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_3 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_4 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_5 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_6 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_7 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_8 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_9 \u2502 \u2502 \u2514\u2500\u2500 train_data_batch_10 \u2502 \u2514\u2500\u2500 val \u2502 \u2514\u2500\u2500 val_data \u2514\u2500\u2500 imagenet8.spec You can look at the working tree status with the following command: $ ml-git dataset status imagenet8 INFO - Repository: dataset: status of ml-git index for [imagenet8] Changes to be committed untracked files imagenet8.spec README.md data\\train\\train_data_batch_1 data\\train\\train_data_batch_2 data\\train\\train_data_batch_3 data\\train\\train_data_batch_4 data\\train\\train_data_batch_5 data\\train\\train_data_batch_6 data\\train\\train_data_batch_7 data\\train\\train_data_batch_8 data\\train\\train_data_batch_9 data\\train\\train_data_batch_10 data\\val\\val_data corrupted files That command allows printing the tracked files and the ones in the index/staging area. Now, you are ready to put that new dataset under ML-Git management. For this, do: $ ml-git dataset add imagenet8 The command \" ml-git dataset add \" adds the files into a specific dataset, such as imagenet8 in the index/staging area. If you check the working tree status, you can see that now the files appear as tracked but not committed yet: $ ml-git dataset status imagenet8 INFO - Repository: dataset: status of ml-git index for [imagenet8] Changes to be committed new file: data\\train\\train_data_batch_1 new file: data\\train\\train_data_batch_2 new file: data\\train\\train_data_batch_3 new file: data\\train\\train_data_batch_4 new file: data\\train\\train_data_batch_5 new file: data\\train\\train_data_batch_6 new file: data\\train\\train_data_batch_7 new file: data\\train\\train_data_batch_8 new file: data\\train\\train_data_batch_9 new file: data\\train\\train_data_batch_10 new file: data\\val\\val_data untracked files corrupted files Then, you can commit the metadata to the local repository. For this purpose, type the following command: $ ml-git dataset commit imagenet8 After that, you can use \" ml-git dataset push \" to update the remote metadata repository just after storing all actual data under management in the specified remote data store. $ ml-git dataset push imagenet8 As you can observe, ML-Git follows very similar workflows as git. Uploading a dataset: Adding data to a dataset \u00b6 If you want to add data to a dataset, perform the following steps: In your workspace, copy the new data in under dataset/<your-dataset>/data Modify the version number. To do this step you have two ways: You can put the option --bumpversion on the add command to auto increment the version number, as shown below. Or, you can put the option --version on the commit command to set an specific version number. After that, like in the previous section, you need to execute the following commands to upload the new data: ml-git dataset add <your-dataset> --bumpversion ml-git dataset commit <your-dataset> ml-git dataset push <your-dataset> This will create a new version of your dataset and push the changes to your remote storage (e.g. S3). Adding data to a dataset: Uploading labels associated to a dataset \u00b6 To create and upload labels associated to a dataset, you must be in an already initialized project, if necessary read section 1 to initialize and configure the project. Also, you will need to have a dataset already versioned by ML-Git in your repository, see section 2 . The first step is to configure your metadata and data repository/storage. $ ml-git repository remote labels add git@github.com:example/your-mlgit-labels.git $ ml-git repository storage add mlgit-labels $ ml-git labels init Even these commands show a different bucket to store the labels data. It would be possible to store both datasets and labels into the same bucket. If you look at your config file, you would see the following information: $ ml-git repository config config: {'batch_size': 20, 'cache_path': '', 'dataset': {'git': 'git@github.com:example/your-mlgit-datasets.git'}, 'index_path': '', 'labels': {'git': 'git@github.com:example/your-mlgit-labels.git'}, 'metadata_path': '', 'mlgit_conf': 'config.yaml', 'mlgit_path': '.ml-git', 'model': {'git': ''}, 'object_path': '', 'refs_path': '', 'store': {'s3': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'}, 'region': 'us-east-1'}}, 's3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': None, 'region': 'us-east-1'}}}, 's3h': {'mlgit-labels': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': None, 'region': 'us-east-1'}}}, 'verbose': 'info'} Then, you can create your first set of labels. As an example, we will use mscoco. ML-Git expects any set of labels to be specified under the labels/ directory of your project. Also, it expects a specification file with the name of the labels . $ ml-git labels create mscoco-captions --category=computer-vision --category=captions --mutability=mutable --store-type=s3h --bucket-name=mlgit-labels --version=1 After create the entity, you can create the README.md describing your set of labels. Below is the tree of caption labels for the mscoco directory and file structure: mscoco-captions/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 captions_train2014.json \u2502 \u2514\u2500\u2500 captions_val2014.json \u2514\u2500\u2500 mscoco-captions.spec Now, you are ready to put the new set of labels under ML-Git management. We assume there is an existing mscoco dataset. For this, do: $ ml-git labels add mscoco-captions $ ml-git labels commit mscoco-captions --dataset=mscoco $ ml-git labels push mscoco-captions The commands are very similar to dataset operations. However, you can note one particular change in the commit command. There is an option \" --dataset \" which is used to tell ML-Git that the labels should be linked to the specified dataset. Internally, ML-Git will look at the checked out dataset in your workspace for that specified dataset. Then, it will include the git tag and sha into the specification file to be committed into the metadata repository. Once done, anyone will be able to retrieve the exact same version of the dataset that has been used for that specific set of labels. One can look at the specific dataset associated with that set of labels by executing the following command: $ ml-git labels show mscoco-captions -- labels : mscoco-captions -- categories: - computer-vision - captions dataset: sha: 607fa818da716c3313a6855eb3bbd4587e412816 tag: computer-vision__images__mscoco__1 manifest: files: MANIFEST.yaml store: s3h://mlgit-datasets name: mscoco-captions version: 1 As you can see, there is a new section \" dataset \" that has been added by ML-Git with the sha & tag fields. It can be used to checkout the exact version of the dataset for that set of labels. Uploading labels related to a dataset: Downloading a dataset \u00b6 We assume there is an existing ML-Git repository with a few ML datasets under its management and you'd like to download one of the available datasets. If you don't have a dataset versioned by the ML-Git, see section 2 on how to do this. To download a dataset, you need to be in an initialized and configured ML-Git project. If you have a repository with your saved settings, you can run the following command to set up your environment: $ ml-git clone git@github.com:example/your-mlgit-repository.git If you are in a configured ML-Git project directory, the following command will update the metadata repository, allowing visibility of what has been shared since the last update (new ML entity, new versions). $ ml-git dataset update Or update all metadata repository: $ ml-git repository update To discover which datasets are under ML-Git management, you can execute the following command: $ ml-git dataset list ML dataset |-- computer-vision | |-- images | | |-- dataset-ex-minio | | |-- imagenet8 | | |-- dataset-ex The ML-Git repository contains 3 different datasets, all falling under the same category computer-vision/images . In order for ML-Git to manage the different versions of the same dataset. It internally creates a tag based on categories, ML entity name and its version. To show all these tag representing the versions of a dataset, simply type the following: $ ml-git dataset tag list imagenet8 computer-vision__images__imagenet8__1 computer-vision__images__imagenet8__2 It means there are actually 2 versions under ML-Git management. You can check what version is checked out in the ML-Git workspace with the following command: $ ml-git dataset branch imagenet8 ('vision-computing__images__imagenet8__2', '48ba1e994a1e39e1b508bff4a3302a5c1bb9063e') The output is a tuple: The tag auto-generated by ML-Git based on the .spec. The sha of the git commit of that version. It is simple to retrieve a specific version locally to start any experiment by executing one of the following commands: $ ml-git dataset checkout computer-vision__images__imagenet8__1 or $ ml-git dataset checkout imagenet8 --version=1 If you want to get the latest available version of an entity you can just pass its name in the checkout command, as shown below: $ ml-git dataset checkout imagenet8 Getting the data will auto-create a directory structure under dataset directory as shown below. That structure computer-vision/images is coming from the categories defined in the dataset spec file. Doing that way allows for easy download many datasets in one single ML-Git project without creating any conflicts. computer-vision/ \u2514\u2500\u2500 images \u2514\u2500\u2500 imagenet8 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_1 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_2 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_3 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_4 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_5 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_6 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_7 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_8 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_9 \u2502 \u2502 \u2514\u2500\u2500 train_data_batch_10 \u2502 \u2514\u2500\u2500 val \u2502 \u2514\u2500\u2500 val_data \u2514\u2500\u2500 imagenet8.spec Downloading a dataset: Checking data integrity \u00b6 If at some point you want to check the integrity of the metadata repository (e.g. computer shuts down during a process), simply type the following command: $ ml-git dataset fsck INFO - HashFS: starting integrity check on [.\\.ml-git\\dataset\\objects\\hashfs] ERROR - HashFS: corruption detected for chunk [zdj7WVccN8cRj1RcvweX3FNUEQyBe1oKEsWsutJNJoxt12mn1] - got [zdj7WdCbyFbcqHVMarj3KCLJ7yjTM3S9X26RyXWTfXGB2czeB] INFO - HashFS: starting integrity check on [.\\.ml-git\\dataset\\index\\hashfs] [1] corrupted file(s) in Local Repository: ['zdj7WVccN8cRj1RcvweX3FNUEQyBe1oKEsWsutJNJoxt12mn1'] [0] corrupted file(s) in Index: [] Total of corrupted files: 1 That command will walk through the internal ML-Git directories (index & local repository) and will check the integrity of all blobs under its management. It will return the list of blobs that are corrupted. Checking data integrity: Changing a Dataset \u00b6 When adding files to an entity ML-Git locks the files for read only. When the entity's mutability type is flexible or mutable, you can change the data of a file and resubmit it without being considered corrupted. In case of a flexible entity you should perform the following command to unlock the file: ml-git dataset unlock imagenet8 data\\train\\train_data_batch_1 After that, the unlocked file is subject to modification. If you modify the file without performing this command, it will be considered corrupted. To upload the data, you can execute the following commands: ml-git dataset add <yourdataset> --bumpversion ml-git dataset commit <yourdataset> ml-git dataset push <yourdataset> This will create a new version of your dataset and push the changes to your remote storage (e.g. S3). Changing a dataset:","title":"Creating Your First Project"},{"location":"first_project/#your-1st-ml-artefacts-under-ml-git-management","text":"We will divide this quick howto into 6 main sections: ML-Git repository configuration / intialization This section explains how to initialize and configure a repository for ML-Git, considering the scenarios of the store be an S3 or a MinIO. Uploading a dataset Having a repository initialized, this section explains how to create and upload a dataset to the store. Adding data to a dataset This section explains how to add new data to an entity already versioned by ML-Git. Uploading labels associated to a dataset This section describes how to upload a set of labels by associating the dataset to which these labels refer. Downloading a dataset This section describes how to download a versioned data set using ML-Git. Checking data integrity This section explains how to check the integrity of the metadata repository. At the end of each section there is a video to demonstrate the ML-Git usage.","title":"Your 1st ML artefacts under ML-Git management"},{"location":"first_project/#initial-configuration-of-ml-git","text":"Make sure you have created your own git repository (more information) for dataset metadata and a S3 bucket or a MinIO server for the dataset actual data. If you haven't created it yet, you can use the resources initialization script which aims to facilitate the creation of resources (buckets and repositories). After that, create a ML-Git project. To do this, use the following commands (note that 'mlgit-project' is the project name used as example): $ mkdir mlgit-project && cd mlgit-project (or clone an existing repo from Github or Github Enterprise) $ ml-git repository init Now, we need to configure our project with the remote configurations. This section is divided into two parts according to the storage: Setting up a ml-git project with S3 and Setting up a ml-git project with MinIO . After configuring the project with the bucket, the remote ones, the credentials that will be used, and the other configurations that were performed in this section, a good practice is to make the version of the .ml-git folder that was generated in a git repository. That way in future projects or if you want to share with someone you can use the command ml-git clone to import the project's settings, without having to configure it for each new project.","title":"Initial configuration of ML-Git"},{"location":"first_project/#setting-up-a-ml-git-project-with-s3","text":"In addition to creating the bucket in S3, it is necessary to configure the settings that the ML-Git uses to interact with your bucket, see how to configure a S3 bucket for more details. For a basic ML-Git repository, you need to add a remote repository for metadata and a S3 bucket configuration. $ ml-git repository remote dataset add git@github.com:example/your-mlgit-datasets.git $ ml-git repository storage add mlgit-datasets --credentials=mlgit Last but not least, initialize the metadata repository. $ ml-git dataset init","title":"Setting up a ML-Git project with S3 "},{"location":"first_project/#setting-up-a-ml-git-project-with-minio","text":"Same as for S3, in addition to creating the MinIO server, it is necessary to configure the settings that the ML-Git uses to interact with your bucket, see how to configure a MinIO for this. For a basic ML-Git repository, you need to add a remote repository for metadata and the MinIO bucket configuration. $ ml-git dataset remote add git@github.com:example/your-mlgit-datasets.git $ ml-git storage add mlgit-datasets --credentials=mlgit --endpoint-url=<minio-endpoint-url> After that initialize the metadata repository. $ ml-git dataset init Setting up ML-Git project with MinIO:","title":"Setting up a ML-Git project with MinIO "},{"location":"first_project/#why-ml-git-uses-git","text":"The ML-Git uses git to versioning project's metadata. See below versioned metadata: .spec , is the specification file that contains informations like version number, artefact name, entity type (dataset, label, model), categories (tree struct that caracterize an entity). MANIFEST.yaml , is responsible to map artefact's files. The files are mapped by hashes, that are the references used to perform operations in local files, and download/upload operations in Stores (AWS|MinIO). You can find more information about metadata here . All configurations are stored in .ml-git/config.yaml and you can look at configuration state at any time with the following command: $ ml-git repository config config: {'batch_size': 20, 'cache_path': '', 'dataset': {'git': 'git@github.com:example/your-mlgit-datasets.git'}, 'index_path': '', 'labels': {'git': ''}, 'metadata_path': '', 'mlgit_conf': 'config.yaml', 'mlgit_path': '.ml-git', 'model': {'git': ''}, 'object_path': '', 'push_threads_count': 10, 'refs_path': '', 'store': {'s3': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'}, 'region': 'us-east-1'}}, 's3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'mlgit'}, 'endpoint-url': <minio-endpoint-url>, 'region': 'us-east-1'}}}, 'verbose': 'info'}","title":"Why ML-Git uses git?"},{"location":"first_project/#uploading-a-dataset","text":"To create and upload a dataset to a storage, you must be in an already initialized project, if necessary read section 1 to initialize and configure a project. ML-Git expects any dataset to be specified under dataset/ directory of your project and it expects a specification file with the name of the dataset. To create this specification file for a new entity you must run the following command: $ ml-git dataset create imagenet8 --category=computer-vision --category=images --mutability=strict --storage-type=s3h --bucket-name=mlgit-datasets --version=1 After that a file must have been created in dataset/imagenet8/imagenet8.spec and should look like this: dataset: categories: - computer-vision - images manifest: store: s3h://mlgit-datasets mutability: strict name: imagenet8 version: 1 There are 5 main items in the spec file: name : it's the dataset name version : the version should be a positive integer, incremented each time a new version is pushed into ML-Git. You can use the --bumpversion as an argument to do the automatic increment when you add more files to a dataset. categories : describes a tree structure to characterize the dataset category. This information is used by ML-Git to create a directory structure in the git repository managing the metadata. manifest : describes the data storage in which the data is actually stored. In the above example, a S3 bucket named mlgit-datasets . The AWS credential profile name and AWS region should be found in the ML-Git config file. mutability : describes the mutability option that your project has. The mutability options are \"strict\", \"flexible\" and \"mutable\", after selecting one of these options, you cannot change that. If you want to know more about each type of mutability and how it works, please take a look at mutability documentation . The items listed above are mandatory in the spec. An important point to note is if the user wishes, it is possible to add new items that will be versioned with the spec. The example below presents a spec with the entity's owner information to be versioned. Those information were put under metadata field just for purpose of organization. dataset: categories: - computer-vision - images mutability: strict manifest: store: s3h://mlgit-datasets name: imagenet8 version: 1 metadata: owner: name: <your-name-here> email: <your-email-here> After creating the dataset spec file, you can create a README.md to create a web page describing your dataset, adding references and any other useful information. Then, you can put the data of that dataset under the directory. Below, you will see the tree of imagenet8 directory and file structure: imagenet8/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_1 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_2 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_3 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_4 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_5 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_6 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_7 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_8 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_9 \u2502 \u2502 \u2514\u2500\u2500 train_data_batch_10 \u2502 \u2514\u2500\u2500 val \u2502 \u2514\u2500\u2500 val_data \u2514\u2500\u2500 imagenet8.spec You can look at the working tree status with the following command: $ ml-git dataset status imagenet8 INFO - Repository: dataset: status of ml-git index for [imagenet8] Changes to be committed untracked files imagenet8.spec README.md data\\train\\train_data_batch_1 data\\train\\train_data_batch_2 data\\train\\train_data_batch_3 data\\train\\train_data_batch_4 data\\train\\train_data_batch_5 data\\train\\train_data_batch_6 data\\train\\train_data_batch_7 data\\train\\train_data_batch_8 data\\train\\train_data_batch_9 data\\train\\train_data_batch_10 data\\val\\val_data corrupted files That command allows printing the tracked files and the ones in the index/staging area. Now, you are ready to put that new dataset under ML-Git management. For this, do: $ ml-git dataset add imagenet8 The command \" ml-git dataset add \" adds the files into a specific dataset, such as imagenet8 in the index/staging area. If you check the working tree status, you can see that now the files appear as tracked but not committed yet: $ ml-git dataset status imagenet8 INFO - Repository: dataset: status of ml-git index for [imagenet8] Changes to be committed new file: data\\train\\train_data_batch_1 new file: data\\train\\train_data_batch_2 new file: data\\train\\train_data_batch_3 new file: data\\train\\train_data_batch_4 new file: data\\train\\train_data_batch_5 new file: data\\train\\train_data_batch_6 new file: data\\train\\train_data_batch_7 new file: data\\train\\train_data_batch_8 new file: data\\train\\train_data_batch_9 new file: data\\train\\train_data_batch_10 new file: data\\val\\val_data untracked files corrupted files Then, you can commit the metadata to the local repository. For this purpose, type the following command: $ ml-git dataset commit imagenet8 After that, you can use \" ml-git dataset push \" to update the remote metadata repository just after storing all actual data under management in the specified remote data store. $ ml-git dataset push imagenet8 As you can observe, ML-Git follows very similar workflows as git. Uploading a dataset:","title":"Uploading a dataset"},{"location":"first_project/#adding-data-to-a-dataset","text":"If you want to add data to a dataset, perform the following steps: In your workspace, copy the new data in under dataset/<your-dataset>/data Modify the version number. To do this step you have two ways: You can put the option --bumpversion on the add command to auto increment the version number, as shown below. Or, you can put the option --version on the commit command to set an specific version number. After that, like in the previous section, you need to execute the following commands to upload the new data: ml-git dataset add <your-dataset> --bumpversion ml-git dataset commit <your-dataset> ml-git dataset push <your-dataset> This will create a new version of your dataset and push the changes to your remote storage (e.g. S3). Adding data to a dataset:","title":"Adding data to a dataset"},{"location":"first_project/#uploading-labels-associated-to-a-dataset","text":"To create and upload labels associated to a dataset, you must be in an already initialized project, if necessary read section 1 to initialize and configure the project. Also, you will need to have a dataset already versioned by ML-Git in your repository, see section 2 . The first step is to configure your metadata and data repository/storage. $ ml-git repository remote labels add git@github.com:example/your-mlgit-labels.git $ ml-git repository storage add mlgit-labels $ ml-git labels init Even these commands show a different bucket to store the labels data. It would be possible to store both datasets and labels into the same bucket. If you look at your config file, you would see the following information: $ ml-git repository config config: {'batch_size': 20, 'cache_path': '', 'dataset': {'git': 'git@github.com:example/your-mlgit-datasets.git'}, 'index_path': '', 'labels': {'git': 'git@github.com:example/your-mlgit-labels.git'}, 'metadata_path': '', 'mlgit_conf': 'config.yaml', 'mlgit_path': '.ml-git', 'model': {'git': ''}, 'object_path': '', 'refs_path': '', 'store': {'s3': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'}, 'region': 'us-east-1'}}, 's3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': None, 'region': 'us-east-1'}}}, 's3h': {'mlgit-labels': {'aws-credentials': {'profile': 'default'}, 'endpoint-url': None, 'region': 'us-east-1'}}}, 'verbose': 'info'} Then, you can create your first set of labels. As an example, we will use mscoco. ML-Git expects any set of labels to be specified under the labels/ directory of your project. Also, it expects a specification file with the name of the labels . $ ml-git labels create mscoco-captions --category=computer-vision --category=captions --mutability=mutable --store-type=s3h --bucket-name=mlgit-labels --version=1 After create the entity, you can create the README.md describing your set of labels. Below is the tree of caption labels for the mscoco directory and file structure: mscoco-captions/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 captions_train2014.json \u2502 \u2514\u2500\u2500 captions_val2014.json \u2514\u2500\u2500 mscoco-captions.spec Now, you are ready to put the new set of labels under ML-Git management. We assume there is an existing mscoco dataset. For this, do: $ ml-git labels add mscoco-captions $ ml-git labels commit mscoco-captions --dataset=mscoco $ ml-git labels push mscoco-captions The commands are very similar to dataset operations. However, you can note one particular change in the commit command. There is an option \" --dataset \" which is used to tell ML-Git that the labels should be linked to the specified dataset. Internally, ML-Git will look at the checked out dataset in your workspace for that specified dataset. Then, it will include the git tag and sha into the specification file to be committed into the metadata repository. Once done, anyone will be able to retrieve the exact same version of the dataset that has been used for that specific set of labels. One can look at the specific dataset associated with that set of labels by executing the following command: $ ml-git labels show mscoco-captions -- labels : mscoco-captions -- categories: - computer-vision - captions dataset: sha: 607fa818da716c3313a6855eb3bbd4587e412816 tag: computer-vision__images__mscoco__1 manifest: files: MANIFEST.yaml store: s3h://mlgit-datasets name: mscoco-captions version: 1 As you can see, there is a new section \" dataset \" that has been added by ML-Git with the sha & tag fields. It can be used to checkout the exact version of the dataset for that set of labels. Uploading labels related to a dataset:","title":"Uploading labels associated to a dataset"},{"location":"first_project/#downloading-a-dataset","text":"We assume there is an existing ML-Git repository with a few ML datasets under its management and you'd like to download one of the available datasets. If you don't have a dataset versioned by the ML-Git, see section 2 on how to do this. To download a dataset, you need to be in an initialized and configured ML-Git project. If you have a repository with your saved settings, you can run the following command to set up your environment: $ ml-git clone git@github.com:example/your-mlgit-repository.git If you are in a configured ML-Git project directory, the following command will update the metadata repository, allowing visibility of what has been shared since the last update (new ML entity, new versions). $ ml-git dataset update Or update all metadata repository: $ ml-git repository update To discover which datasets are under ML-Git management, you can execute the following command: $ ml-git dataset list ML dataset |-- computer-vision | |-- images | | |-- dataset-ex-minio | | |-- imagenet8 | | |-- dataset-ex The ML-Git repository contains 3 different datasets, all falling under the same category computer-vision/images . In order for ML-Git to manage the different versions of the same dataset. It internally creates a tag based on categories, ML entity name and its version. To show all these tag representing the versions of a dataset, simply type the following: $ ml-git dataset tag list imagenet8 computer-vision__images__imagenet8__1 computer-vision__images__imagenet8__2 It means there are actually 2 versions under ML-Git management. You can check what version is checked out in the ML-Git workspace with the following command: $ ml-git dataset branch imagenet8 ('vision-computing__images__imagenet8__2', '48ba1e994a1e39e1b508bff4a3302a5c1bb9063e') The output is a tuple: The tag auto-generated by ML-Git based on the .spec. The sha of the git commit of that version. It is simple to retrieve a specific version locally to start any experiment by executing one of the following commands: $ ml-git dataset checkout computer-vision__images__imagenet8__1 or $ ml-git dataset checkout imagenet8 --version=1 If you want to get the latest available version of an entity you can just pass its name in the checkout command, as shown below: $ ml-git dataset checkout imagenet8 Getting the data will auto-create a directory structure under dataset directory as shown below. That structure computer-vision/images is coming from the categories defined in the dataset spec file. Doing that way allows for easy download many datasets in one single ML-Git project without creating any conflicts. computer-vision/ \u2514\u2500\u2500 images \u2514\u2500\u2500 imagenet8 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_1 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_2 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_3 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_4 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_5 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_6 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_7 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_8 \u2502 \u2502 \u251c\u2500\u2500 train_data_batch_9 \u2502 \u2502 \u2514\u2500\u2500 train_data_batch_10 \u2502 \u2514\u2500\u2500 val \u2502 \u2514\u2500\u2500 val_data \u2514\u2500\u2500 imagenet8.spec Downloading a dataset:","title":"Downloading a dataset"},{"location":"first_project/#checking-data-integrity","text":"If at some point you want to check the integrity of the metadata repository (e.g. computer shuts down during a process), simply type the following command: $ ml-git dataset fsck INFO - HashFS: starting integrity check on [.\\.ml-git\\dataset\\objects\\hashfs] ERROR - HashFS: corruption detected for chunk [zdj7WVccN8cRj1RcvweX3FNUEQyBe1oKEsWsutJNJoxt12mn1] - got [zdj7WdCbyFbcqHVMarj3KCLJ7yjTM3S9X26RyXWTfXGB2czeB] INFO - HashFS: starting integrity check on [.\\.ml-git\\dataset\\index\\hashfs] [1] corrupted file(s) in Local Repository: ['zdj7WVccN8cRj1RcvweX3FNUEQyBe1oKEsWsutJNJoxt12mn1'] [0] corrupted file(s) in Index: [] Total of corrupted files: 1 That command will walk through the internal ML-Git directories (index & local repository) and will check the integrity of all blobs under its management. It will return the list of blobs that are corrupted. Checking data integrity:","title":"Checking data integrity"},{"location":"first_project/#changing-a-dataset","text":"When adding files to an entity ML-Git locks the files for read only. When the entity's mutability type is flexible or mutable, you can change the data of a file and resubmit it without being considered corrupted. In case of a flexible entity you should perform the following command to unlock the file: ml-git dataset unlock imagenet8 data\\train\\train_data_batch_1 After that, the unlocked file is subject to modification. If you modify the file without performing this command, it will be considered corrupted. To upload the data, you can execute the following commands: ml-git dataset add <yourdataset> --bumpversion ml-git dataset commit <yourdataset> ml-git dataset push <yourdataset> This will create a new version of your dataset and push the changes to your remote storage (e.g. S3). Changing a dataset:","title":"Changing a Dataset"},{"location":"gdrive_configurations/","text":"Google Drive API configuration \u00b6 This section aims to explain how to enable Google Drive API and configure OAuth 2.0 credentials to use with ml-git. Enabling Drive API \u00b6 You need to create a project in Google developer console to activate Drive API, follow instructions bellow: 1. Access console developer and click on create project: 2. Then type name of your preference and click on \"CREATE\" button: 3. Go back to dashboard and enable Drive API: 4. Search for Drive API on search bar: Creating credentials \u00b6 When you finish Enabling API step, you need to create your credentials and configure authentication consent screen. 1. Click on create credentials: 2. Select user type of your consent: 3. Add application name to authentication consent screen: 4. Change application's scope if you prefer and save: 5. Go back to dashboard and click on create credentials and generate your API KEY: 6. Generate OAuth client id: 7. Add client name and select application type: 8. Finally you can download your credentials file: Setting up a ml-git project with Google Drive \u00b6 Create directory with name of your preference and copy your credentials file with name credentials.json inside the directory. Add store configurations example: $ ml-git repository store add path-in-your-drive --type=gdriveh --credentials=/home/profile/.gdrive After that initialize the metadata repository: $ ml-git dataset init We strongly recommend that you add push_threads_count: 10 option in your . ml-git/config.yaml , because of Google Drive API request limit of 10/s. This option change the number of workers used in multithreading push process, by default the number of workers is cpu numbers multiplied by 5. The push command was tested with 10 workers and the request limit was not exceeded. Configuration example: batch_size: 20 push_threads_count: 10 dataset: git: '' labels: git: '' model: git: '' store: s3: mlgit-datasets: aws-credentials: profile: default region: us-east-1","title":"Google Drive"},{"location":"gdrive_configurations/#google-drive-api-configuration","text":"This section aims to explain how to enable Google Drive API and configure OAuth 2.0 credentials to use with ml-git.","title":"Google Drive API configuration"},{"location":"gdrive_configurations/#enabling-drive-api","text":"You need to create a project in Google developer console to activate Drive API, follow instructions bellow: 1. Access console developer and click on create project: 2. Then type name of your preference and click on \"CREATE\" button: 3. Go back to dashboard and enable Drive API: 4. Search for Drive API on search bar:","title":"Enabling Drive API"},{"location":"gdrive_configurations/#creating-credentials","text":"When you finish Enabling API step, you need to create your credentials and configure authentication consent screen. 1. Click on create credentials: 2. Select user type of your consent: 3. Add application name to authentication consent screen: 4. Change application's scope if you prefer and save: 5. Go back to dashboard and click on create credentials and generate your API KEY: 6. Generate OAuth client id: 7. Add client name and select application type: 8. Finally you can download your credentials file:","title":"Creating credentials"},{"location":"gdrive_configurations/#setting-up-a-ml-git-project-with-google-drive","text":"Create directory with name of your preference and copy your credentials file with name credentials.json inside the directory. Add store configurations example: $ ml-git repository store add path-in-your-drive --type=gdriveh --credentials=/home/profile/.gdrive After that initialize the metadata repository: $ ml-git dataset init We strongly recommend that you add push_threads_count: 10 option in your . ml-git/config.yaml , because of Google Drive API request limit of 10/s. This option change the number of workers used in multithreading push process, by default the number of workers is cpu numbers multiplied by 5. The push command was tested with 10 workers and the request limit was not exceeded. Configuration example: batch_size: 20 push_threads_count: 10 dataset: git: '' labels: git: '' model: git: '' store: s3: mlgit-datasets: aws-credentials: profile: default region: us-east-1","title":"Setting up a ml-git project with Google Drive"},{"location":"mlgit_commands/","text":"ML-Git commands \u00b6 ml-git --help Usage: ml-git [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. Commands: clone Clone a ml-git repository ML_GIT_REPOSITORY_URL dataset management of datasets within this ml-git repository labels management of labels sets within this ml-git repository login login command generates new Aws credential. model management of models within this ml-git repository repository Management of this ml-git repository Example: $ ml-git --help ml-git --version Displays the installed version of ML-Git. ml-git <ml-entity> add Usage: ml-git dataset add [OPTIONS] ML_ENTITY_NAME [FILE_PATH]... Add dataset change set ML_ENTITY_NAME to the local ml-git staging area. Options: --bumpversion Increment the version number when adding more files. --fsck Run fsck after command execution. --help Show this message and exit. Example: $ ml-git dataset add dataset-ex --bumpversion ml-git expects datasets to be managed under dataset directory. \\<ml-entity-name> is also expected to be a repository under the tree structure and ml-git will search for it in the tree. Under that repository, it is also expected to have a \\<ml-entity-name>.spec file, defining the ML entity to be added. Optionally, one can add a README.md which will describe the dataset and be what will be shown in the github repository for that specific dataset. Internally, the ml-git add will add all the files under the \\<ml-entity> directory into the ml-git index / staging area. ml-git <ml-entity> branch Usage: ml-git dataset branch [OPTIONS] ML_ENTITY_NAME This command allows to check which tag is checked out in the ml-git workspace. Options: --help Show this message and exit. Example: $ ml-git dataset branch imagenet8 ('vision-computing__images__imagenet8__1', '48ba1e994a1e39e1b508bff4a3302a5c1bb9063e') That information is equal to the HEAD reference from a git concept. ml-git keeps that information on a per \\<ml-entity-name> basis. which enables independent checkout of each of these \\<ml-entity-name>. The output is a tuple: 1) the tag auto-generated by ml-git based on the \\<ml-entity-name>.spec (composite with categories, \\<ml-entity-name>, version) 2) the sha of the git commit of that \\<ml-entity> version Both are the same representation. One is human-readable and is also used internally by ml-git to find out the path to the referenced \\<ml-entity-name>. ml-git <ml-entity> checkout Usage: ml-git model checkout [OPTIONS] ML_ENTITY_TAG|ML_ENTITY Checkout the ML_ENTITY_TAG|ML_ENTITY of a model set into user workspace. Options: -l, --with-labels The checkout associated labels in user workspace as well. -d, --with-dataset The checkout associated dataset in user workspace as well. --retry INTEGER Number of retries to download the files from the storage [default: 2]. --force Force checkout command to delete untracked/uncommitted files from local repository. --bare Ability to add/commit/push without having the ml- entity checked out. --version INTEGER Number of artifact version to be downloaded [default: latest]. --verbose Debug mode Examples: $ ml-git dataset checkout computer-vision__images__faces__fddb__1 or you can use the name of the entity directly and download the latest available tag $ ml-git dataset checkout fddb Note: --d: It can only be used in checkout of labels and models to get the entities that are associated with the entity. --l: It can only be used in checkout of models to get the label entity that are associated with the entity. --sample-type, --sampling, --seed: These options are available only for dataset. If you use this option ml-git will not allow you to make changes to the entity and create a new tag. ml-git <ml-entity> commit Usage: ml-git model commit [OPTIONS] ML_ENTITY_NAME Commit model change set of ML_ENTITY_NAME locally to this ml-git repository. Options: --dataset TEXT Link dataset entity name to this model set version. --labels TEXT Link labels entity name to this model set version. --tag TEXT Ml-git tag to identify a specific version of a ML entity. --version-number, --version INTEGER RANGE Set the number of artifact version. [DEPRECATED:--version-number] -m, --message TEXT Use the provided <msg> as the commit message. --fsck TEXT Run fsck after command execution. --verbose Debug mode Example: $ ml-git model commit model-ex --dataset=dataset-ex This command commits the index / staging area to the local repository. It is a 2-step operation in which 1) the actual data (blobs) is copied to the local repository, 2) committing the metadata to the git repository managing the metadata. Internally, ml-git keeps track of files that have been added to the data store and is storing that information to the metadata management layer to be able to restore any version of each \\<ml-entity-name>. Another important feature of ml-git is the ability to keep track of the relationship between the ML entities. So when committing a label set, one can (should) provide the option --dataset=<dataset-name> . Internally, ml-git will inspect the HEAD / ref of the specified \\<dataset-name> checked out in the ml-git repository and will add that information to the specificatino file that is committed to the metadata repository. With that relationship kept into the metadata repository, it is now possible for anyone to checkout exactly the same versions of labels and dataset. Same for ML model, one can specify which dataset and label set that have been used to generate that model through --dataset=<dataset-name> and --labels=<labels-name> ml-git <ml-entity> create Usage: ml-git dataset create [OPTIONS] ARTIFACT_NAME This command will create the workspace structure with data and spec file for an entity and set the git and store configurations. Options: --category TEXT Artifact's category name. [required] --mutability [strict|flexible|mutable] Mutability type. [required] --store-type, --storage-type [s3h|azureblobh|gdriveh] Data storage type [default: s3h]. [DEPRECATED:--store-type] --version-number, --version INTEGER RANGE Number of artifact version. [DEPRECATED:--version-number] --import TEXT Path to be imported to the project. NOTE: Mutually exclusive with argument: credentials_path, import_url. --wizard-config If specified, ask interactive questions. at console for git & store configurations. --bucket-name TEXT Bucket name --import-url TEXT Import data from a google drive url. NOTE: Mutually exclusive with argument: import. --credentials-path TEXT Directory of credentials.json. NOTE: This option is required if --import-url is used. --unzip Unzip imported zipped files. Only available if --import-url is used. --verbose Debug mode Examples: - To create an entity with s3 as storage and importing files from a path of your computer: ml-git dataset create imagenet8 --storage-type=s3h --category=computer-vision --category=images --version=0 --import='/path/to/dataset' --mutability=strict To create an entity with s3 as storage and importing files from a google drive URL: ml-git dataset create imagenet8 --storage-type=s3h --category=computer-vision --category=images --import-url='gdrive.url' --credentials-path='/path/to/gdrive/credentials' --mutability=strict --unzip ml-git <ml-entity> export Usage: ml-git dataset export [OPTIONS] ML_ENTITY_TAG BUCKET_NAME This command allows you to export files from one store (S3|MinIO) to another (S3|MinIO). Options: --credentials TEXT Profile of AWS credentials [default: default]. --endpoint TEXT Endpoint where you want to export --region TEXT AWS region name [default: us-east-1]. --retry INTEGER Number of retries to upload or download the files from the storage [default: 2]. --help Show this message and exit. Example: $ ml-git dataset export computer-vision__images__faces__fddb__1 minio ml-git <ml-entity> fetch Usage: ml-git dataset fetch [OPTIONS] ML_ENTITY_TAG Allows you to download just the metadata files of an entity. Options: --sample-type [group|range|random] --sampling TEXT The group: <amount>:<group> The group sample option consists of amount and group used to download a sample. range: <start:stop:step> The range sample option consists of start, stop and step used to download a sample. The start parameter can be equal or greater than zero.The stop parameter can be 'all', -1 or any integer above zero. random: <amount:frequency> The random sample option consists of amount and frequency used to download a sample. --seed TEXT Seed to be used in random-based samplers. --retry INTEGER Number of retries to download the files from the storage [default: 2]. --help Show this message and exit. Example: ml-git dataset fetch computer-vision__images__faces__fddb__1 ml-git <ml-entity> fsck Usage: ml-git dataset fsck [OPTIONS] Perform fsck on dataset in this ml-git repository. Options: --help Show this message and exit. Example: $ ml-git dataset fsck This command will walk through the internal ml-git directories (index & local repository) and will check the integrity of all blobs under its management. It will return the list of blobs that are corrupted. Note: in the future, fsck should be able to fix some errors of detected corruption. ml-git <ml-entity> import Usage: ml-git dataset import [OPTIONS] BUCKET_NAME ENTITY_DIR This command allows you to download a file or directory from the S3 bucket or Gdrive to ENTITY_DIR. Options: --credentials TEXT Profile of AWS credentials [default: default]. --region TEXT AWS region name [default: us-east-1]. --retry INTEGER Number of retries to download the files from the storage [default: 2]. --path TEXT Bucket folder path. --object TEXT Filename in bucket. --store-type, --storage-type [s3|gdrive] Data storage type [default: s3h]. [DEPRECATED:--store-type] --endpoint-url Storage endpoint url. --help Show this message and exit. Example: $ ml-git dataset import bucket-name dataset/computer-vision/imagenet8/data For google drive store: $ ml-git dataset import gdrive-folder --store-type=gdrive --object=file_to_download --credentials=credentials-path dataset/ ml-git <ml-entity> init Usage: ml-git dataset init [OPTIONS] Init a ml-git dataset repository. Options: --help Show this message and exit. Example: $ ml-git dataset init This command is mandatory to be executed just after the addition of a remote metadata repository ( ml-git \\<ml-entity> remote add ). It initializes the metadata by pulling all metadata to the local repository. ml-git <ml-entity> list Usage: ml-git dataset list [OPTIONS] List dataset managed under this ml-git repository. Options: --help Show this message and exit. Example: $ ml-git dataset list ML dataset |-- computer-vision | |-- images | | |-- dataset-ex-minio | | |-- imagenet8 | | |-- dataset-ex ml-git <ml-entity> log Usage: ml-git dataset log [OPTIONS] ML_ENTITY_NAME This command shows ml-entity-name's commit information like author, date, commit message. Options: --stat Show amount of files and size of an ml-entity. --fullstat Show added and deleted files. --help Show this message and exit. Example: ml-git dataset log dataset-ex ml-git <ml-entity> push Usage: ml-git dataset push [OPTIONS] ML_ENTITY_NAME Push local commits from ML_ENTITY_NAME to remote ml-git repository & store. Options: --retry INTEGER Number of retries to upload or download the files from the storage [default: 2]. --clearonfail Remove the files from the store in case of failure during the push operation. --help Show this message and exit. Example: ml-git dataset push dataset-ex This command will perform a 2-step operations: 1. push all blobs to the configured data store. 2. push all metadata related to the commits to the remote metadata repository. ml-git <ml-entity> remote-fsck Usage: ml-git dataset remote-fsck [OPTIONS] ML_ENTITY_NAME This command will check and repair the remote by uploading lacking chunks/blobs. Options: --thorough Try to download the IPLD if it is not present in the local repository to verify the existence of all contained IPLD links associated. --paranoid Adds an additional step that will download all IPLD and its associated IPLD links to verify the content by computing the multihash of all these. --retry INTEGER Number of retries to download the files from the storage [default: 2]. --help Show this message and exit. Example: ml-git dataset remote-fsck dataset-ex This ml-git command will basically try to: Detects any chunk/blob lacking in a remote store for a specific ML artefact version Repair - if possible - by uploading lacking chunks/blobs In paranoid mode, verifies the content of all the blobs ml-git <ml-entity> reset Usage: ml-git dataset reset [OPTIONS] ML_ENTITY_NAME Reset ml-git state(s) of an ML_ENTITY_NAME Options: --hard Remove untracked files from workspace, files to be committed from staging area as well as committed files upto <reference>. --mixed Revert the committed files and the staged files to 'Untracked Files'. This is the default action. --soft Revert the committed files to 'Changes to be committed'. --reference [head|head~1] head:Will keep the metadata in the current commit. head~1:Will move the metadata to the last commit. --help Show this message and exit. Examples: ml-git reset --hard Undo the committed changes. Undo the added/tracked files. Reset the workspace to fit with the current HEAD state. ml-git reset --mixed if HEAD: * nothing happens. else: * Undo the committed changes. * Undo the added/tracked files. ml-git reset --soft if HEAD: * nothing happens. else: * Undo the committed changes. ml-git <ml-entity> show Usage: ml-git dataset show [OPTIONS] ML_ENTITY_NAME Print the specification file of the entity. Options: --help Show this message and exit. Example: $ ml-git dataset show dataset-ex -- dataset : imagenet8 -- categories: - vision-computing - images manifest: files: MANIFEST.yaml store: s3h://mlgit-datasets name: imagenet8 version: 1 ml-git <ml-entity> status Usage: ml-git dataset status [OPTIONS] ML_ENTITY_NAME [STATUS_DIRECTORY] Print the files that are tracked or not and the ones that are in the index/staging area. Options: --full Show all contents for each directory. --verbose Debug mode Example: $ ml-git dataset status dataset-ex ml-git <ml-entity> tag add Usage: ml-git dataset tag add [OPTIONS] ML_ENTITY_NAME TAG Use this command to associate a tag to a commit. Options: --help Show this message and exit. Example: $ ml-git dataset tag add dataset-ex my_tag ml-git <ml-entity> tag list Usage: ml-git dataset tag list [OPTIONS] ML_ENTITY_NAME List tags of ML_ENTITY_NAME from this ml-git repository. Options: --help Show this message and exit. Example: $ ml-git dataset tag list dataset-ex ml-git <ml-entity> update Usage: ml-git dataset update [OPTIONS] This command will update the metadata repository. Options: --help Show this message and exit. Example: $ ml-git dataset update This command enables one to have the visibility of what has been shared since the last update (new ML entity, new versions). ml-git <ml-entity> unlock Usage: ml-git dataset unlock [OPTIONS] ML_ENTITY_NAME FILE This command add read and write permissions to file or directory. Note: You should only use this command for the flexible mutability option. Options: --help Show this message and exit. Example: $ ml-git dataset unlock dataset-ex data/file1.txt Note: You should only use this command for the flexible mutability option. ml-git clone <repository-url> Usage: ml-git clone [OPTIONS] REPOSITORY_URL Clone a ml-git repository ML_GIT_REPOSITORY_URL Options: --folder TEXT --track --help Show this message and exit. Example: $ ml-git clone https://git@github.com/mlgit-repository ml-git login Usage: ml-git login [OPTIONS] login command generates new Aws credential. Options: --credentials TEXT profile name for store credentials [default: default]. --insecure use this option when operating in a insecure location. This option prevents storage of a cookie in the folder. Never execute this program without --insecure option in a compute device you do not trust. --rolearn TEXT directly STS to this AWS Role ARN instead of the selecting the option during runtime. --help Show this message and exit. Example: ml-git login Note: ml-git repository config Usage: ml-git repository config [OPTIONS] Configuration of this ml-git repository Options: --help Show this message and exit. Example: $ ml-git repository config config: {'dataset': {'git': 'git@github.com:example/your-mlgit-datasets'}, 'store': {'s3': {'mlgit-datasets': {'aws-credentials': {'profile': 'mlgit'}, 'region': 'us-east-1'}}}, 'verbose': 'info'} Use this command if you want to check what configuration ml-git is running with. It is highly likely one will need to change the default configuration to adapt for her needs. ml-git repository gc Usage: ml-git repository gc [OPTIONS] Cleanup unnecessary files and optimize the use of the disk space. Options: --verbose Debug mode This command will remove unnecessary files contained in the cache and objects directories of the ml-git metadata (.ml-git). ml-git repository init Usage: ml-git repository init [OPTIONS] Initialiation of this ml-git repository Options: --help Show this message and exit. Example: $ ml-git repository init This is the first command you need to run to initialize a ml-git project. It will bascially create a default .ml-git/config.yaml ml-git repository remote <ml-entity> add Usage: ml-git repository remote dataset add [OPTIONS] REMOTE_URL Add remote dataset metadata REMOTE_URL to this ml-git repository Options: --help Show this message and exit. Example: $ ml-git repository remote dataset add https://git@github.com/mlgit-datasets ml-git repository remote <ml-entity> del Usage: ml-git repository remote dataset del Remove remote dataset metadata REMOTE_URL from this ml-git repository Options: --help Show this message and exit. Example: $ ml-git repository remote dataset del ml-git repository store add (DEPRECATED) Usage: ml-git repository store add [OPTIONS] BUCKET_NAME [DEPRECATED]: Add a storage BUCKET_NAME to ml-git Options: --credentials TEXT Profile name for storage credentials --region TEXT Aws region name for S3 bucket --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --endpoint-url TEXT Storage endpoint url -g, --global Use this option to set configuration at global level --verbose Debug mode Example: $ ml-git repository store add minio --endpoint-url=<minio-endpoint-url> Use this command to add a data storage to a ml-git project. Note: Command deprecated, use storage instead store. ml-git repository store del (DEPRECATED) Usage: ml-git repository store del [OPTIONS] BUCKET_NAME [DEPRECATED]: Delete a store BUCKET_NAME from ml-git Options: --type [s3h|s3|azureblobh|gdriveh] Store type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --help Show this message and exit. Example: $ ml-git repository store del minio Note: Command deprecated, use storage instead store. ml-git repository storage add Usage: ml-git repository storage add [OPTIONS] BUCKET_NAME Add a storage BUCKET_NAME to ml-git Options: --credentials TEXT Profile name for storage credentials --region TEXT Aws region name for S3 bucket --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --endpoint-url TEXT Storage endpoint url -g, --global Use this option to set configuration at global level --verbose Debug mode Example: $ ml-git repository storage add minio --endpoint-url=<minio-endpoint-url> Use this command to add a data storage to a ml-git project. ml-git repository storage del Usage: ml-git repository storage del [OPTIONS] BUCKET_NAME Delete a storage BUCKET_NAME from ml-git Options: --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --help Show this message and exit. Example: $ ml-git repository storage del minio ml-git repository update Usage: ml-git repository update This command updates the metadata for all entities. Example: $ ml-git repository update","title":"Commands"},{"location":"mlgit_commands/#ml-git-commands","text":"ml-git --help Usage: ml-git [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. Commands: clone Clone a ml-git repository ML_GIT_REPOSITORY_URL dataset management of datasets within this ml-git repository labels management of labels sets within this ml-git repository login login command generates new Aws credential. model management of models within this ml-git repository repository Management of this ml-git repository Example: $ ml-git --help ml-git --version Displays the installed version of ML-Git. ml-git <ml-entity> add Usage: ml-git dataset add [OPTIONS] ML_ENTITY_NAME [FILE_PATH]... Add dataset change set ML_ENTITY_NAME to the local ml-git staging area. Options: --bumpversion Increment the version number when adding more files. --fsck Run fsck after command execution. --help Show this message and exit. Example: $ ml-git dataset add dataset-ex --bumpversion ml-git expects datasets to be managed under dataset directory. \\<ml-entity-name> is also expected to be a repository under the tree structure and ml-git will search for it in the tree. Under that repository, it is also expected to have a \\<ml-entity-name>.spec file, defining the ML entity to be added. Optionally, one can add a README.md which will describe the dataset and be what will be shown in the github repository for that specific dataset. Internally, the ml-git add will add all the files under the \\<ml-entity> directory into the ml-git index / staging area. ml-git <ml-entity> branch Usage: ml-git dataset branch [OPTIONS] ML_ENTITY_NAME This command allows to check which tag is checked out in the ml-git workspace. Options: --help Show this message and exit. Example: $ ml-git dataset branch imagenet8 ('vision-computing__images__imagenet8__1', '48ba1e994a1e39e1b508bff4a3302a5c1bb9063e') That information is equal to the HEAD reference from a git concept. ml-git keeps that information on a per \\<ml-entity-name> basis. which enables independent checkout of each of these \\<ml-entity-name>. The output is a tuple: 1) the tag auto-generated by ml-git based on the \\<ml-entity-name>.spec (composite with categories, \\<ml-entity-name>, version) 2) the sha of the git commit of that \\<ml-entity> version Both are the same representation. One is human-readable and is also used internally by ml-git to find out the path to the referenced \\<ml-entity-name>. ml-git <ml-entity> checkout Usage: ml-git model checkout [OPTIONS] ML_ENTITY_TAG|ML_ENTITY Checkout the ML_ENTITY_TAG|ML_ENTITY of a model set into user workspace. Options: -l, --with-labels The checkout associated labels in user workspace as well. -d, --with-dataset The checkout associated dataset in user workspace as well. --retry INTEGER Number of retries to download the files from the storage [default: 2]. --force Force checkout command to delete untracked/uncommitted files from local repository. --bare Ability to add/commit/push without having the ml- entity checked out. --version INTEGER Number of artifact version to be downloaded [default: latest]. --verbose Debug mode Examples: $ ml-git dataset checkout computer-vision__images__faces__fddb__1 or you can use the name of the entity directly and download the latest available tag $ ml-git dataset checkout fddb Note: --d: It can only be used in checkout of labels and models to get the entities that are associated with the entity. --l: It can only be used in checkout of models to get the label entity that are associated with the entity. --sample-type, --sampling, --seed: These options are available only for dataset. If you use this option ml-git will not allow you to make changes to the entity and create a new tag. ml-git <ml-entity> commit Usage: ml-git model commit [OPTIONS] ML_ENTITY_NAME Commit model change set of ML_ENTITY_NAME locally to this ml-git repository. Options: --dataset TEXT Link dataset entity name to this model set version. --labels TEXT Link labels entity name to this model set version. --tag TEXT Ml-git tag to identify a specific version of a ML entity. --version-number, --version INTEGER RANGE Set the number of artifact version. [DEPRECATED:--version-number] -m, --message TEXT Use the provided <msg> as the commit message. --fsck TEXT Run fsck after command execution. --verbose Debug mode Example: $ ml-git model commit model-ex --dataset=dataset-ex This command commits the index / staging area to the local repository. It is a 2-step operation in which 1) the actual data (blobs) is copied to the local repository, 2) committing the metadata to the git repository managing the metadata. Internally, ml-git keeps track of files that have been added to the data store and is storing that information to the metadata management layer to be able to restore any version of each \\<ml-entity-name>. Another important feature of ml-git is the ability to keep track of the relationship between the ML entities. So when committing a label set, one can (should) provide the option --dataset=<dataset-name> . Internally, ml-git will inspect the HEAD / ref of the specified \\<dataset-name> checked out in the ml-git repository and will add that information to the specificatino file that is committed to the metadata repository. With that relationship kept into the metadata repository, it is now possible for anyone to checkout exactly the same versions of labels and dataset. Same for ML model, one can specify which dataset and label set that have been used to generate that model through --dataset=<dataset-name> and --labels=<labels-name> ml-git <ml-entity> create Usage: ml-git dataset create [OPTIONS] ARTIFACT_NAME This command will create the workspace structure with data and spec file for an entity and set the git and store configurations. Options: --category TEXT Artifact's category name. [required] --mutability [strict|flexible|mutable] Mutability type. [required] --store-type, --storage-type [s3h|azureblobh|gdriveh] Data storage type [default: s3h]. [DEPRECATED:--store-type] --version-number, --version INTEGER RANGE Number of artifact version. [DEPRECATED:--version-number] --import TEXT Path to be imported to the project. NOTE: Mutually exclusive with argument: credentials_path, import_url. --wizard-config If specified, ask interactive questions. at console for git & store configurations. --bucket-name TEXT Bucket name --import-url TEXT Import data from a google drive url. NOTE: Mutually exclusive with argument: import. --credentials-path TEXT Directory of credentials.json. NOTE: This option is required if --import-url is used. --unzip Unzip imported zipped files. Only available if --import-url is used. --verbose Debug mode Examples: - To create an entity with s3 as storage and importing files from a path of your computer: ml-git dataset create imagenet8 --storage-type=s3h --category=computer-vision --category=images --version=0 --import='/path/to/dataset' --mutability=strict To create an entity with s3 as storage and importing files from a google drive URL: ml-git dataset create imagenet8 --storage-type=s3h --category=computer-vision --category=images --import-url='gdrive.url' --credentials-path='/path/to/gdrive/credentials' --mutability=strict --unzip ml-git <ml-entity> export Usage: ml-git dataset export [OPTIONS] ML_ENTITY_TAG BUCKET_NAME This command allows you to export files from one store (S3|MinIO) to another (S3|MinIO). Options: --credentials TEXT Profile of AWS credentials [default: default]. --endpoint TEXT Endpoint where you want to export --region TEXT AWS region name [default: us-east-1]. --retry INTEGER Number of retries to upload or download the files from the storage [default: 2]. --help Show this message and exit. Example: $ ml-git dataset export computer-vision__images__faces__fddb__1 minio ml-git <ml-entity> fetch Usage: ml-git dataset fetch [OPTIONS] ML_ENTITY_TAG Allows you to download just the metadata files of an entity. Options: --sample-type [group|range|random] --sampling TEXT The group: <amount>:<group> The group sample option consists of amount and group used to download a sample. range: <start:stop:step> The range sample option consists of start, stop and step used to download a sample. The start parameter can be equal or greater than zero.The stop parameter can be 'all', -1 or any integer above zero. random: <amount:frequency> The random sample option consists of amount and frequency used to download a sample. --seed TEXT Seed to be used in random-based samplers. --retry INTEGER Number of retries to download the files from the storage [default: 2]. --help Show this message and exit. Example: ml-git dataset fetch computer-vision__images__faces__fddb__1 ml-git <ml-entity> fsck Usage: ml-git dataset fsck [OPTIONS] Perform fsck on dataset in this ml-git repository. Options: --help Show this message and exit. Example: $ ml-git dataset fsck This command will walk through the internal ml-git directories (index & local repository) and will check the integrity of all blobs under its management. It will return the list of blobs that are corrupted. Note: in the future, fsck should be able to fix some errors of detected corruption. ml-git <ml-entity> import Usage: ml-git dataset import [OPTIONS] BUCKET_NAME ENTITY_DIR This command allows you to download a file or directory from the S3 bucket or Gdrive to ENTITY_DIR. Options: --credentials TEXT Profile of AWS credentials [default: default]. --region TEXT AWS region name [default: us-east-1]. --retry INTEGER Number of retries to download the files from the storage [default: 2]. --path TEXT Bucket folder path. --object TEXT Filename in bucket. --store-type, --storage-type [s3|gdrive] Data storage type [default: s3h]. [DEPRECATED:--store-type] --endpoint-url Storage endpoint url. --help Show this message and exit. Example: $ ml-git dataset import bucket-name dataset/computer-vision/imagenet8/data For google drive store: $ ml-git dataset import gdrive-folder --store-type=gdrive --object=file_to_download --credentials=credentials-path dataset/ ml-git <ml-entity> init Usage: ml-git dataset init [OPTIONS] Init a ml-git dataset repository. Options: --help Show this message and exit. Example: $ ml-git dataset init This command is mandatory to be executed just after the addition of a remote metadata repository ( ml-git \\<ml-entity> remote add ). It initializes the metadata by pulling all metadata to the local repository. ml-git <ml-entity> list Usage: ml-git dataset list [OPTIONS] List dataset managed under this ml-git repository. Options: --help Show this message and exit. Example: $ ml-git dataset list ML dataset |-- computer-vision | |-- images | | |-- dataset-ex-minio | | |-- imagenet8 | | |-- dataset-ex ml-git <ml-entity> log Usage: ml-git dataset log [OPTIONS] ML_ENTITY_NAME This command shows ml-entity-name's commit information like author, date, commit message. Options: --stat Show amount of files and size of an ml-entity. --fullstat Show added and deleted files. --help Show this message and exit. Example: ml-git dataset log dataset-ex ml-git <ml-entity> push Usage: ml-git dataset push [OPTIONS] ML_ENTITY_NAME Push local commits from ML_ENTITY_NAME to remote ml-git repository & store. Options: --retry INTEGER Number of retries to upload or download the files from the storage [default: 2]. --clearonfail Remove the files from the store in case of failure during the push operation. --help Show this message and exit. Example: ml-git dataset push dataset-ex This command will perform a 2-step operations: 1. push all blobs to the configured data store. 2. push all metadata related to the commits to the remote metadata repository. ml-git <ml-entity> remote-fsck Usage: ml-git dataset remote-fsck [OPTIONS] ML_ENTITY_NAME This command will check and repair the remote by uploading lacking chunks/blobs. Options: --thorough Try to download the IPLD if it is not present in the local repository to verify the existence of all contained IPLD links associated. --paranoid Adds an additional step that will download all IPLD and its associated IPLD links to verify the content by computing the multihash of all these. --retry INTEGER Number of retries to download the files from the storage [default: 2]. --help Show this message and exit. Example: ml-git dataset remote-fsck dataset-ex This ml-git command will basically try to: Detects any chunk/blob lacking in a remote store for a specific ML artefact version Repair - if possible - by uploading lacking chunks/blobs In paranoid mode, verifies the content of all the blobs ml-git <ml-entity> reset Usage: ml-git dataset reset [OPTIONS] ML_ENTITY_NAME Reset ml-git state(s) of an ML_ENTITY_NAME Options: --hard Remove untracked files from workspace, files to be committed from staging area as well as committed files upto <reference>. --mixed Revert the committed files and the staged files to 'Untracked Files'. This is the default action. --soft Revert the committed files to 'Changes to be committed'. --reference [head|head~1] head:Will keep the metadata in the current commit. head~1:Will move the metadata to the last commit. --help Show this message and exit. Examples: ml-git reset --hard Undo the committed changes. Undo the added/tracked files. Reset the workspace to fit with the current HEAD state. ml-git reset --mixed if HEAD: * nothing happens. else: * Undo the committed changes. * Undo the added/tracked files. ml-git reset --soft if HEAD: * nothing happens. else: * Undo the committed changes. ml-git <ml-entity> show Usage: ml-git dataset show [OPTIONS] ML_ENTITY_NAME Print the specification file of the entity. Options: --help Show this message and exit. Example: $ ml-git dataset show dataset-ex -- dataset : imagenet8 -- categories: - vision-computing - images manifest: files: MANIFEST.yaml store: s3h://mlgit-datasets name: imagenet8 version: 1 ml-git <ml-entity> status Usage: ml-git dataset status [OPTIONS] ML_ENTITY_NAME [STATUS_DIRECTORY] Print the files that are tracked or not and the ones that are in the index/staging area. Options: --full Show all contents for each directory. --verbose Debug mode Example: $ ml-git dataset status dataset-ex ml-git <ml-entity> tag add Usage: ml-git dataset tag add [OPTIONS] ML_ENTITY_NAME TAG Use this command to associate a tag to a commit. Options: --help Show this message and exit. Example: $ ml-git dataset tag add dataset-ex my_tag ml-git <ml-entity> tag list Usage: ml-git dataset tag list [OPTIONS] ML_ENTITY_NAME List tags of ML_ENTITY_NAME from this ml-git repository. Options: --help Show this message and exit. Example: $ ml-git dataset tag list dataset-ex ml-git <ml-entity> update Usage: ml-git dataset update [OPTIONS] This command will update the metadata repository. Options: --help Show this message and exit. Example: $ ml-git dataset update This command enables one to have the visibility of what has been shared since the last update (new ML entity, new versions). ml-git <ml-entity> unlock Usage: ml-git dataset unlock [OPTIONS] ML_ENTITY_NAME FILE This command add read and write permissions to file or directory. Note: You should only use this command for the flexible mutability option. Options: --help Show this message and exit. Example: $ ml-git dataset unlock dataset-ex data/file1.txt Note: You should only use this command for the flexible mutability option. ml-git clone <repository-url> Usage: ml-git clone [OPTIONS] REPOSITORY_URL Clone a ml-git repository ML_GIT_REPOSITORY_URL Options: --folder TEXT --track --help Show this message and exit. Example: $ ml-git clone https://git@github.com/mlgit-repository ml-git login Usage: ml-git login [OPTIONS] login command generates new Aws credential. Options: --credentials TEXT profile name for store credentials [default: default]. --insecure use this option when operating in a insecure location. This option prevents storage of a cookie in the folder. Never execute this program without --insecure option in a compute device you do not trust. --rolearn TEXT directly STS to this AWS Role ARN instead of the selecting the option during runtime. --help Show this message and exit. Example: ml-git login Note: ml-git repository config Usage: ml-git repository config [OPTIONS] Configuration of this ml-git repository Options: --help Show this message and exit. Example: $ ml-git repository config config: {'dataset': {'git': 'git@github.com:example/your-mlgit-datasets'}, 'store': {'s3': {'mlgit-datasets': {'aws-credentials': {'profile': 'mlgit'}, 'region': 'us-east-1'}}}, 'verbose': 'info'} Use this command if you want to check what configuration ml-git is running with. It is highly likely one will need to change the default configuration to adapt for her needs. ml-git repository gc Usage: ml-git repository gc [OPTIONS] Cleanup unnecessary files and optimize the use of the disk space. Options: --verbose Debug mode This command will remove unnecessary files contained in the cache and objects directories of the ml-git metadata (.ml-git). ml-git repository init Usage: ml-git repository init [OPTIONS] Initialiation of this ml-git repository Options: --help Show this message and exit. Example: $ ml-git repository init This is the first command you need to run to initialize a ml-git project. It will bascially create a default .ml-git/config.yaml ml-git repository remote <ml-entity> add Usage: ml-git repository remote dataset add [OPTIONS] REMOTE_URL Add remote dataset metadata REMOTE_URL to this ml-git repository Options: --help Show this message and exit. Example: $ ml-git repository remote dataset add https://git@github.com/mlgit-datasets ml-git repository remote <ml-entity> del Usage: ml-git repository remote dataset del Remove remote dataset metadata REMOTE_URL from this ml-git repository Options: --help Show this message and exit. Example: $ ml-git repository remote dataset del ml-git repository store add (DEPRECATED) Usage: ml-git repository store add [OPTIONS] BUCKET_NAME [DEPRECATED]: Add a storage BUCKET_NAME to ml-git Options: --credentials TEXT Profile name for storage credentials --region TEXT Aws region name for S3 bucket --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --endpoint-url TEXT Storage endpoint url -g, --global Use this option to set configuration at global level --verbose Debug mode Example: $ ml-git repository store add minio --endpoint-url=<minio-endpoint-url> Use this command to add a data storage to a ml-git project. Note: Command deprecated, use storage instead store. ml-git repository store del (DEPRECATED) Usage: ml-git repository store del [OPTIONS] BUCKET_NAME [DEPRECATED]: Delete a store BUCKET_NAME from ml-git Options: --type [s3h|s3|azureblobh|gdriveh] Store type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --help Show this message and exit. Example: $ ml-git repository store del minio Note: Command deprecated, use storage instead store. ml-git repository storage add Usage: ml-git repository storage add [OPTIONS] BUCKET_NAME Add a storage BUCKET_NAME to ml-git Options: --credentials TEXT Profile name for storage credentials --region TEXT Aws region name for S3 bucket --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --endpoint-url TEXT Storage endpoint url -g, --global Use this option to set configuration at global level --verbose Debug mode Example: $ ml-git repository storage add minio --endpoint-url=<minio-endpoint-url> Use this command to add a data storage to a ml-git project. ml-git repository storage del Usage: ml-git repository storage del [OPTIONS] BUCKET_NAME Delete a storage BUCKET_NAME from ml-git Options: --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --help Show this message and exit. Example: $ ml-git repository storage del minio ml-git repository update Usage: ml-git repository update This command updates the metadata for all entities. Example: $ ml-git repository update","title":"ML-Git commands"},{"location":"mlgit_internals/","text":"ML-Git: architecture and internals \u00b6 Metadata & data decoupling \u00b6 ML-Git's first design concept is to decouple the ML entities' metadata management from the actual data. So, the tool has two main layers: The metadata management, is responsible for organize the ML entities (Models, Datasets, and Labels) through specification files. Then, these files are managed by a git repository to store and retrieve versions of the ML entities. The data store, is responsible to keep the files of the ML entities. Figure 1. Decoupling Metadata & Data Management Layers Content Addressable Storage for ML-Git \u00b6 ML-Git has been implemented as a Content Addressable Storage (CAS), meaning that we can retrieve the information based on the content and not based on the information's location. Figure 2. Self-Describing Content-Addressed ID Figure 2 shows the basic principle of multihash to obtain a Content Identifier (CID) which is used under the hood by ML-Git to implement its CAS layer. In a nutshell, CID is a self-describing content-addressed identifier that enables natural evolution and customization over simple and fixed cryptographic hashing schemes. An argument why multihash is a valuable feature is that any cryptographic function ultimately ends being weak. It's been a challenge for many software to use another cryptographic hash (including git). For example, when collisions have been proven with SHA-1. Summarizing, a CID is: A unique identifier/hash of \u201cmultihash\u201d content. Encoding the digest of the original content enabling anyone to retrieve thatcontent wherever it lies (through some routing). Enabling the integrity check of the retrieved content (thx to multihash and the encoded digest). Figure 3. IPLD - CID for a file There are a few steps to chunk a file to get an IPLD - CID format: Slide the file in piece of, say, 256KB. For each slice, compute its digest (currently, ml-git uses sha2-256). Obtain the CID for all these digests. These slice of files will be saved in a data store with the computed CID as their filename. Build a json describing all the chunks of the file. Obtain the CID of that json. That json will also be saved in the data store with the computed CID as its filename. Note that this last CID is the only piece of information you need to keep to retrieve the whole image.jpg file. And last but not least, one can ensure the integrity of the file while downloading by computing the digests of all downloaded chunks and checking against the digest encoded in their CID. Below, you can find useful links for more information on: Multihash CID IPLD Why slicing files in chunks? \u00b6 IPFS uses small chunk size of 256KB \u2026 Why? security - easy to DOS nodes without forcing small chunks deduplication - small chunks can dedup. big ones effectively dont. latency - can externalize small pieces already (think a stream) bandwidth - optimize the use of bandwidth across many peers performance - better perf to hold small pieces in memory. Hash along the dag to verify integrity of the whole thing. The big DOS problem with huge leaves is that malicious nodes can serve bogus stuff for a long time before a node can detect the problem (imagine having to download 4GB before you can check whether any of it is valid). This was super harmful for bittorrent (when people started choosing huge piece sizes), attackers would routinely do this, very cheaply - just serve bogus random data. This is why smaller chunks are used in our approach. ML-Git high-level architecture and metadata \u00b6 Figure 4. ML-Git high-level architecture and metadata relationships So IPLD/CID has been implemented on top of the S3 driver. The chunking strategy is a recommendation to turn S3 interactions more efficient when dealing with large files. It's also interesting to note that if ML-Git implements a Thread pool to concurrently upload & download files to a S3 bucket. Last, it would be possible to further accelerate ML-Git interactions with a S3 bucket through the AWS CloudFront. (not implemented yet) ML-Git baseline performance numbers \u00b6 CamSeq01 under ML-Git \u00b6 CamSeq01 size : 92MB Locations: website in Cambridge -- S3 bucket in us-east-1 -- me in South Brazil Download from website: ~4min22s Upload to S3 with ml-git : 6m49s Download to S3 with ml-git : 1m11s MSCoco (all files) under ML-Git \u00b6 MSCoco : Size : 26GB Number of files : 164065 ; chunked into ~400-500K blobs (todo: exact blob count) Locations: original dataset: unknown -- S3 bucket in us-east-1 -- me in South Brazil Download from website: unknown Upload to S3 with ml-git : 12h30m Download to S3 with ml-git : 10h45m MSCoco (zip files) under ML-Git \u00b6 MSCoco : Size : 25GB number of files : 3 (train.zip, test.zip, val.zip) ; 102299 blobs Locations: original dataset: unknown -- S3 bucket in us-east-1 -- me in South Brazil Download from website: unknown Upload to S3 with ml-git : 4h35m Download to S3 with ml-git : 3h39m A couple of comments: Even though Python GIL is a challenge for true concurrency in the Python interpreter, it still is very helpful and provides a significant improvement for ML-Git performance. Not surprisingly, the number of files will affect the overall performance as it means there will be many more connections to AWS. However, ML-Git have an option to download some dataset partially (checkout with sampling) to enable CI/CD workflows for which some ML engineers may run some experiments locally on their own machine. For that reason, it is interesting to avoid downloading the full dataset if it's very large. This option is not applicable if the data set was loaded as some zip files. ML-Git add, commit, push commands internals \u00b6 Figure 5. ML-Git commands internals Description ML-Git internal commands \u00b6 Commands : \u00b6 ml-git --help Display help information about ML-Git commands. ml-git --version Show version passed as parameter in click function. ml-git <ml-entity> add ml-git add search for metadata (.spec file) inside ml-git index corresponding to ml-entity-name (mandatory use): ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 <ml-entity-name>/ <-- Search .spec file | \u251c\u2500\u2500 <ml-entity-name>.spec \u2514\u2500\u2500 <ml-entity>/ Then compares the tag of .spec file with the tag of git repository: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 <ml-entity-name>.spec | \u2514\u2500\u2500 metadata/ <- Check tag in git repository \u2514\u2500\u2500 <ml-entity>/ If the ml-git tag doesn't exist in git repository, ml-git create INDEX.yaml and MANIFEST.yaml : ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 <ml-entity-name>.spec | | \u251c\u2500\u2500 INDEX.yaml <-- INDEX.yaml created. | | \u251c\u2500\u2500 MANIFEST.yaml < -- Manifest created. | \u2514\u2500\u2500 metadata/ <- Check tag in git repository \u2514\u2500\u2500 <ml-entity>/ The content of MANIFEST.yaml is a set of added multihash's files. Then ml-git caches the file with hard links in cache path and add chunked files in objects : ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 <entity-name>.spec | | \u251c\u2500\u2500 INDEX.yaml <-- INDEX.yaml created. | | \u251c\u2500\u2500 MANIFEST.yaml < -- Manifest created | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 cache/ | \u2514\u2500\u2500 hashfs/ <- Hard link of chunked files | \u2514\u2500\u2500 objects/ | | \u2514\u2500\u2500 hashfs/ <-- Chunk files \u2514\u2500\u2500 <ml-entity>/ MANIFESTEST.yaml structure example: zdj7WWMZbq7cgw76BGeqoNUGFRkyw59p4Y6zD5eb8cyWL6MW5: !!set data/1.jpg: null zdj7WWgUF7spcvxkBEN49gh44ZUMzbYMG9Mm5RPGU8bsXEDTu: !!set data/test.txt: null zdj7WX8pZHGEAHXuzdJc2dwRXpyABuZznSx3BW867DA53Vksf: !!set data/8.jpg: null zdj7WYF38pFqHrvQPnD3FXMw76UDbMaZkSXJ4qMZci1nxWqiU: !!set data/2.jpg: null INDEX.yaml structure example: data/1.jpg: ctime: 1582208519.35017 <-- Creation time. hash: zdj7WWMZbq7cgw76BGeqoNUGFRkyw59p4Y6zD5eb8cyWL6MW5 mtime: 1582208519.3581703 <-- Modification time. status: a <-- Status file, (a, u, c) data/test.txt: ctime: 1582208519.3521693 hash: zdj7WWgUF7spcvxkBEN49gh44ZUMzbYMG9Mm5RPGU8bsXEDTu mtime: 1582208519.3561785 status: a data/8.jpg: ctime: 1582208519.3531702 hash: zdj7WX8pZHGEAHXuzdJc2dwRXpyABuZznSx3BW867DA53Vksf mtime: 1582208519.4149985 status: a data/2.jpg: ctime: 1582208519.3551724 hash: zdj7WYF38pFqHrvQPnD3FXMw76UDbMaZkSXJ4qMZci1nxWqiU mtime: 1582208519.5029979 status: a ml-git <ml-entity> branch Search for HEAD file in: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 cache/ | \u2514\u2500\u2500 refs/ | \u2514\u2500\u2500 <ml-entity-name>/ | \u251c\u2500\u2500 HEAD <-- Search here. \u2514\u2500\u2500 <ml-entity>/ Parse HEAD file as yaml and list the tags and their corresponding SHA-1. HEAD structure example: computer-vision__images__imagenet8__1: 00da0d518914cfaeb765633f68ade09a5d80b252 ml-git <ml-entity> checkout ml-git (dataset|labels|model) checkout ML_ENTITY_TAG|ML_ENTITY You can use this command by passing a specific tag or just the name of the entity as an argument. If you use the name of the entity, ml-git will checkout the latest available version of that entity. If you use a tag, the ml-git break up the ML_ENTITY into categories, specname and version, if the ML_ENTITY_TAG is the current tag, the command show the message \"Repository: already at tag [\\<ml-entity-tag>]\" , otherwise execute git checkout to the ML_ENTITY_TAG , then verify if cache has tag's objects: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 cache/ \u2514\u2500\u2500 hashfs/ <-- Objects here When objects not found in cache, the command download the blobs from data store to the workspace: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 cache/ | \u2514\u2500\u2500 hashfs/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 <categories>*/ \u2514\u2500\u2500 <ml-entity-name>/ < -- Workspace When objects is found in cache, the command update the objects hard link to the workspace: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 cache/ <-- Check here \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 <categories>*/ \u2514\u2500\u2500 <ml-entity-name>/ <-- Update here Then update the HEAD with ML_ENTITY_TAG and SHA-1, then execute git checkout to branch master . ml-git \\<ml-entity> checkout \\<ml-entity-tag> [--sample-type=\\<sample>] [--sampling] [--seed] \u00b6 This command has three types of sampling options available only for dataset: --sample-type=group --seed , --sample-type=random --seed , --sample-type=range . We use random.sample(population, k) to return a sample of the size k from the population elements. We use random.seed() to set the seed so that the sample generated by random.sample() can be reproduced between experiments. We use the range() object to take samples from a given range. Note: If you use this option ml-git will not allow you to make changes to the entity and create a new tag. Exemple: \u00b6 Let's assume that we have a dataset that contains 12 files. ml-git dataset checkout computer-vision__images__dataset-ex__22 --sample-type=group --sampling=2:5 --seed=1 : This command selects 2 files randomly from every group of five files to download. ml-git dataset checkout computer-vision__images__dataset-ex__22 --sample-type=random --sampling=2:6 --seed=1 : This command makes a sample = (amount * len (dataset))% frequency ratio, sample = 4, so four files are selected randomly to download. ml-git dataset checkout computer-vision__images__dataset-ex__22 --sample-type=range --sampling=2:11:2 : This command selects the files at indexes generated by range(start=2, stop=11, step=2) . ml-git <ml-entity> commit Firstly commit verifies ml-git tag existence, then updates status file in .ml-git/<ml-entity>/index/metadata/<ml-entity-name>/INDEX.yaml and merge the metadata .ml-git/<ml-entity>/index/metadata/<ml-entity-name>/MAFINEST.yaml with .ml-git/<ml-entity>/metadata/<ml-entity-name>/MAFINEST.yaml . Update INDEX.yaml : data/1.jpg: ctime: 1582208519.35017 hash: zdj7WWMZbq7cgw76BGeqoNUGFRkyw59p4Y6zD5eb8cyWL6MW5 mtime: 1582208519.3581703 status: a <- Change status 'a' to 'u'. data/10.jpg: ctime: 1582208519.3561785 hash: zdj7WZrTe7SU5oFQc8kr1kNiAkb5TBeMP1vgcXM1fvfgn5jq5 mtime: 1582208519.6050372 status: u data/2.jpg: ctime: 1582208519.3551724 hash: zdj7WYF38pFqHrvQPnD3FXMw76UDbMaZkSXJ4qMZci1nxWqiU mtime: 1582208519.5029979 status: u Merge the metadata .ml-git/<ml-entity>/index/metadata/<ml-entity-name>/MAFINEST.yaml with .ml-git/<ml-entity>/metadata/<ml-entity-name>/MAFINEST.yaml : ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 MANIFEST.yaml < -- (1) Get data from here | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 <categopries>*/ | \u251c\u2500\u2500 MANIFEST.yaml < -- Union data (1) here, and delete (1). \u2514\u2500\u2500 <ml-entity>/ Get content of \\<ml-entity-name>.spec (structure with representational values): dataset: categories: - computer-vision - images mutability: strict manifest: store: s3h://mlgit-datasets name: imagenet8 version: 1 And insert new attribute: dataset: categories: - computer-vision - images mutability: strict manifest: files: MANIFEST.yaml store: s3h://mlgit-datasets name: imagenet8 version: 1 Then save file in: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 MANIFEST.yaml | | \u251c\u2500\u2500 <ml-entity-name>.spec < -- Copy content and change | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 <categopries>*/ | \u251c\u2500\u2500 MANIFEST.yaml | \u251c\u2500\u2500 <ml-entity-name>.spec < -- Save here \u2514\u2500\u2500 <ml-entity>/ After committing the .spec file and MANIFEST.yaml, ml-git updates the HEAD of repository with tag's SHA-1. HEAD structure: computer-vision__images__imagenet8__1: 00da0d518914cfaeb765633f68ade09a5d80b252 HEAD directory: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <entity-name>/ | | \u251c\u2500\u2500 MANIFEST.yaml | | \u251c\u2500\u2500 <entity-name>.spec < -- Copy content and change | \u2514\u2500\u2500 refs/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 HEAD <-- Update tag with SHA-1 here. | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 <categopries>*/ | \u251c\u2500\u2500 MANIFEST.yaml | \u251c\u2500\u2500 <ml-entity-name>.spec < -- Save here \u2514\u2500\u2500 <ml-entity>/ * Categories path is a tree of categories paths described in .spec file. (Ex: categories/images/MANIFEST.yaml) . ml-git <ml-entity> create ml-git (dataset|labels|model) create ARTEFACT_NAME Create the the workspace structure as follow: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500config.yaml <ml-entity> \u2514\u2500\u2500 ARTEFACT_NAME/ \u2514\u2500\u2500 data/ \u2514\u2500\u2500 ARTEFACT_NAME.spec \u2514\u2500\u2500 README.md The parameters passed --category and --version are used to fill the spec file. The parameter --mutability must be used to define the entity's mutability, which can be: strict, flexible, mutable. If you want to know more about each type of mutability and how it works, please take a look at mutability helper documentation . The parameter --import is used to import files from a src folder to data folder. The optional parameter --wizard-questions if passed, ask interactive questions at console for git & store configurations and update the config.yaml file. The parameter --store-type must be used to define the entity's storage, which can be: s3h, azureblobh, gdriveh. The parameter --import-url is used to import files from Google Drive to data folder. Using this option it will be necessary to inform the path to your google drive credentials through the credentials-path argument. In addition, you can use the --unzip option to unzip the files imported. ml-git <ml-entity> export This command allows you to export files from one store (S3|MinIO) to another (S3|MinIO). ml-git (dataset|labels|model) export ML_ENTITY_TAG BUCKET_NAME Initially, it checks if the user is in an initialized ml-git project. With the entity tag , --credentials , --region , --endpoint , --retry and bucket name arguments, ml-git connects to the store (S3|MinIO) bucket. Then the files are exported to the target store (S3|MinIO) bucket. ml-git <ml-entity> fetch ml-git (dataset|labels|model) fetch ML_ENTITY_TAG Break up the ML_ENTITY_TAG into categories, specname and version, then verify if cache has tag's objects, if not, download the blobs. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 cache/ \u2514\u2500\u2500 hashfs/ <-- Objects here ml-git <ml-entity> fsck Reads objects in: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 hashfs/ <-- Objects here \u2514\u2500\u2500 objects/ \u2514\u2500\u2500 hashfs/ <-- Objects here Applies SHA2 to content of objects , uses multihash to generate the CID, and compares the CID with the file name, if it is different it mean that the file is corrupted, so ml-git fsck show the number of corrupted files and in which directory. When object is valid but not in originally directory, ml-git accept that it's corrupted. ml-git <ml-entity> import This command allows you to download a file or directory from the S3 bucket. ml-git (dataset|labels|model) import BUCKET_NAME ENTITY_DIR Initially checks if the user is in an initialized ml-git project. With the --credentials, --region (optional), --path and bucket name arguments ml-git connects to the S3 bucket. The S3 files for the file or directory specified in --path or --object will be downloaded. The files will be saved in the directory specified by the user in ENTITY_DIR, if not exists, the path will be created. ml-git <ml-entity> init When ml-git init is executed, it will read .ml-git/config.yaml to get the git repository url. ml-git will create directory .ml-git/ \\<ml-entity>/metadata if doesn't exists and clone the repository into it. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 metadata/ <-- The example command clone git repository here. Obs: Must have executed ml-git init before, to create ml-git initial configuration files. ml-git <ml-entity> list That command will list all \\<ml-entity> under management in the ml-git repository. To do this, ml-git goes through the metadata directory to identify the structure of categories and entities that are under management. ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 metadata/ <-- Check here the directory structure | \u2514\u2500\u2500 computer-vision/ | \u2514\u2500\u2500 images/ | \u2514\u2500\u2500 imagenet8/ $ ml-git dataset list ML dataset |-- computer-vision | |-- images | | |-- imagenet8 ml-git <ml-entity> log Usage: ml-git dataset log [OPTIONS] ML_ENTITY_NAME This command shows ml-entity-name's commit information like author, date, commit message. Options: --stat Show amount of files and size of an ml-entity. --fullstat Show added and deleted files. --help Show this message and exit. Example: ml-git dataset log dataset-ex ml-git <ml-entity> push ml-git (dataset|labels|model) push ML_ENTITY_NAME Verify the git global configuration, and try upload objects from local repository to data store creating a thread pool with maximum of ten workers. This process use store configuration from spec file and AWS credentials. .spec file: dataset: categories: - computer-vision - images manifest: store: s3h://mlgit-datasets < -- store configuration name: imagenet8 version: 1 Directory: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 objects/ < -- Files to be uploaded. \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 <ml-entity-name>/ \u251c\u2500\u2500 <ml-entity-name>.spec < -- Spec file with store configuration. After the upload process, ml-git executes git push from local repository .ml-git/dataset/metadata to the remote repository configured in config.yaml . ml-git <ml-entity> remote-fsck Starting point of a remote fsck is to identify all the IPLD files contained in the MANIFEST file associated with the specified artefact spec (\\<ml-artefact-name>) and then executes the following steps: Verify the existence of all these IPLDs in the remote store If one IPLD does not exist and it is present in the local repository, upload it to the remote store If the IPLD is present in the local repository: Open it and identify all blobs associated with that IPLD. Verify the existence of these blobs in the remote store. If one blob does not exist and it is present in the local repository, upload it to the remote store. If the IPLD is NOT present in the local repository and --thorough option is set Download the IPLD Open it and identify all blobs associated with that IPLD. Verify the existence of these blobs in the remote store. If one blob does not exist and it is present in the local repository, upload it to the remote store. [--paranoid] : Paranoid mode adds an additional step that will download all IPLD and its associated IPLD links to verify the content by computing the multihash of all these. [--thorough] : Ml-git will try to download the IPLD if it is not present in the local repository to verify the existence of all contained IPLD links associated. ml-git <ml-entity> reset In ml-git project (as in git) we have three areas to manage and track the changes of the data. The workspace - where the data itself is added, deleted or updated. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500<ml-entity-name> \u2514\u2500\u2500HERE That staged area - Where the changes are added and tracked. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ \u2514\u2500\u2500HERE \u2514\u2500\u2500 metadata/ The committed area - Where the data are packed to push. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500HERE Depending how to commands are passed we manage this three areas accordingly. The Default option is HEAD. ml-git <ml-entity> show Verify tag and SHA-1 in HEAD: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 objects/ \u2514\u2500\u2500 refs/ \u2514\u2500\u2500 <ml-entity-name>/ \u251c\u2500\u2500 HEAD < -- Verify tag If tag was not found, the command return the message \"Local Repository: no HEAD for [\\<ml-entity-name>]\" , otherwise do git checkout to the tag and search for all \\<ml-entity-name>.spec file in: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 <categories>*/ | \u2514\u2500\u2500 <ml-entity-name>/ <-- Search all .spec file here \u2514\u2500\u2500 objects/ \u2514\u2500\u2500 refs/ After found all .spec files the command show each one contents, then execute git checkout to branch master . * Categories path is a tree of categories paths described in .spec file. ml-git <ml-entity> status Displays paths that have differences between the index file and the current HEAD commit, paths that have differences between the working tree and the index file, and paths in the working tree that are not tracked by ML-Git. First is described the files tracked to be commited. Those files are those ones in the manifest file. There are two types: New files - Those files are at the entities directory and hard-linked with those ones at index directory. These files are also listed in manifest file. Deleted file. - Files who was deleted from the entities directory, but still are into the manifest file. ml-git_project/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 <ml-entity-name>/ \u251c\u2500\u2500 <-- Files Checked ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 <metadata>/ | \u2514\u2500\u2500 <ml-entity-name> <-- Hard link poited to files located here | \u2514\u2500\u2500 MANIFEST.yaml <-- Files listed here Then are described the untracked files. These files are located under the entities directory and listed if they have more than one hard-link. ml-git <ml-entity> tag add ml-git (dataset|model|label) tag add dataset-ex my_tag You can use this command to associate a tag to a commit. ml-git <ml-entity> tag list This command lists the tags of an entity. To do this, it access the metadata of an entity to get the git repository and then executes git commands to list local tags. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 .git/ < -- Git repository \u2514\u2500\u2500 objects/ \u2514\u2500\u2500 refs/ ml-git <ml-entity> update Locate metadata directory where is git repository: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 .git < -- Git repository goes here. Then, ML-Git execute \"git pull\" on \"origin\" to update all metadatas from remote repository. ml-git <ml-entity> unlock This command add read and write permissions to file or directory. So that if you are working with a flexible mutability repository you will be allowed to make changes to the unlocked file without making it corrupt. If you are working with a strict repository, changing files is not allowed, so the unlock command is not performed. In the case of a mutable repository, the files are already unlocked for modification, so it is unnecessary to execute the unlock command. You should only use this command for the flexible mutability option. ml-git clone <repository-url> The command clones the git repository which should contain a directory .ml-git , then initialize the metadata according to configurations. ml-git will create directory .ml-git/ \\<ml-entity> /metadata if doesn't exists and clone the repository into it. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 metadata/ Options : --folder : The configuration files are cloned in specified folder. --track : The ml-git clone preserves .git folder in the same directory of cloned configuration files. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 .git ml-git login This command generates new Aws credentials in the /.aws directory. Note: ml-git repository config Command try to load the configurations from the file .ml-git/config.yaml . If the file is found, it will show the configurations read from the file, if not it will show the default configurations in the project. ml-git repository gc ml-git repository gc This command will scan the metadata in each entity's index directory to identify which objects are being used by the user's worskpace. After this check, objects that are not being used and that are contained in the cache and object directories will be removed. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 cache/ | \u2514\u2500\u2500 hashfs/ <-- Objects here \u2514\u2500\u2500 objects/ \u2514\u2500\u2500 hashfs/ <-- Objects here ml-git repository init ml-git repository init verify if the current directory has .ml-git , where configuration files goes, and if doesn't have it, ml-git will create the directory and save config.yaml inside, with the informations provided by a dict in project code. Directory structure : ml-git-project/ \u2514\u2500\u2500 .ml-git/ \u251c\u2500\u2500\u2500 config.yaml config.yaml structure : dataset: git: git@github.com:standel/ml-datasets.git <-- git project url store: s3: <-- store type (AWS) mlgit-datasets: <-- bucket name aws-credentials: profile: mlgit region: us-east-1 ml-git repository remote <ml-entity> add This command load the configuration file .ml-git/config.yaml and change the attribute git to the url specified on arguments, then save it. This command require that you have executed ml-git init before. ml-git repository remote <ml-entity> del This command load the configuration file .ml-git/config.yaml and change the attribute git to empty, the save it. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 .git <- Change actual origin url to a blank url. ml-git repository store add (DEPRECATED) Usage: ml-git repository store add [OPTIONS] BUCKET_NAME [DEPRECATED]: Add a storage BUCKET_NAME to ml-git Options: --credentials TEXT Profile name for storage credentials --region TEXT Aws region name for S3 bucket --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --endpoint-url TEXT Storage endpoint url -g, --global Use this option to set configuration at global level --verbose Debug mode Example: $ ml-git repository store add minio --endpoint-url=<minio-endpoint-url> Use this command to add a data storage to a ML-Git project. Note: Command deprecated, use storage instead store. ml-git repository store del (DEPRECATED) Usage: ml-git repository store del [OPTIONS] BUCKET_NAME [DEPRECATED]: Delete a store BUCKET_NAME from ml-git Options: --type [s3h|s3|azureblobh|gdriveh] Store type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --help Show this message and exit. Example: $ ml-git repository store del minio Note: Command deprecated, use storage instead store. ml-git repository storage add Usage: ml-git repository storage add [OPTIONS] BUCKET_NAME Add a storage BUCKET_NAME to ml-git Options: --credentials TEXT Profile name for storage credentials --region TEXT Aws region name for S3 bucket --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --endpoint-url TEXT Storage endpoint url -g, --global Use this option to set configuration at global level --verbose Debug mode Example: $ ml-git repository storage add minio --endpoint-url=<minio-endpoint-url> Use this command to add a data storage to a ML-Git project. ml-git repository storage del Usage: ml-git repository storage del [OPTIONS] BUCKET_NAME Delete a storage BUCKET_NAME from ml-git Options: --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --help Show this message and exit. Example: $ ml-git repository storage del minio ml-git repository update Usage: ml-git repository update This command updates the metadata for all entities. Example: $ ml-git repository update","title":"Internals"},{"location":"mlgit_internals/#ml-git-architecture-and-internals","text":"","title":"ML-Git: architecture and internals"},{"location":"mlgit_internals/#metadata-data-decoupling","text":"ML-Git's first design concept is to decouple the ML entities' metadata management from the actual data. So, the tool has two main layers: The metadata management, is responsible for organize the ML entities (Models, Datasets, and Labels) through specification files. Then, these files are managed by a git repository to store and retrieve versions of the ML entities. The data store, is responsible to keep the files of the ML entities. Figure 1. Decoupling Metadata & Data Management Layers","title":"Metadata &amp; data decoupling"},{"location":"mlgit_internals/#content-addressable-storage-for-ml-git","text":"ML-Git has been implemented as a Content Addressable Storage (CAS), meaning that we can retrieve the information based on the content and not based on the information's location. Figure 2. Self-Describing Content-Addressed ID Figure 2 shows the basic principle of multihash to obtain a Content Identifier (CID) which is used under the hood by ML-Git to implement its CAS layer. In a nutshell, CID is a self-describing content-addressed identifier that enables natural evolution and customization over simple and fixed cryptographic hashing schemes. An argument why multihash is a valuable feature is that any cryptographic function ultimately ends being weak. It's been a challenge for many software to use another cryptographic hash (including git). For example, when collisions have been proven with SHA-1. Summarizing, a CID is: A unique identifier/hash of \u201cmultihash\u201d content. Encoding the digest of the original content enabling anyone to retrieve thatcontent wherever it lies (through some routing). Enabling the integrity check of the retrieved content (thx to multihash and the encoded digest). Figure 3. IPLD - CID for a file There are a few steps to chunk a file to get an IPLD - CID format: Slide the file in piece of, say, 256KB. For each slice, compute its digest (currently, ml-git uses sha2-256). Obtain the CID for all these digests. These slice of files will be saved in a data store with the computed CID as their filename. Build a json describing all the chunks of the file. Obtain the CID of that json. That json will also be saved in the data store with the computed CID as its filename. Note that this last CID is the only piece of information you need to keep to retrieve the whole image.jpg file. And last but not least, one can ensure the integrity of the file while downloading by computing the digests of all downloaded chunks and checking against the digest encoded in their CID. Below, you can find useful links for more information on: Multihash CID IPLD","title":"Content Addressable Storage for ML-Git"},{"location":"mlgit_internals/#why-slicing-files-in-chunks","text":"IPFS uses small chunk size of 256KB \u2026 Why? security - easy to DOS nodes without forcing small chunks deduplication - small chunks can dedup. big ones effectively dont. latency - can externalize small pieces already (think a stream) bandwidth - optimize the use of bandwidth across many peers performance - better perf to hold small pieces in memory. Hash along the dag to verify integrity of the whole thing. The big DOS problem with huge leaves is that malicious nodes can serve bogus stuff for a long time before a node can detect the problem (imagine having to download 4GB before you can check whether any of it is valid). This was super harmful for bittorrent (when people started choosing huge piece sizes), attackers would routinely do this, very cheaply - just serve bogus random data. This is why smaller chunks are used in our approach.","title":"Why slicing files in chunks?"},{"location":"mlgit_internals/#ml-git-high-level-architecture-and-metadata","text":"Figure 4. ML-Git high-level architecture and metadata relationships So IPLD/CID has been implemented on top of the S3 driver. The chunking strategy is a recommendation to turn S3 interactions more efficient when dealing with large files. It's also interesting to note that if ML-Git implements a Thread pool to concurrently upload & download files to a S3 bucket. Last, it would be possible to further accelerate ML-Git interactions with a S3 bucket through the AWS CloudFront. (not implemented yet)","title":"ML-Git high-level architecture and metadata"},{"location":"mlgit_internals/#ml-git-baseline-performance-numbers","text":"","title":"ML-Git baseline performance numbers"},{"location":"mlgit_internals/#camseq01-under-ml-git","text":"CamSeq01 size : 92MB Locations: website in Cambridge -- S3 bucket in us-east-1 -- me in South Brazil Download from website: ~4min22s Upload to S3 with ml-git : 6m49s Download to S3 with ml-git : 1m11s","title":"CamSeq01 under ML-Git"},{"location":"mlgit_internals/#mscoco-all-files-under-ml-git","text":"MSCoco : Size : 26GB Number of files : 164065 ; chunked into ~400-500K blobs (todo: exact blob count) Locations: original dataset: unknown -- S3 bucket in us-east-1 -- me in South Brazil Download from website: unknown Upload to S3 with ml-git : 12h30m Download to S3 with ml-git : 10h45m","title":"MSCoco (all files) under ML-Git"},{"location":"mlgit_internals/#mscoco-zip-files-under-ml-git","text":"MSCoco : Size : 25GB number of files : 3 (train.zip, test.zip, val.zip) ; 102299 blobs Locations: original dataset: unknown -- S3 bucket in us-east-1 -- me in South Brazil Download from website: unknown Upload to S3 with ml-git : 4h35m Download to S3 with ml-git : 3h39m A couple of comments: Even though Python GIL is a challenge for true concurrency in the Python interpreter, it still is very helpful and provides a significant improvement for ML-Git performance. Not surprisingly, the number of files will affect the overall performance as it means there will be many more connections to AWS. However, ML-Git have an option to download some dataset partially (checkout with sampling) to enable CI/CD workflows for which some ML engineers may run some experiments locally on their own machine. For that reason, it is interesting to avoid downloading the full dataset if it's very large. This option is not applicable if the data set was loaded as some zip files.","title":"MSCoco (zip files) under ML-Git"},{"location":"mlgit_internals/#ml-git-add-commit-push-commands-internals","text":"Figure 5. ML-Git commands internals","title":"ML-Git add, commit, push commands internals"},{"location":"mlgit_internals/#description-ml-git-internal-commands","text":"","title":"Description ML-Git internal commands"},{"location":"mlgit_internals/#commands","text":"ml-git --help Display help information about ML-Git commands. ml-git --version Show version passed as parameter in click function. ml-git <ml-entity> add ml-git add search for metadata (.spec file) inside ml-git index corresponding to ml-entity-name (mandatory use): ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 <ml-entity-name>/ <-- Search .spec file | \u251c\u2500\u2500 <ml-entity-name>.spec \u2514\u2500\u2500 <ml-entity>/ Then compares the tag of .spec file with the tag of git repository: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 <ml-entity-name>.spec | \u2514\u2500\u2500 metadata/ <- Check tag in git repository \u2514\u2500\u2500 <ml-entity>/ If the ml-git tag doesn't exist in git repository, ml-git create INDEX.yaml and MANIFEST.yaml : ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 <ml-entity-name>.spec | | \u251c\u2500\u2500 INDEX.yaml <-- INDEX.yaml created. | | \u251c\u2500\u2500 MANIFEST.yaml < -- Manifest created. | \u2514\u2500\u2500 metadata/ <- Check tag in git repository \u2514\u2500\u2500 <ml-entity>/ The content of MANIFEST.yaml is a set of added multihash's files. Then ml-git caches the file with hard links in cache path and add chunked files in objects : ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 <entity-name>.spec | | \u251c\u2500\u2500 INDEX.yaml <-- INDEX.yaml created. | | \u251c\u2500\u2500 MANIFEST.yaml < -- Manifest created | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 cache/ | \u2514\u2500\u2500 hashfs/ <- Hard link of chunked files | \u2514\u2500\u2500 objects/ | | \u2514\u2500\u2500 hashfs/ <-- Chunk files \u2514\u2500\u2500 <ml-entity>/ MANIFESTEST.yaml structure example: zdj7WWMZbq7cgw76BGeqoNUGFRkyw59p4Y6zD5eb8cyWL6MW5: !!set data/1.jpg: null zdj7WWgUF7spcvxkBEN49gh44ZUMzbYMG9Mm5RPGU8bsXEDTu: !!set data/test.txt: null zdj7WX8pZHGEAHXuzdJc2dwRXpyABuZznSx3BW867DA53Vksf: !!set data/8.jpg: null zdj7WYF38pFqHrvQPnD3FXMw76UDbMaZkSXJ4qMZci1nxWqiU: !!set data/2.jpg: null INDEX.yaml structure example: data/1.jpg: ctime: 1582208519.35017 <-- Creation time. hash: zdj7WWMZbq7cgw76BGeqoNUGFRkyw59p4Y6zD5eb8cyWL6MW5 mtime: 1582208519.3581703 <-- Modification time. status: a <-- Status file, (a, u, c) data/test.txt: ctime: 1582208519.3521693 hash: zdj7WWgUF7spcvxkBEN49gh44ZUMzbYMG9Mm5RPGU8bsXEDTu mtime: 1582208519.3561785 status: a data/8.jpg: ctime: 1582208519.3531702 hash: zdj7WX8pZHGEAHXuzdJc2dwRXpyABuZznSx3BW867DA53Vksf mtime: 1582208519.4149985 status: a data/2.jpg: ctime: 1582208519.3551724 hash: zdj7WYF38pFqHrvQPnD3FXMw76UDbMaZkSXJ4qMZci1nxWqiU mtime: 1582208519.5029979 status: a ml-git <ml-entity> branch Search for HEAD file in: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 cache/ | \u2514\u2500\u2500 refs/ | \u2514\u2500\u2500 <ml-entity-name>/ | \u251c\u2500\u2500 HEAD <-- Search here. \u2514\u2500\u2500 <ml-entity>/ Parse HEAD file as yaml and list the tags and their corresponding SHA-1. HEAD structure example: computer-vision__images__imagenet8__1: 00da0d518914cfaeb765633f68ade09a5d80b252 ml-git <ml-entity> checkout ml-git (dataset|labels|model) checkout ML_ENTITY_TAG|ML_ENTITY You can use this command by passing a specific tag or just the name of the entity as an argument. If you use the name of the entity, ml-git will checkout the latest available version of that entity. If you use a tag, the ml-git break up the ML_ENTITY into categories, specname and version, if the ML_ENTITY_TAG is the current tag, the command show the message \"Repository: already at tag [\\<ml-entity-tag>]\" , otherwise execute git checkout to the ML_ENTITY_TAG , then verify if cache has tag's objects: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 cache/ \u2514\u2500\u2500 hashfs/ <-- Objects here When objects not found in cache, the command download the blobs from data store to the workspace: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 cache/ | \u2514\u2500\u2500 hashfs/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 <categories>*/ \u2514\u2500\u2500 <ml-entity-name>/ < -- Workspace When objects is found in cache, the command update the objects hard link to the workspace: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 cache/ <-- Check here \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 <categories>*/ \u2514\u2500\u2500 <ml-entity-name>/ <-- Update here Then update the HEAD with ML_ENTITY_TAG and SHA-1, then execute git checkout to branch master .","title":"Commands:"},{"location":"mlgit_internals/#ml-git-ml-entity-checkout-ml-entity-tag-sample-typesample-sampling-seed","text":"This command has three types of sampling options available only for dataset: --sample-type=group --seed , --sample-type=random --seed , --sample-type=range . We use random.sample(population, k) to return a sample of the size k from the population elements. We use random.seed() to set the seed so that the sample generated by random.sample() can be reproduced between experiments. We use the range() object to take samples from a given range. Note: If you use this option ml-git will not allow you to make changes to the entity and create a new tag.","title":"ml-git \\&lt;ml-entity> checkout \\&lt;ml-entity-tag>  [--sample-type=\\&lt;sample>] [--sampling] [--seed]"},{"location":"mlgit_internals/#exemple","text":"Let's assume that we have a dataset that contains 12 files. ml-git dataset checkout computer-vision__images__dataset-ex__22 --sample-type=group --sampling=2:5 --seed=1 : This command selects 2 files randomly from every group of five files to download. ml-git dataset checkout computer-vision__images__dataset-ex__22 --sample-type=random --sampling=2:6 --seed=1 : This command makes a sample = (amount * len (dataset))% frequency ratio, sample = 4, so four files are selected randomly to download. ml-git dataset checkout computer-vision__images__dataset-ex__22 --sample-type=range --sampling=2:11:2 : This command selects the files at indexes generated by range(start=2, stop=11, step=2) . ml-git <ml-entity> commit Firstly commit verifies ml-git tag existence, then updates status file in .ml-git/<ml-entity>/index/metadata/<ml-entity-name>/INDEX.yaml and merge the metadata .ml-git/<ml-entity>/index/metadata/<ml-entity-name>/MAFINEST.yaml with .ml-git/<ml-entity>/metadata/<ml-entity-name>/MAFINEST.yaml . Update INDEX.yaml : data/1.jpg: ctime: 1582208519.35017 hash: zdj7WWMZbq7cgw76BGeqoNUGFRkyw59p4Y6zD5eb8cyWL6MW5 mtime: 1582208519.3581703 status: a <- Change status 'a' to 'u'. data/10.jpg: ctime: 1582208519.3561785 hash: zdj7WZrTe7SU5oFQc8kr1kNiAkb5TBeMP1vgcXM1fvfgn5jq5 mtime: 1582208519.6050372 status: u data/2.jpg: ctime: 1582208519.3551724 hash: zdj7WYF38pFqHrvQPnD3FXMw76UDbMaZkSXJ4qMZci1nxWqiU mtime: 1582208519.5029979 status: u Merge the metadata .ml-git/<ml-entity>/index/metadata/<ml-entity-name>/MAFINEST.yaml with .ml-git/<ml-entity>/metadata/<ml-entity-name>/MAFINEST.yaml : ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 MANIFEST.yaml < -- (1) Get data from here | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 <categopries>*/ | \u251c\u2500\u2500 MANIFEST.yaml < -- Union data (1) here, and delete (1). \u2514\u2500\u2500 <ml-entity>/ Get content of \\<ml-entity-name>.spec (structure with representational values): dataset: categories: - computer-vision - images mutability: strict manifest: store: s3h://mlgit-datasets name: imagenet8 version: 1 And insert new attribute: dataset: categories: - computer-vision - images mutability: strict manifest: files: MANIFEST.yaml store: s3h://mlgit-datasets name: imagenet8 version: 1 Then save file in: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 MANIFEST.yaml | | \u251c\u2500\u2500 <ml-entity-name>.spec < -- Copy content and change | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 <categopries>*/ | \u251c\u2500\u2500 MANIFEST.yaml | \u251c\u2500\u2500 <ml-entity-name>.spec < -- Save here \u2514\u2500\u2500 <ml-entity>/ After committing the .spec file and MANIFEST.yaml, ml-git updates the HEAD of repository with tag's SHA-1. HEAD structure: computer-vision__images__imagenet8__1: 00da0d518914cfaeb765633f68ade09a5d80b252 HEAD directory: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | | \u2514\u2500\u2500 metadata/ | | \u2514\u2500\u2500 <entity-name>/ | | \u251c\u2500\u2500 MANIFEST.yaml | | \u251c\u2500\u2500 <entity-name>.spec < -- Copy content and change | \u2514\u2500\u2500 refs/ | | \u2514\u2500\u2500 <ml-entity-name>/ | | \u251c\u2500\u2500 HEAD <-- Update tag with SHA-1 here. | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 <categopries>*/ | \u251c\u2500\u2500 MANIFEST.yaml | \u251c\u2500\u2500 <ml-entity-name>.spec < -- Save here \u2514\u2500\u2500 <ml-entity>/ * Categories path is a tree of categories paths described in .spec file. (Ex: categories/images/MANIFEST.yaml) . ml-git <ml-entity> create ml-git (dataset|labels|model) create ARTEFACT_NAME Create the the workspace structure as follow: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500config.yaml <ml-entity> \u2514\u2500\u2500 ARTEFACT_NAME/ \u2514\u2500\u2500 data/ \u2514\u2500\u2500 ARTEFACT_NAME.spec \u2514\u2500\u2500 README.md The parameters passed --category and --version are used to fill the spec file. The parameter --mutability must be used to define the entity's mutability, which can be: strict, flexible, mutable. If you want to know more about each type of mutability and how it works, please take a look at mutability helper documentation . The parameter --import is used to import files from a src folder to data folder. The optional parameter --wizard-questions if passed, ask interactive questions at console for git & store configurations and update the config.yaml file. The parameter --store-type must be used to define the entity's storage, which can be: s3h, azureblobh, gdriveh. The parameter --import-url is used to import files from Google Drive to data folder. Using this option it will be necessary to inform the path to your google drive credentials through the credentials-path argument. In addition, you can use the --unzip option to unzip the files imported. ml-git <ml-entity> export This command allows you to export files from one store (S3|MinIO) to another (S3|MinIO). ml-git (dataset|labels|model) export ML_ENTITY_TAG BUCKET_NAME Initially, it checks if the user is in an initialized ml-git project. With the entity tag , --credentials , --region , --endpoint , --retry and bucket name arguments, ml-git connects to the store (S3|MinIO) bucket. Then the files are exported to the target store (S3|MinIO) bucket. ml-git <ml-entity> fetch ml-git (dataset|labels|model) fetch ML_ENTITY_TAG Break up the ML_ENTITY_TAG into categories, specname and version, then verify if cache has tag's objects, if not, download the blobs. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 cache/ \u2514\u2500\u2500 hashfs/ <-- Objects here ml-git <ml-entity> fsck Reads objects in: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 hashfs/ <-- Objects here \u2514\u2500\u2500 objects/ \u2514\u2500\u2500 hashfs/ <-- Objects here Applies SHA2 to content of objects , uses multihash to generate the CID, and compares the CID with the file name, if it is different it mean that the file is corrupted, so ml-git fsck show the number of corrupted files and in which directory. When object is valid but not in originally directory, ml-git accept that it's corrupted. ml-git <ml-entity> import This command allows you to download a file or directory from the S3 bucket. ml-git (dataset|labels|model) import BUCKET_NAME ENTITY_DIR Initially checks if the user is in an initialized ml-git project. With the --credentials, --region (optional), --path and bucket name arguments ml-git connects to the S3 bucket. The S3 files for the file or directory specified in --path or --object will be downloaded. The files will be saved in the directory specified by the user in ENTITY_DIR, if not exists, the path will be created. ml-git <ml-entity> init When ml-git init is executed, it will read .ml-git/config.yaml to get the git repository url. ml-git will create directory .ml-git/ \\<ml-entity>/metadata if doesn't exists and clone the repository into it. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 metadata/ <-- The example command clone git repository here. Obs: Must have executed ml-git init before, to create ml-git initial configuration files. ml-git <ml-entity> list That command will list all \\<ml-entity> under management in the ml-git repository. To do this, ml-git goes through the metadata directory to identify the structure of categories and entities that are under management. ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 metadata/ <-- Check here the directory structure | \u2514\u2500\u2500 computer-vision/ | \u2514\u2500\u2500 images/ | \u2514\u2500\u2500 imagenet8/ $ ml-git dataset list ML dataset |-- computer-vision | |-- images | | |-- imagenet8 ml-git <ml-entity> log Usage: ml-git dataset log [OPTIONS] ML_ENTITY_NAME This command shows ml-entity-name's commit information like author, date, commit message. Options: --stat Show amount of files and size of an ml-entity. --fullstat Show added and deleted files. --help Show this message and exit. Example: ml-git dataset log dataset-ex ml-git <ml-entity> push ml-git (dataset|labels|model) push ML_ENTITY_NAME Verify the git global configuration, and try upload objects from local repository to data store creating a thread pool with maximum of ten workers. This process use store configuration from spec file and AWS credentials. .spec file: dataset: categories: - computer-vision - images manifest: store: s3h://mlgit-datasets < -- store configuration name: imagenet8 version: 1 Directory: ml-git_project/ \u2514\u2500\u2500 .ml-git/ | \u2514\u2500\u2500 <ml-entity>/ | \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 objects/ < -- Files to be uploaded. \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 <ml-entity-name>/ \u251c\u2500\u2500 <ml-entity-name>.spec < -- Spec file with store configuration. After the upload process, ml-git executes git push from local repository .ml-git/dataset/metadata to the remote repository configured in config.yaml . ml-git <ml-entity> remote-fsck Starting point of a remote fsck is to identify all the IPLD files contained in the MANIFEST file associated with the specified artefact spec (\\<ml-artefact-name>) and then executes the following steps: Verify the existence of all these IPLDs in the remote store If one IPLD does not exist and it is present in the local repository, upload it to the remote store If the IPLD is present in the local repository: Open it and identify all blobs associated with that IPLD. Verify the existence of these blobs in the remote store. If one blob does not exist and it is present in the local repository, upload it to the remote store. If the IPLD is NOT present in the local repository and --thorough option is set Download the IPLD Open it and identify all blobs associated with that IPLD. Verify the existence of these blobs in the remote store. If one blob does not exist and it is present in the local repository, upload it to the remote store. [--paranoid] : Paranoid mode adds an additional step that will download all IPLD and its associated IPLD links to verify the content by computing the multihash of all these. [--thorough] : Ml-git will try to download the IPLD if it is not present in the local repository to verify the existence of all contained IPLD links associated. ml-git <ml-entity> reset In ml-git project (as in git) we have three areas to manage and track the changes of the data. The workspace - where the data itself is added, deleted or updated. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500<ml-entity-name> \u2514\u2500\u2500HERE That staged area - Where the changes are added and tracked. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ \u2514\u2500\u2500HERE \u2514\u2500\u2500 metadata/ The committed area - Where the data are packed to push. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500HERE Depending how to commands are passed we manage this three areas accordingly. The Default option is HEAD. ml-git <ml-entity> show Verify tag and SHA-1 in HEAD: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 objects/ \u2514\u2500\u2500 refs/ \u2514\u2500\u2500 <ml-entity-name>/ \u251c\u2500\u2500 HEAD < -- Verify tag If tag was not found, the command return the message \"Local Repository: no HEAD for [\\<ml-entity-name>]\" , otherwise do git checkout to the tag and search for all \\<ml-entity-name>.spec file in: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ \u2514\u2500\u2500 metadata/ | \u2514\u2500\u2500 <categories>*/ | \u2514\u2500\u2500 <ml-entity-name>/ <-- Search all .spec file here \u2514\u2500\u2500 objects/ \u2514\u2500\u2500 refs/ After found all .spec files the command show each one contents, then execute git checkout to branch master . * Categories path is a tree of categories paths described in .spec file. ml-git <ml-entity> status Displays paths that have differences between the index file and the current HEAD commit, paths that have differences between the working tree and the index file, and paths in the working tree that are not tracked by ML-Git. First is described the files tracked to be commited. Those files are those ones in the manifest file. There are two types: New files - Those files are at the entities directory and hard-linked with those ones at index directory. These files are also listed in manifest file. Deleted file. - Files who was deleted from the entities directory, but still are into the manifest file. ml-git_project/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 <ml-entity-name>/ \u251c\u2500\u2500 <-- Files Checked ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 <metadata>/ | \u2514\u2500\u2500 <ml-entity-name> <-- Hard link poited to files located here | \u2514\u2500\u2500 MANIFEST.yaml <-- Files listed here Then are described the untracked files. These files are located under the entities directory and listed if they have more than one hard-link. ml-git <ml-entity> tag add ml-git (dataset|model|label) tag add dataset-ex my_tag You can use this command to associate a tag to a commit. ml-git <ml-entity> tag list This command lists the tags of an entity. To do this, it access the metadata of an entity to get the git repository and then executes git commands to list local tags. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 .git/ < -- Git repository \u2514\u2500\u2500 objects/ \u2514\u2500\u2500 refs/ ml-git <ml-entity> update Locate metadata directory where is git repository: ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 .git < -- Git repository goes here. Then, ML-Git execute \"git pull\" on \"origin\" to update all metadatas from remote repository. ml-git <ml-entity> unlock This command add read and write permissions to file or directory. So that if you are working with a flexible mutability repository you will be allowed to make changes to the unlocked file without making it corrupt. If you are working with a strict repository, changing files is not allowed, so the unlock command is not performed. In the case of a mutable repository, the files are already unlocked for modification, so it is unnecessary to execute the unlock command. You should only use this command for the flexible mutability option. ml-git clone <repository-url> The command clones the git repository which should contain a directory .ml-git , then initialize the metadata according to configurations. ml-git will create directory .ml-git/ \\<ml-entity> /metadata if doesn't exists and clone the repository into it. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 metadata/ Options : --folder : The configuration files are cloned in specified folder. --track : The ml-git clone preserves .git folder in the same directory of cloned configuration files. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 .git ml-git login This command generates new Aws credentials in the /.aws directory. Note: ml-git repository config Command try to load the configurations from the file .ml-git/config.yaml . If the file is found, it will show the configurations read from the file, if not it will show the default configurations in the project. ml-git repository gc ml-git repository gc This command will scan the metadata in each entity's index directory to identify which objects are being used by the user's worskpace. After this check, objects that are not being used and that are contained in the cache and object directories will be removed. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 index/ | \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 cache/ | \u2514\u2500\u2500 hashfs/ <-- Objects here \u2514\u2500\u2500 objects/ \u2514\u2500\u2500 hashfs/ <-- Objects here ml-git repository init ml-git repository init verify if the current directory has .ml-git , where configuration files goes, and if doesn't have it, ml-git will create the directory and save config.yaml inside, with the informations provided by a dict in project code. Directory structure : ml-git-project/ \u2514\u2500\u2500 .ml-git/ \u251c\u2500\u2500\u2500 config.yaml config.yaml structure : dataset: git: git@github.com:standel/ml-datasets.git <-- git project url store: s3: <-- store type (AWS) mlgit-datasets: <-- bucket name aws-credentials: profile: mlgit region: us-east-1 ml-git repository remote <ml-entity> add This command load the configuration file .ml-git/config.yaml and change the attribute git to the url specified on arguments, then save it. This command require that you have executed ml-git init before. ml-git repository remote <ml-entity> del This command load the configuration file .ml-git/config.yaml and change the attribute git to empty, the save it. ml-git_project/ \u2514\u2500\u2500 .ml-git/ \u2514\u2500\u2500 <ml-entity>/ \u2514\u2500\u2500 metadata/ \u2514\u2500\u2500 .git <- Change actual origin url to a blank url. ml-git repository store add (DEPRECATED) Usage: ml-git repository store add [OPTIONS] BUCKET_NAME [DEPRECATED]: Add a storage BUCKET_NAME to ml-git Options: --credentials TEXT Profile name for storage credentials --region TEXT Aws region name for S3 bucket --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --endpoint-url TEXT Storage endpoint url -g, --global Use this option to set configuration at global level --verbose Debug mode Example: $ ml-git repository store add minio --endpoint-url=<minio-endpoint-url> Use this command to add a data storage to a ML-Git project. Note: Command deprecated, use storage instead store. ml-git repository store del (DEPRECATED) Usage: ml-git repository store del [OPTIONS] BUCKET_NAME [DEPRECATED]: Delete a store BUCKET_NAME from ml-git Options: --type [s3h|s3|azureblobh|gdriveh] Store type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --help Show this message and exit. Example: $ ml-git repository store del minio Note: Command deprecated, use storage instead store. ml-git repository storage add Usage: ml-git repository storage add [OPTIONS] BUCKET_NAME Add a storage BUCKET_NAME to ml-git Options: --credentials TEXT Profile name for storage credentials --region TEXT Aws region name for S3 bucket --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --endpoint-url TEXT Storage endpoint url -g, --global Use this option to set configuration at global level --verbose Debug mode Example: $ ml-git repository storage add minio --endpoint-url=<minio-endpoint-url> Use this command to add a data storage to a ML-Git project. ml-git repository storage del Usage: ml-git repository storage del [OPTIONS] BUCKET_NAME Delete a storage BUCKET_NAME from ml-git Options: --type [s3h|s3|azureblobh|gdriveh] Storage type (s3h, s3, azureblobh, gdriveh ...) [default: s3h] --help Show this message and exit. Example: $ ml-git repository storage del minio ml-git repository update Usage: ml-git repository update This command updates the metadata for all entities. Example: $ ml-git repository update","title":"Exemple:"},{"location":"mutability_helper/","text":"Mutability \u00b6 What is the mutability? \u00b6 Mutability is the attribute that defines whether an entity's files can be changed by the user from one version to another. It is important to note that for all types of mutability the user is able to add and remove files, the mutability attribute refers to editing files already added. You must define carefully because once mutability is defined, it cannot be changed. Where to define mutability policy? \u00b6 Mutability is defined when creating a new entity. With the command ml-git (dataset|label|model) create you must pass the mandatory attribute mutability to define the type of mutability for the created entity. Your entity specification file (.spec) should look like this: dataset: categories: - computer-vision - images mutability: flexible manifest: store: s3h://mlgit-datasets name: imagenet8 version: 1 If you create an entity without using the create command and without mutability, when trying to perform the ml-git (dataset|label|model) add the command will not be executed and you will be informed that you must define a mutability for that new entity. Note: For entities that were created before the mutability parameter became mandatory and that did not define mutability, ml-git treats these entities as strict. Because it is an attribute defined in the spec, you can define a type of mutability for each entity that the project has. For example, you can have in the same project a dataset-ex1 that has strict mutability while a dataset-ex2 has flexible mutability. Polices \u00b6 Currently the user can define one of the following three types of mutability for their entities: Mutable: Disable ml-git cache (will slow down some operations). Files that were already added may be changed and added again. Flexible: Added files will be set to read-only after added to avoid any changes. If you want to change a file, you MUST use ml-git <ml-entity> unlock <file> . About unlock command: Decouple the file from ml-git cache to avoid propagating changes and creating \"corruptions\". Enable file write permission, so that you can edit the file. If you modify a file without previously executing the unlock command, the file will be considered corrupted. Strict: Added files will be set to read-only after added to avoid any changes. Once added, the files could not be modified in any other tag. Choosing the type of mutability \u00b6 The type of mutability must be chosen based on the characteristics of the entity you are working with. You must define carefully because once mutability is defined, it cannot be changed. If you are working with images , it is recommended that the type of mutability chosen is strict , since it is not common for images to be changed. Rather, new images are added to the set. In a scenario such as data augmentation, new images will be created from the originals, but the originals must remain intact. If you are working with shared cache we encourage to use mutability strict only. Take a look at the document about centralized cache. When dealing with files that must be modified over time, such as a csv file with your dataset labels, or your model file , we encourage you to use flexible or mutable . The choice will depend on how often you believe these files will be modified.","title":"Mutability Helper"},{"location":"mutability_helper/#mutability","text":"","title":"Mutability"},{"location":"mutability_helper/#what-is-the-mutability","text":"Mutability is the attribute that defines whether an entity's files can be changed by the user from one version to another. It is important to note that for all types of mutability the user is able to add and remove files, the mutability attribute refers to editing files already added. You must define carefully because once mutability is defined, it cannot be changed.","title":"What is the mutability?"},{"location":"mutability_helper/#where-to-define-mutability-policy","text":"Mutability is defined when creating a new entity. With the command ml-git (dataset|label|model) create you must pass the mandatory attribute mutability to define the type of mutability for the created entity. Your entity specification file (.spec) should look like this: dataset: categories: - computer-vision - images mutability: flexible manifest: store: s3h://mlgit-datasets name: imagenet8 version: 1 If you create an entity without using the create command and without mutability, when trying to perform the ml-git (dataset|label|model) add the command will not be executed and you will be informed that you must define a mutability for that new entity. Note: For entities that were created before the mutability parameter became mandatory and that did not define mutability, ml-git treats these entities as strict. Because it is an attribute defined in the spec, you can define a type of mutability for each entity that the project has. For example, you can have in the same project a dataset-ex1 that has strict mutability while a dataset-ex2 has flexible mutability.","title":"Where to define mutability policy?"},{"location":"mutability_helper/#polices","text":"Currently the user can define one of the following three types of mutability for their entities: Mutable: Disable ml-git cache (will slow down some operations). Files that were already added may be changed and added again. Flexible: Added files will be set to read-only after added to avoid any changes. If you want to change a file, you MUST use ml-git <ml-entity> unlock <file> . About unlock command: Decouple the file from ml-git cache to avoid propagating changes and creating \"corruptions\". Enable file write permission, so that you can edit the file. If you modify a file without previously executing the unlock command, the file will be considered corrupted. Strict: Added files will be set to read-only after added to avoid any changes. Once added, the files could not be modified in any other tag.","title":"Polices"},{"location":"mutability_helper/#choosing-the-type-of-mutability","text":"The type of mutability must be chosen based on the characteristics of the entity you are working with. You must define carefully because once mutability is defined, it cannot be changed. If you are working with images , it is recommended that the type of mutability chosen is strict , since it is not common for images to be changed. Rather, new images are added to the set. In a scenario such as data augmentation, new images will be created from the originals, but the originals must remain intact. If you are working with shared cache we encourage to use mutability strict only. Take a look at the document about centralized cache. When dealing with files that must be modified over time, such as a csv file with your dataset labels, or your model file , we encourage you to use flexible or mutable . The choice will depend on how often you believe these files will be modified.","title":"Choosing the type of mutability"},{"location":"plugins/","text":"ML-Git Data Specialization Plugins \u00b6 Data specialization plugins are resources that can be added to ML-Git providing specific processing and metadata collection for specific data formats. This document aims to provide instructions on how data specialization plugins can be developed for ML-Git, defining interface methods that must be implemented to provide the necessary functionalities for processing these data. Plugin contracts \u00b6 add_metadata This method is responsible for processing or gathering information about the versioned data and inserting it into the specification file. If the plugin is installed and properly configured, this signature will be triggered before the metadata is committed. Definition: def add_metadata ( work_space_path , metadata ): \"\"\" Args: work_space_path (str): Absolute path where the files managed by ml-git will be used to generate extra information that can be inserted in metadata. metadata (dict): Content of spec file that can be changed to add extra data. \"\"\" Note: The plugin doesn't need to implement all methods defined in the plugin contract. How to create a plugin \u00b6 When developing the plugin we recommend that the user follow the structure proposed below: ml-git-plugin-project-name/ tests/ core_tests.py package_name/ <-- name of your main package. __init__.py core.py <-- main module where the entry function is located. setup.py In package_name/core.py it is expected to contain only the contract methods essential to the operation of the plugin. # package_name/core.py def add_metadata ( work_space_path , metadata ): ... ... In package_name/__init__.py it's necessary to import the implemented contract's methods. As in the following example: # package_name/__init__.py from package_name.core import add_metadata The main purpose of the setup script is to describe your module distribution. # setup.py from setuptools import setup , find_packages setup ( name = 'ml-git-plugin-project-name' , version = '0.1' , license = 'GNU General Public License v2.0' , author = '' , description = '' , packages = find_packages (), platforms = 'Any' , zip_safe = True , ) How to install a plugin \u00b6 cd plugin-project-name pip3 install --user . For an entity of your preference, change the spec file like below: (ex: dataset/dataset-ex/dataset-ex.spec) dataset : categories : - computer-vision - images manifest : data-plugin : package_name <-- type here the package name in your plugin project. files : MANIFEST.yaml store : s3h://mlgit mutability : strict name : dataset-ex version : 1","title":"Plugins"},{"location":"plugins/#ml-git-data-specialization-plugins","text":"Data specialization plugins are resources that can be added to ML-Git providing specific processing and metadata collection for specific data formats. This document aims to provide instructions on how data specialization plugins can be developed for ML-Git, defining interface methods that must be implemented to provide the necessary functionalities for processing these data.","title":"ML-Git Data Specialization Plugins"},{"location":"plugins/#plugin-contracts","text":"add_metadata This method is responsible for processing or gathering information about the versioned data and inserting it into the specification file. If the plugin is installed and properly configured, this signature will be triggered before the metadata is committed. Definition: def add_metadata ( work_space_path , metadata ): \"\"\" Args: work_space_path (str): Absolute path where the files managed by ml-git will be used to generate extra information that can be inserted in metadata. metadata (dict): Content of spec file that can be changed to add extra data. \"\"\" Note: The plugin doesn't need to implement all methods defined in the plugin contract.","title":"Plugin contracts"},{"location":"plugins/#how-to-create-a-plugin","text":"When developing the plugin we recommend that the user follow the structure proposed below: ml-git-plugin-project-name/ tests/ core_tests.py package_name/ <-- name of your main package. __init__.py core.py <-- main module where the entry function is located. setup.py In package_name/core.py it is expected to contain only the contract methods essential to the operation of the plugin. # package_name/core.py def add_metadata ( work_space_path , metadata ): ... ... In package_name/__init__.py it's necessary to import the implemented contract's methods. As in the following example: # package_name/__init__.py from package_name.core import add_metadata The main purpose of the setup script is to describe your module distribution. # setup.py from setuptools import setup , find_packages setup ( name = 'ml-git-plugin-project-name' , version = '0.1' , license = 'GNU General Public License v2.0' , author = '' , description = '' , packages = find_packages (), platforms = 'Any' , zip_safe = True , )","title":"How to create a plugin"},{"location":"plugins/#how-to-install-a-plugin","text":"cd plugin-project-name pip3 install --user . For an entity of your preference, change the spec file like below: (ex: dataset/dataset-ex/dataset-ex.spec) dataset : categories : - computer-vision - images manifest : data-plugin : package_name <-- type here the package name in your plugin project. files : MANIFEST.yaml store : s3h://mlgit mutability : strict name : dataset-ex version : 1","title":"How to install a plugin"},{"location":"qs_checkout/","text":"Downloading a dataset from a configured repository \u00b6 To download a dataset, you need to be in an initialized and configured ML-Git project. If you have a repository with your saved settings, you can run the following command to set up your environment: $ ml-git clone git@github.com:example/your-mlgit-repository.git Then, you can retrieve a specific version of a dataset to run an experiment. To achieve that, you can use the version tag to download this version to your local environment using one of the following commands: $ ml-git dataset checkout computer-vision__images__faces__fddb__1 or $ ml-git dataset checkout fddb --version=1 If you want to get the latest available version of a dataset, you can pass its name in the checkout command, as shown below: $ ml-git dataset checkout fddb Then, your directory should look like this: computer-vision/ \u2514\u2500\u2500 images \u2514\u2500\u2500 faces \u2514\u2500\u2500 fddb \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 2002 \u2502 \u2514\u2500\u2500 2003 \u2514\u2500\u2500 fddb.spec","title":"QS Checkout"},{"location":"qs_checkout/#downloading-a-dataset-from-a-configured-repository","text":"To download a dataset, you need to be in an initialized and configured ML-Git project. If you have a repository with your saved settings, you can run the following command to set up your environment: $ ml-git clone git@github.com:example/your-mlgit-repository.git Then, you can retrieve a specific version of a dataset to run an experiment. To achieve that, you can use the version tag to download this version to your local environment using one of the following commands: $ ml-git dataset checkout computer-vision__images__faces__fddb__1 or $ ml-git dataset checkout fddb --version=1 If you want to get the latest available version of a dataset, you can pass its name in the checkout command, as shown below: $ ml-git dataset checkout fddb Then, your directory should look like this: computer-vision/ \u2514\u2500\u2500 images \u2514\u2500\u2500 faces \u2514\u2500\u2500 fddb \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 2002 \u2502 \u2514\u2500\u2500 2003 \u2514\u2500\u2500 fddb.spec","title":" Downloading a dataset from a configured repository "},{"location":"qs_configure_repository/","text":"Creating a configured repository \u00b6 It's recommended to version, in a git repository, the .ml-git folder containing the settings you frequently use. This way, you will be able to use it in future projects or share it with another ML-Git user if you want. To create the .ml-git folder that will be versioned, the following commands are necessary: Initialize the ML-Git project. $ ml-git init Configure remotes for the entities that will be used. $ ml-git dataset remote add git@github.com:example/your-mlgit-datasets.git Configure the storages which will be used. $ ml-git storage add mlgit-datasets --credentials=mlgit After that, you should version, in a git repository, the .ml-git folder created during this process. To use these settings in a new project, all you have to do is execute the command ml-git clone to import the project's settings. NOTE : If you would like to share these settings with another ML-Git user, this user must have access to the git repository where the settings are stored.","title":"QS Configure Repository"},{"location":"qs_configure_repository/#creating-a-configured-repository","text":"It's recommended to version, in a git repository, the .ml-git folder containing the settings you frequently use. This way, you will be able to use it in future projects or share it with another ML-Git user if you want. To create the .ml-git folder that will be versioned, the following commands are necessary: Initialize the ML-Git project. $ ml-git init Configure remotes for the entities that will be used. $ ml-git dataset remote add git@github.com:example/your-mlgit-datasets.git Configure the storages which will be used. $ ml-git storage add mlgit-datasets --credentials=mlgit After that, you should version, in a git repository, the .ml-git folder created during this process. To use these settings in a new project, all you have to do is execute the command ml-git clone to import the project's settings. NOTE : If you would like to share these settings with another ML-Git user, this user must have access to the git repository where the settings are stored.","title":" Creating a configured repository"},{"location":"quick_start/","text":"Quick start \u00b6 In this document we describe all steps necessary to execute the following basic tasks with ml-git: Downloading a dataset from a configured repository Having a repository that stores the settings used by ml-git, learn how to download a dataset. Creating a configured repository Learn how to create a repository that stores the settings used by ml-git.","title":"Quick start #"},{"location":"quick_start/#quick-start","text":"In this document we describe all steps necessary to execute the following basic tasks with ml-git: Downloading a dataset from a configured repository Having a repository that stores the settings used by ml-git, learn how to download a dataset. Creating a configured repository Learn how to create a repository that stores the settings used by ml-git.","title":"Quick start"},{"location":"resources_initialization/","text":"Resource initialization script \u00b6 About \u00b6 As mentioned in ML-Git internals , the design concept about ML-Git is to decouple the ML entities metadata management from the actual data, such that there are two main layers in the tool: The metadata management: There are for each ML entities managed under ml-git, the user needs to define a small specification file. These files are then managed by a git repository to retrieve the different versions. The data store management: To store data from managed artifacts. This script aims to facilitate the creation of resources (buckets and repositories) that are needed to use ML-Git. Prerequisites: \u00b6 To use this script, you must have configured it in your environment: Github Access Token: You must create a personal access token to use instead of a password with a command line or with an API. See github documentation to learn how to configure a token. Note: As this script uses the github API, it is necessary that you store the token in GITHUB_TOKEN environment variable. If you are setting up a bucket of S3 type: AWS CLI : The AWS Command Line Interface (CLI) is a unified tool for managing your AWS services. If you are setting up a bucket of azure type: Azure CLI : The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources. How to use: \u00b6 Once all the necessary requirements for the settings you want to make are installed, just run the command: Linux: Execute on terminal: cd ml-git ./scripts/resources_initialization/resources_initialization.sh Windows: Execute on Powershell or CMD: cd ml-git .\\ scripts \\ resources_initialization \\ resources_initialization . bat At the end of the script execution, the user must have configured the repositories to store the metadata, a repository available to perform the ml-git clone command and import these settings, in addition to having instantiated the buckets in the chosen services.","title":"Resources Initialization"},{"location":"resources_initialization/#resource-initialization-script","text":"","title":"Resource initialization script"},{"location":"resources_initialization/#about","text":"As mentioned in ML-Git internals , the design concept about ML-Git is to decouple the ML entities metadata management from the actual data, such that there are two main layers in the tool: The metadata management: There are for each ML entities managed under ml-git, the user needs to define a small specification file. These files are then managed by a git repository to retrieve the different versions. The data store management: To store data from managed artifacts. This script aims to facilitate the creation of resources (buckets and repositories) that are needed to use ML-Git.","title":"About"},{"location":"resources_initialization/#prerequisites","text":"To use this script, you must have configured it in your environment: Github Access Token: You must create a personal access token to use instead of a password with a command line or with an API. See github documentation to learn how to configure a token. Note: As this script uses the github API, it is necessary that you store the token in GITHUB_TOKEN environment variable. If you are setting up a bucket of S3 type: AWS CLI : The AWS Command Line Interface (CLI) is a unified tool for managing your AWS services. If you are setting up a bucket of azure type: Azure CLI : The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources.","title":"Prerequisites:"},{"location":"resources_initialization/#how-to-use","text":"Once all the necessary requirements for the settings you want to make are installed, just run the command: Linux: Execute on terminal: cd ml-git ./scripts/resources_initialization/resources_initialization.sh Windows: Execute on Powershell or CMD: cd ml-git .\\ scripts \\ resources_initialization \\ resources_initialization . bat At the end of the script execution, the user must have configured the repositories to store the metadata, a repository available to perform the ml-git clone command and import these settings, in addition to having instantiated the buckets in the chosen services.","title":"How to use:"},{"location":"s3_configurations/","text":"S3 bucket configuration \u00b6 This section explains how to configure the settings that the ml-git uses to interact with your bucket. This requires that you have the following data: Profile Name Access Key ID Secret Access Key Region Name Output Format The Access Key ID and Secret Access Key are your credentials. The Region Name identifies the AWS Region whose servers you want to send your requests. The Output Format specifies how the results are formatted. Ml-git allows you to have your bucket directly on AWS infrastructure or through MinIO. This document is divided into two sections wich describe how configure each one of these. AWS \u00b6 Internally ml-git uses Boto3 to communicate with AWS services. Boto3 is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services. Boto3 looks at various configuration locations until it finds configuration values. The following lookup order is used searching through sources for configuration values: Environment variables The ~/.aws/config file Note: If, when creating a storage, you define a specific profile to be used, Boto3 will only search for that profile in the ~/.aws/config file. You can configure the AWS in three ways (environment variables, through the console or with the AWS Command Line Interface ). These are described in the following sections. 1 - Environment Variables Linux or macOS : ``` $ export AWS_ACCESS_KEY_ID=your-access-key $ export AWS_SECRET_ACCESS_KEY=your-secret-access-key $ export AWS_DEFAULT_REGION=us-west-2 ``` Windows : ``` C:\\> setx AWS_ACCESS_KEY_ID your-access-key C:\\> setx AWS_SECRET_ACCESS_KEY your-secret-access-key C:\\> setx AWS_DEFAULT_REGION us-west-2 ``` 2 - Console From the home directory (UserProfile) execute: $ mkdir .aws You need to create two files to store the sensitive credential information (~/.aws/credentials) separated from the less sensitive configuration options (~/.aws/config). To create these two files type the following commands: For config file: $ echo \" [your-profile-name] region=bucket-region output=json \" > .aws/config For credentials file: $ echo \" [your-profile-name] aws_access_key_id = your-access-key aws_secret_access_key = your-secret-access-key \" > .aws/credentials 3 - AWS CLI For general use, the aws configure command is the fastest way to set up, but requires the AWS CLI installed. For install and configure type the following commands: $ pip install awscli $ aws configure AWS Access Key ID [None]: your-access-key AWS Secret Access Key [None]: your-secret-access-key Default region name [None]: bucket-region Default output format [None]: json These commands will create the files ~/.aws/credentials and ~/.aws/config. Demonstrating AWS Configure MinIO \u00b6 MinIO is a cloud storage server compatible with Amazon S3. That said, you can configure it in the same way as described above by placing Access Key and Secret Access Key of your MinIO bucket.","title":"MinIO"},{"location":"s3_configurations/#s3-bucket-configuration","text":"This section explains how to configure the settings that the ml-git uses to interact with your bucket. This requires that you have the following data: Profile Name Access Key ID Secret Access Key Region Name Output Format The Access Key ID and Secret Access Key are your credentials. The Region Name identifies the AWS Region whose servers you want to send your requests. The Output Format specifies how the results are formatted. Ml-git allows you to have your bucket directly on AWS infrastructure or through MinIO. This document is divided into two sections wich describe how configure each one of these.","title":"S3 bucket configuration"},{"location":"s3_configurations/#aws","text":"Internally ml-git uses Boto3 to communicate with AWS services. Boto3 is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services. Boto3 looks at various configuration locations until it finds configuration values. The following lookup order is used searching through sources for configuration values: Environment variables The ~/.aws/config file Note: If, when creating a storage, you define a specific profile to be used, Boto3 will only search for that profile in the ~/.aws/config file. You can configure the AWS in three ways (environment variables, through the console or with the AWS Command Line Interface ). These are described in the following sections. 1 - Environment Variables Linux or macOS : ``` $ export AWS_ACCESS_KEY_ID=your-access-key $ export AWS_SECRET_ACCESS_KEY=your-secret-access-key $ export AWS_DEFAULT_REGION=us-west-2 ``` Windows : ``` C:\\> setx AWS_ACCESS_KEY_ID your-access-key C:\\> setx AWS_SECRET_ACCESS_KEY your-secret-access-key C:\\> setx AWS_DEFAULT_REGION us-west-2 ``` 2 - Console From the home directory (UserProfile) execute: $ mkdir .aws You need to create two files to store the sensitive credential information (~/.aws/credentials) separated from the less sensitive configuration options (~/.aws/config). To create these two files type the following commands: For config file: $ echo \" [your-profile-name] region=bucket-region output=json \" > .aws/config For credentials file: $ echo \" [your-profile-name] aws_access_key_id = your-access-key aws_secret_access_key = your-secret-access-key \" > .aws/credentials 3 - AWS CLI For general use, the aws configure command is the fastest way to set up, but requires the AWS CLI installed. For install and configure type the following commands: $ pip install awscli $ aws configure AWS Access Key ID [None]: your-access-key AWS Secret Access Key [None]: your-secret-access-key Default region name [None]: bucket-region Default output format [None]: json These commands will create the files ~/.aws/credentials and ~/.aws/config. Demonstrating AWS Configure","title":"AWS"},{"location":"s3_configurations/#minio","text":"MinIO is a cloud storage server compatible with Amazon S3. That said, you can configure it in the same way as described above by placing Access Key and Secret Access Key of your MinIO bucket.","title":"MinIO"},{"location":"storage_configurations/","text":"Storage configurations \u00b6 ML-Git supports four types of storage (S3, MinIO, Azure or Google Drive). You can find files describing how to configure each of these types of storage below: S3 MinIO Azure Google Drive","title":"Configurations"},{"location":"storage_configurations/#storage-configurations","text":"ML-Git supports four types of storage (S3, MinIO, Azure or Google Drive). You can find files describing how to configure each of these types of storage below: S3 MinIO Azure Google Drive","title":"Storage configurations"},{"location":"api/","text":"ML-Git API \u00b6 Methods available in the API \u00b6 add def add ( entity_type , entity_name , bumpversion = False , fsck = False , file_path = []): \"\"\"This command will add all the files under the directory into the ml-git index/staging area. Example: add('dataset', 'dataset-ex', bumpversion=True) Args: entity_type (str): The type of an ML entity. (dataset, labels or model) entity_name (str): The name of the ML entity you want to add the files. bumpversion (bool, optional): Increment the entity version number when adding more files [default: False]. fsck (bool, optional): Run fsck after command execution [default: False]. file_path (list, optional): List of files that must be added by the command [default: all files]. \"\"\" checkout def checkout ( entity , tag , sampling = None , retries = 2 , force = False , dataset = False , labels = False ): \"\"\"This command allows retrieving the data of a specific version of an ML entity. Example: checkout('dataset', 'computer-vision__images3__imagenet__1') Args: entity (str): The type of an ML entity. (dataset, labels or model) tag (str): An ml-git tag to identify a specific version of an ML entity. sampling (dict): group: <amount>:<group> The group sample option consists of amount and group used to download a sample.\\n range: <start:stop:step> The range sample option consists of start, stop and step used to download a sample. The start parameter can be equal or greater than zero. The stop parameter can be 'all', -1 or any integer above zero.\\n random: <amount:frequency> The random sample option consists of amount and frequency used to download a sample. seed: The seed is used to initialize the pseudorandom numbers. retries (int, optional): Number of retries to download the files from the storage [default: 2]. force (bool, optional): Force checkout command to delete untracked/uncommitted files from the local repository [default: False]. dataset (bool, optional): If exist a dataset related with the model or labels, this one must be downloaded [default: False]. labels (bool, optional): If exist labels related with the model, they must be downloaded [default: False]. Returns: str: Return the path where the data was checked out. \"\"\" clone def clone ( repository_url , folder = None , track = False ): \"\"\"This command will clone minimal configuration files from repository-url with valid .ml-git/config.yaml, then initialize the metadata according to configurations. Example: clone('https://git@github.com/mlgit-repository') Args: repository_url (str): The git repository that will be cloned. folder (str, optional): Directory that can be created to execute the clone command. [Default: current path] track (bool, optional): Set if the tracking of the cloned repository should be kept. [Default: False] \"\"\" commit def commit ( entity , ml_entity_name , commit_message = None , related_dataset = None , related_labels = None ): \"\"\"This command commits the index / staging area to the local repository. Example: commit('dataset', 'dataset-ex') Args: entity (str): The type of an ML entity. (dataset, labels or model). ml_entity_name (str): Artefact name to commit. commit_message (str, optional): Message of commit. related_dataset (str, optional): Artefact name of dataset related to commit. related_labels (str, optional): Artefact name of labels related to commit. \"\"\" create def create ( entity , entity_name , categories , mutability , ** kwargs ): \"\"\"This command will create the workspace structure with data and spec file for an entity and set the store configurations. Example: create('dataset', 'dataset-ex', categories=['computer-vision', 'images'], mutability='strict') Args: entity (str): The type of an ML entity. (dataset, labels or model). entity_name (str): An ml-git entity name to identify a ML entity. categories (list): Artifact's category name. mutability (str): Mutability type. The mutability options are strict, flexible and mutable. store_type (str, optional): Data store type [default: s3h]. version (int, optional): Number of artifact version [default: 1]. import_path (str, optional): Path to be imported to the project. bucket_name (str, optional): Bucket name. import_url (str, optional): Import data from a google drive url. credentials_path (str, optional): Directory of credentials.json. unzip (bool, optional): Unzip imported zipped files [default: False]. \"\"\" init def init ( entity ): \"\"\"This command will start the ml-git entity. Examples: init('repository') init('dataset') Args: entity (str): The type of entity that will be initialized. (repository, dataset, labels or model). \"\"\" push def push ( entity , entity_name , retries = 2 , clear_on_fail = False ): \"\"\"This command allows pushing the data of a specific version of an ML entity. Example: push('dataset', 'dataset-ex') Args: entity (str): The type of an ML entity. (dataset, labels or model). entity_name (str): An ml-git entity name to identify a ML entity. retries (int, optional): Number of retries to upload the files to the storage [default: 2]. clear_on_fail (bool, optional): Remove the files from the store in case of failure during the push operation [default: False]. \"\"\" remote add def remote_add ( entity , remote_url , global_configuration = False ): \"\"\"This command will add a remote to store the metadata from this ml-git project. Examples: remote_add('dataset', 'https://git@github.com/mlgit-datasets') Args: entity (str): The type of an ML entity. (repository, dataset, labels or model). remote_url(str): URL of an existing remote git repository. global_configuration (bool, optional): Use this option to set configuration at global level [default: False]. \"\"\" store add def store_add ( bucket_name , bucket_type = StoreType . S3H . value , credentials = None , global_configuration = False , endpoint_url = None ): \"\"\"This command will add a store to the ml-git project. Examples: store_add('my-bucket', type='s3h') Args: bucket_name (str): The name of the bucket in the storage. bucket_type (str, optional): Store type (s3h, azureblobh or gdriveh) [default: s3h]. credentials (str, optional): Name of the profile that stores the credentials or the path to the credentials. global_configuration (bool, optional): Use this option to set configuration at global level [default: False]. endpoint_url (str, optional): Store endpoint url. \"\"\" API notebooks \u00b6 In the api_scripts directory you can find notebooks running the ML-Git api for some scenarios. To run them, you just need to boot the jupyter notebook in an environment with ML-Git installed and navigate to the notebook.","title":"Methods"},{"location":"api/#ml-git-api","text":"","title":"ML-Git API"},{"location":"api/#methods-available-in-the-api","text":"add def add ( entity_type , entity_name , bumpversion = False , fsck = False , file_path = []): \"\"\"This command will add all the files under the directory into the ml-git index/staging area. Example: add('dataset', 'dataset-ex', bumpversion=True) Args: entity_type (str): The type of an ML entity. (dataset, labels or model) entity_name (str): The name of the ML entity you want to add the files. bumpversion (bool, optional): Increment the entity version number when adding more files [default: False]. fsck (bool, optional): Run fsck after command execution [default: False]. file_path (list, optional): List of files that must be added by the command [default: all files]. \"\"\" checkout def checkout ( entity , tag , sampling = None , retries = 2 , force = False , dataset = False , labels = False ): \"\"\"This command allows retrieving the data of a specific version of an ML entity. Example: checkout('dataset', 'computer-vision__images3__imagenet__1') Args: entity (str): The type of an ML entity. (dataset, labels or model) tag (str): An ml-git tag to identify a specific version of an ML entity. sampling (dict): group: <amount>:<group> The group sample option consists of amount and group used to download a sample.\\n range: <start:stop:step> The range sample option consists of start, stop and step used to download a sample. The start parameter can be equal or greater than zero. The stop parameter can be 'all', -1 or any integer above zero.\\n random: <amount:frequency> The random sample option consists of amount and frequency used to download a sample. seed: The seed is used to initialize the pseudorandom numbers. retries (int, optional): Number of retries to download the files from the storage [default: 2]. force (bool, optional): Force checkout command to delete untracked/uncommitted files from the local repository [default: False]. dataset (bool, optional): If exist a dataset related with the model or labels, this one must be downloaded [default: False]. labels (bool, optional): If exist labels related with the model, they must be downloaded [default: False]. Returns: str: Return the path where the data was checked out. \"\"\" clone def clone ( repository_url , folder = None , track = False ): \"\"\"This command will clone minimal configuration files from repository-url with valid .ml-git/config.yaml, then initialize the metadata according to configurations. Example: clone('https://git@github.com/mlgit-repository') Args: repository_url (str): The git repository that will be cloned. folder (str, optional): Directory that can be created to execute the clone command. [Default: current path] track (bool, optional): Set if the tracking of the cloned repository should be kept. [Default: False] \"\"\" commit def commit ( entity , ml_entity_name , commit_message = None , related_dataset = None , related_labels = None ): \"\"\"This command commits the index / staging area to the local repository. Example: commit('dataset', 'dataset-ex') Args: entity (str): The type of an ML entity. (dataset, labels or model). ml_entity_name (str): Artefact name to commit. commit_message (str, optional): Message of commit. related_dataset (str, optional): Artefact name of dataset related to commit. related_labels (str, optional): Artefact name of labels related to commit. \"\"\" create def create ( entity , entity_name , categories , mutability , ** kwargs ): \"\"\"This command will create the workspace structure with data and spec file for an entity and set the store configurations. Example: create('dataset', 'dataset-ex', categories=['computer-vision', 'images'], mutability='strict') Args: entity (str): The type of an ML entity. (dataset, labels or model). entity_name (str): An ml-git entity name to identify a ML entity. categories (list): Artifact's category name. mutability (str): Mutability type. The mutability options are strict, flexible and mutable. store_type (str, optional): Data store type [default: s3h]. version (int, optional): Number of artifact version [default: 1]. import_path (str, optional): Path to be imported to the project. bucket_name (str, optional): Bucket name. import_url (str, optional): Import data from a google drive url. credentials_path (str, optional): Directory of credentials.json. unzip (bool, optional): Unzip imported zipped files [default: False]. \"\"\" init def init ( entity ): \"\"\"This command will start the ml-git entity. Examples: init('repository') init('dataset') Args: entity (str): The type of entity that will be initialized. (repository, dataset, labels or model). \"\"\" push def push ( entity , entity_name , retries = 2 , clear_on_fail = False ): \"\"\"This command allows pushing the data of a specific version of an ML entity. Example: push('dataset', 'dataset-ex') Args: entity (str): The type of an ML entity. (dataset, labels or model). entity_name (str): An ml-git entity name to identify a ML entity. retries (int, optional): Number of retries to upload the files to the storage [default: 2]. clear_on_fail (bool, optional): Remove the files from the store in case of failure during the push operation [default: False]. \"\"\" remote add def remote_add ( entity , remote_url , global_configuration = False ): \"\"\"This command will add a remote to store the metadata from this ml-git project. Examples: remote_add('dataset', 'https://git@github.com/mlgit-datasets') Args: entity (str): The type of an ML entity. (repository, dataset, labels or model). remote_url(str): URL of an existing remote git repository. global_configuration (bool, optional): Use this option to set configuration at global level [default: False]. \"\"\" store add def store_add ( bucket_name , bucket_type = StoreType . S3H . value , credentials = None , global_configuration = False , endpoint_url = None ): \"\"\"This command will add a store to the ml-git project. Examples: store_add('my-bucket', type='s3h') Args: bucket_name (str): The name of the bucket in the storage. bucket_type (str, optional): Store type (s3h, azureblobh or gdriveh) [default: s3h]. credentials (str, optional): Name of the profile that stores the credentials or the path to the credentials. global_configuration (bool, optional): Use this option to set configuration at global level [default: False]. endpoint_url (str, optional): Store endpoint url. \"\"\"","title":" Methods available in the API "},{"location":"api/#api-notebooks","text":"In the api_scripts directory you can find notebooks running the ML-Git api for some scenarios. To run them, you just need to boot the jupyter notebook in an environment with ML-Git installed and navigate to the notebook.","title":" API notebooks "},{"location":"api/quick_start/","text":"ML-Git API \u00b6 Quick start \u00b6 To use the ML-Git API, it is necessary to have ML-Git in the environment that will be executed and be inside a directory with an initialized ML-Git project (you can also perform this initialization using the api clone command if you already have a repository configured). Clone \u00b6 from ml_git import api repository_url = 'https://git@github.com/mlgit-repository' api . clone ( repository_url ) output: INFO - Metadata Manager: Metadata init [https://git@github.com/mlgit-repository] @ [/home/user/Documentos/mlgit-api/mlgit/.ml-git/dataset/metadata] INFO - Metadata: Successfully loaded configuration files! Checkout \u00b6 We assume there is an initialized ml-git project in the directory. Checkout dataset \u00b6 from ml_git import api entity = 'dataset' tag = 'computer-vision__images__imagenet__1' data_path = api . checkout ( entity , tag ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.87kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.35kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 3.00kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 1.72kfiles into workspace/s] Checkout labels with dataset \u00b6 from ml_git import api entity = 'labels' tag = 'computer-vision__images__mscoco__2' data_path = api . checkout ( entity , tag , dataset = True ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/labels/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 205blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 173chunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 788files into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 1.28kfiles into workspace/s] INFO - Repository: Initializing related dataset download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 3.27kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.37kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.40kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 1.72kfiles into workspace/s] Checkout dataset with sample \u00b6 Group-Sample \u00b6 from ml_git import api entity = 'dataset' tag = 'computer-vision__images__imagenet__1' sampling = { 'group' : '1:2' , 'seed' : '10' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.04kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.83kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.09kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.16kfiles into workspace/s] Range-Sample \u00b6 from ml_git import api entity = 'dataset' tag = 'computer-vision__images__imagenet__1' sampling = { 'range' : '0:4:3' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.71kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.54kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.22kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.55kfiles into workspace/s] Random-Sample \u00b6 from ml_git import api entity = 'dataset' tag = 'computer-vision__images__imagenet__1' sampling = { 'random' : '1:2' , 'seed' : '1' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.47kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.00kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 3.77kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.17kfiles into workspace/s] Add \u00b6 from ml_git import api api . add ( 'dataset' , 'dataset-ex' ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/mlgit-api/mlgit/.ml-git/dataset/metadata] INFO - Repository: dataset adding path [[/home/user/Documentos/mlgit-api/mlgit/dataset//dataset-ex] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 381files/s] Commit \u00b6 from ml_git import api entity = 'dataset' entity_name = 'dataset-ex' message = 'Commit example' api . commit ( entity , entity_name , message ) output: INFO - Metadata Manager: Commit repo[/home/user/Documentos/project/.ml-git/dataset/metadata] --- file[computer-vision/images/dataset-ex] Push \u00b6 from ml_git import api entity = 'dataset' spec = 'dataset-ex' api . push ( entity , spec ) output: files: 100%|##########| 24.0/24.0 [00:00<00:00, 34.3files/s] Create \u00b6 from ml_git import api entity = 'dataset' spec = 'dataset-ex' categories = [ 'computer-vision' , 'images' ] mutability = 'strict' api . create ( entity , spec , categories , mutability , import_path = '/path/to/dataset' , unzip = True , version = 2 ) output: INFO - MLGit: Project Created. Init \u00b6 Repository \u00b6 from ml_git import api api . init ( 'repository' ) output: INFO - Admin: Initialized empty ml-git repository in /home/user/Documentos/project/.ml-git Entity \u00b6 from ml_git import api entity_type = 'dataset' api . init ( entity_type ) output: INFO - Metadata Manager: Metadata init [https://git@github.com/mlgit-datasets] @ [/home/user/Documentos/project/.ml-git/dataset/metadata] Remote add \u00b6 from ml_git import api entity_type = 'dataset' datasets_repository = 'https://git@github.com/mlgit-datasets' api . remote_add ( entity_type , datasets_repository ) output: INFO - Admin: Add remote repository [https://git@github.com/mlgit-datasets] for [dataset] Store add \u00b6 from ml_git import api bucket_name = 'minio' bucket_type = 's3h' endpoint_url = 'http://127.0.0.1:9000/' api . store_add ( bucket_name = bucket_name , bucket_type = bucket_type , endpoint_url = endpoint_url ) output: INFO - Admin: Add store [s3h://minio]","title":"Quick Start"},{"location":"api/quick_start/#ml-git-api","text":"","title":"ML-Git API"},{"location":"api/quick_start/#quick-start","text":"To use the ML-Git API, it is necessary to have ML-Git in the environment that will be executed and be inside a directory with an initialized ML-Git project (you can also perform this initialization using the api clone command if you already have a repository configured).","title":" Quick start "},{"location":"api/quick_start/#clone","text":"from ml_git import api repository_url = 'https://git@github.com/mlgit-repository' api . clone ( repository_url ) output: INFO - Metadata Manager: Metadata init [https://git@github.com/mlgit-repository] @ [/home/user/Documentos/mlgit-api/mlgit/.ml-git/dataset/metadata] INFO - Metadata: Successfully loaded configuration files!","title":"Clone"},{"location":"api/quick_start/#checkout","text":"We assume there is an initialized ml-git project in the directory.","title":"Checkout"},{"location":"api/quick_start/#checkout-dataset","text":"from ml_git import api entity = 'dataset' tag = 'computer-vision__images__imagenet__1' data_path = api . checkout ( entity , tag ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.87kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.35kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 3.00kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 1.72kfiles into workspace/s]","title":"Checkout dataset"},{"location":"api/quick_start/#checkout-labels-with-dataset","text":"from ml_git import api entity = 'labels' tag = 'computer-vision__images__mscoco__2' data_path = api . checkout ( entity , tag , dataset = True ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/labels/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 205blobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 173chunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 788files into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00<00:00, 1.28kfiles into workspace/s] INFO - Repository: Initializing related dataset download blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 3.27kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.37kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 2.40kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00<00:00, 1.72kfiles into workspace/s]","title":"Checkout labels with dataset"},{"location":"api/quick_start/#checkout-dataset-with-sample","text":"","title":"Checkout dataset with sample"},{"location":"api/quick_start/#group-sample","text":"from ml_git import api entity = 'dataset' tag = 'computer-vision__images__imagenet__1' sampling = { 'group' : '1:2' , 'seed' : '10' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.04kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.83kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.09kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.16kfiles into workspace/s]","title":"Group-Sample"},{"location":"api/quick_start/#range-sample","text":"from ml_git import api entity = 'dataset' tag = 'computer-vision__images__imagenet__1' sampling = { 'range' : '0:4:3' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.71kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.54kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.22kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.55kfiles into workspace/s]","title":"Range-Sample"},{"location":"api/quick_start/#random-sample","text":"from ml_git import api entity = 'dataset' tag = 'computer-vision__images__imagenet__1' sampling = { 'random' : '1:2' , 'seed' : '1' } data_path = api . checkout ( entity , tag , sampling ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata] blobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.47kblobs/s] chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 2.00kchunks/s] files into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 3.77kfiles into cache/s] files into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00<00:00, 1.17kfiles into workspace/s]","title":"Random-Sample"},{"location":"api/quick_start/#add","text":"from ml_git import api api . add ( 'dataset' , 'dataset-ex' ) output: INFO - Metadata Manager: Pull [/home/user/Documentos/mlgit-api/mlgit/.ml-git/dataset/metadata] INFO - Repository: dataset adding path [[/home/user/Documentos/mlgit-api/mlgit/dataset//dataset-ex] to ml-git index files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00<00:00, 381files/s]","title":"Add"},{"location":"api/quick_start/#commit","text":"from ml_git import api entity = 'dataset' entity_name = 'dataset-ex' message = 'Commit example' api . commit ( entity , entity_name , message ) output: INFO - Metadata Manager: Commit repo[/home/user/Documentos/project/.ml-git/dataset/metadata] --- file[computer-vision/images/dataset-ex]","title":"Commit"},{"location":"api/quick_start/#push","text":"from ml_git import api entity = 'dataset' spec = 'dataset-ex' api . push ( entity , spec ) output: files: 100%|##########| 24.0/24.0 [00:00<00:00, 34.3files/s]","title":"Push"},{"location":"api/quick_start/#create","text":"from ml_git import api entity = 'dataset' spec = 'dataset-ex' categories = [ 'computer-vision' , 'images' ] mutability = 'strict' api . create ( entity , spec , categories , mutability , import_path = '/path/to/dataset' , unzip = True , version = 2 ) output: INFO - MLGit: Project Created.","title":"Create"},{"location":"api/quick_start/#init","text":"","title":"Init"},{"location":"api/quick_start/#repository","text":"from ml_git import api api . init ( 'repository' ) output: INFO - Admin: Initialized empty ml-git repository in /home/user/Documentos/project/.ml-git","title":"Repository"},{"location":"api/quick_start/#entity","text":"from ml_git import api entity_type = 'dataset' api . init ( entity_type ) output: INFO - Metadata Manager: Metadata init [https://git@github.com/mlgit-datasets] @ [/home/user/Documentos/project/.ml-git/dataset/metadata]","title":"Entity"},{"location":"api/quick_start/#remote-add","text":"from ml_git import api entity_type = 'dataset' datasets_repository = 'https://git@github.com/mlgit-datasets' api . remote_add ( entity_type , datasets_repository ) output: INFO - Admin: Add remote repository [https://git@github.com/mlgit-datasets] for [dataset]","title":"Remote add"},{"location":"api/quick_start/#store-add","text":"from ml_git import api bucket_name = 'minio' bucket_type = 's3h' endpoint_url = 'http://127.0.0.1:9000/' api . store_add ( bucket_name = bucket_name , bucket_type = bucket_type , endpoint_url = endpoint_url ) output: INFO - Admin: Add store [s3h://minio]","title":"Store add"},{"location":"tabular_data/tabular_data/","text":"Working with tabular data \u00b6 What is tabular data? \u00b6 For most people working with small amounts of data, the data table is the fundamental unit of organization. The data table, arguably the oldest data structure, is both a way of organizing data for processing by machines and of presenting data visually for consumption by humans. Tabular data is data that is structured into rows, each of which contains information about some thing. Each row contains the same number of cells (although some of these cells may be empty), which provide values of properties of the thing described by the row. In tabular data, cells within the same column provide values for the same property of the things described by each row. This is what differentiates tabular data from other line-oriented formats. An example of a tabular data structure can be seen below: Versioning \u00b6 Due to the way the data is versioned by ml-git (see internals documentation ) the data organization structure can influence the performance and optimization of the data storage that ml-git has. When ml-git is dealing with tabular data, in order to obtain higher storage usage efficiency, it is recommended to avoid actions that edit data that were previously added.\u200b We strongly recommend that the user organize their data in such a way that the entry of new data into the set is done without changing the data already added. Examples of this type of organization is to partition the data by insertion date. This way, each partition should not be modified by future data insertions.\u200b One good way how we can achieve partitioning is using the folders structure to split data in different physical sets, even with several levels, with a part of the information of the table. As we can see in the picture, the name of each folder should contain the concrete value of the column and optionally also the name of the column. Some criteria must be met when choosing the key partition columns: Be used very frequently with the same conditions. Time-based data: combination of year, month, and day associated with time values. Location-based data: geographic region data associated with some place. Have a reasonable number of different values (cardinality). The number of possible values has to be reasonable to gain efficiency splitting the data. For example a valid range could be between 10 and 1000. Adding or modifying the data \u00b6 Once your data is versioned as suggested in the previous section, you may at some point wish to add new data to this dataset. Whenever this type of operation is to be performed, try to take into consideration editing the smallest number of files that have already been versioned. The increment of new data must be given by the creation of new files. One way to make these changes without modifying the data is to use the append save mode if you are working with parquet data. Using append save mode, you can append a dataframe to an existing parquet file. See more in this link . Note: In exploratory tests it was observed that the use of parquet data with the append writing mode is the most efficient in terms of performance and optimization for ml-git, since this writing mode avoids the modification of previous files. If you are working with another type of data, such as CSV, whenever new data is added to your dataset you must create a new file for that data. Note: CSV format files are generally not recommended for large volumes of data. It is recommended to use a more efficient data structure, such as parquet.","title":"Working With Tabular Data"},{"location":"tabular_data/tabular_data/#working-with-tabular-data","text":"","title":"Working with tabular data"},{"location":"tabular_data/tabular_data/#what-is-tabular-data","text":"For most people working with small amounts of data, the data table is the fundamental unit of organization. The data table, arguably the oldest data structure, is both a way of organizing data for processing by machines and of presenting data visually for consumption by humans. Tabular data is data that is structured into rows, each of which contains information about some thing. Each row contains the same number of cells (although some of these cells may be empty), which provide values of properties of the thing described by the row. In tabular data, cells within the same column provide values for the same property of the things described by each row. This is what differentiates tabular data from other line-oriented formats. An example of a tabular data structure can be seen below:","title":" What is tabular data? "},{"location":"tabular_data/tabular_data/#versioning","text":"Due to the way the data is versioned by ml-git (see internals documentation ) the data organization structure can influence the performance and optimization of the data storage that ml-git has. When ml-git is dealing with tabular data, in order to obtain higher storage usage efficiency, it is recommended to avoid actions that edit data that were previously added.\u200b We strongly recommend that the user organize their data in such a way that the entry of new data into the set is done without changing the data already added. Examples of this type of organization is to partition the data by insertion date. This way, each partition should not be modified by future data insertions.\u200b One good way how we can achieve partitioning is using the folders structure to split data in different physical sets, even with several levels, with a part of the information of the table. As we can see in the picture, the name of each folder should contain the concrete value of the column and optionally also the name of the column. Some criteria must be met when choosing the key partition columns: Be used very frequently with the same conditions. Time-based data: combination of year, month, and day associated with time values. Location-based data: geographic region data associated with some place. Have a reasonable number of different values (cardinality). The number of possible values has to be reasonable to gain efficiency splitting the data. For example a valid range could be between 10 and 1000.","title":" Versioning "},{"location":"tabular_data/tabular_data/#adding-or-modifying-the-data","text":"Once your data is versioned as suggested in the previous section, you may at some point wish to add new data to this dataset. Whenever this type of operation is to be performed, try to take into consideration editing the smallest number of files that have already been versioned. The increment of new data must be given by the creation of new files. One way to make these changes without modifying the data is to use the append save mode if you are working with parquet data. Using append save mode, you can append a dataframe to an existing parquet file. See more in this link . Note: In exploratory tests it was observed that the use of parquet data with the append writing mode is the most efficient in terms of performance and optimization for ml-git, since this writing mode avoids the modification of previous files. If you are working with another type of data, such as CSV, whenever new data is added to your dataset you must create a new file for that data. Note: CSV format files are generally not recommended for large volumes of data. It is recommended to use a more efficient data structure, such as parquet.","title":" Adding or modifying the data "}]}