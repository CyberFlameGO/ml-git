{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a basic flow with ml-git with api. In it, we will show how to obtain a dataset already versioned by ml-git, how to perform a versioning process of a model and new data generated. For this, we will use the MNIST dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset MNIST is a set of small images of handwritten digits, in the version available in our docker environment, the set has a total of 70,000 images from numbers 0 to 9. Look at the below image which has a few examples instances:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset](MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start working with our dataset it is necessary to carry out the checkout command of ml-git in order to bring the data from our storage to the user's workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_git import api\n",
    "\n",
    "# def checkout(entity, tag, sampling=None, retries=2, force=False, dataset=False, labels=False, version=-1)\n",
    "api.checkout('labels', 'labelsmnist', dataset=True)\n",
    "mnist_dataset_path = 'dataset/handwritten/digits/mnist/data/'\n",
    "mnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important points to highlight here are that the tag parameter can be the name of the entity, this way the ml-git will get the latest version available for this entity. With the dataset=True signals that ml-git should look for the dataset associated with these labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data in the workspace, we can load it into variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.data import loadlocal_mnist\n",
    "import numpy as np\n",
    "\n",
    "X_train, y_train = loadlocal_mnist(\n",
    "    images_path= mnist_dataset_path + 'train-images.idx3-ubyte', \n",
    "    labels_path= mnist_labels_path + 'train-labels.idx1-ubyte')\n",
    "\n",
    "print('Training data: ')\n",
    "print('Dimensions: %s x %s' % (X_train.shape[0], X_train.shape[1]))\n",
    "print('Digits: %s' % np.unique(y_train))\n",
    "print('Class distribution: %s' % np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data consists of 60,000 entries of 784 pixels, distributed among the possible values ​​according to the output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = loadlocal_mnist(\n",
    "    images_path= mnist_dataset_path + 't10k-images.idx3-ubyte', \n",
    "    labels_path= mnist_labels_path + 't10k-labels.idx1-ubyte')\n",
    "\n",
    "print('Test data: ')\n",
    "print('Dimensions: %s x %s' % (X_test.shape[0], X_test.shape[1]))\n",
    "print('Digits: %s' % np.unique(y_test))\n",
    "print('Class distribution: %s' % np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data consists of 10,000 entries of 784 pixels, distributed among the possible values according to the output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Training and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take an example of RandomForest Classifier and train it on the dataset and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Training on the existing dataset\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy score after training on existing dataset\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Versioning our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do not have any previously versioned models, it will be necessary to create a new entity. For this we use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ml-git model create modelmnist --category=handwritten --category=digits --bucket-name=mlgit\n",
    "! echo \"  mutability: mutable\" >> model/modelmnist/modelmnist.spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our model trained and evaluated, we will version it with ml-git. For that we need to save it in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model):\n",
    "    filename = 'model/modelmnist/data/rf_mnist.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "save_model(rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the file in the workspace we use the following commands to create a version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type = 'model'\n",
    "entity_name = 'modelmnist'\n",
    "\n",
    "# def add(entity_type, entity_name, bumpversion=False, fsck=False, file_path=[])\n",
    "api.add(entity_type, entity_name)\n",
    "\n",
    "# def commit(entity, ml_entity_name, commit_message=None, related_dataset=None, related_labels=None)\n",
    "api.commit(entity_type, entity_name, related_dataset='mnist', related_labels='labelsmnist')\n",
    "\n",
    "# def push(entity, entity_name, retries=2, clear_on_fail=False)\n",
    "api.push(entity_type, entity_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Improving accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve by tuning hyperparameters of the Algorithm or by trying a different algorithm altogether. Sometimes the Algorithm requires more dataset to improve the prediction function, we can expand the dataset using the same dataset itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we’ve discussed earlier, each instance of the dataset is nothing but a vector of (784) pixels values. (which is actually a representation of 28x28 image)\n",
    "What if we shift the images by one pixel at either of side? See, the below example.\n",
    "\n",
    "![dataset](MNIST_SHIFTED.png)\n",
    "\n",
    "\n",
    "We will shift the images to each of the four directions by one pixel and generate four more images from a single image. The resulting dataset would now contain 3,00,000 images(60000 x 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "\n",
    "# Method to shift the image by given dimension\n",
    "def shift_image(image, dx, dy):\n",
    "    image = image.reshape((28, 28))\n",
    "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
    "    return shifted_image.reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Augmented Dataset\n",
    "X_train_augmented = [image for image in X_train]\n",
    "y_train_augmented = [image for image in y_train]\n",
    "\n",
    "for dx, dy in ((1,0), (-1,0), (0,1), (0,-1)):\n",
    "     for image, label in zip(X_train, y_train):\n",
    "             X_train_augmented.append(shift_image(image, dx, dy))\n",
    "             y_train_augmented.append(label)\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffle_idx = np.random.permutation(len(X_train_augmented))\n",
    "X_train_augmented = np.array(X_train_augmented)[shuffle_idx]\n",
    "y_train_augmented = np.array(y_train_augmented)[shuffle_idx]\n",
    "\n",
    "print('Test data: ')\n",
    "print('Dimensions: %s x %s' % (X_train_augmented.shape[0], X_train_augmented.shape[1]))\n",
    "print('Digits: %s' % np.unique(y_train_augmented))\n",
    "print('Class distribution: %s' % np.bincount(y_train_augmented))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data now consists of 300,000 entries of 784 pixels, distributed among the possible values according to the output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Versioning the dataset and labels with the new entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_augmented_file = 'dataset/handwritten/digits/mnist/data/train-images.idx3-ubyte'\n",
    "pickle.dump(X_train_augmented, open(dataset_augmented_file, 'wb'))\n",
    "\n",
    "labels_augmented_file = 'labels/handwritten/digits/labelsmnist/data/train-labels.idx1-ubyte'\n",
    "pickle.dump(y_train_augmented, open(labels_augmented_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versioning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type = 'dataset'\n",
    "entity_name = 'mnist'\n",
    "\n",
    "api.add(entity_type, entity_name, bumpversion=True)\n",
    "api.commit(entity_type, entity_name)\n",
    "api.push(entity_type, entity_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versioning the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type = 'labels'\n",
    "entity_name = 'labelsmnist'\n",
    "\n",
    "api.add(entity_type, entity_name, bumpversion=True)\n",
    "api.commit(entity_type, entity_name, related_dataset='mnist')\n",
    "api.push(entity_type, entity_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on augmented dataset\n",
    "rf_clf_for_augmented = RandomForestClassifier(random_state=42)\n",
    "rf_clf_for_augmented.fit(X_train_augmented, y_train_augmented)\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred_after_augmented = rf_clf_for_augmented.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred_after_augmented)\n",
    "print(\"Accuracy score after training on augmented dataset\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve improved the accuracy by ~0.9%. This is great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - Versioning our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(rf_clf_for_augmented)\n",
    "\n",
    "entity_type = 'model'\n",
    "entity_name = 'modelmnist'\n",
    "\n",
    "api.add(entity_type, entity_name, bumpversion=True)\n",
    "api.commit(entity_type, entity_name, related_dataset='mnist', related_labels='labels_mnist')\n",
    "api.push(entity_type, entity_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:blue\"> 9 - Reproducing our experiment with ml-git</span> \n",
    "\n",
    "\n",
    "Once the experiment data is versioned, it is common that it is necessary to re-evaluate the result, or that someone else wants to see the result of an already trained model.\n",
    "\n",
    "For this, we will perform the model checkout in version 1 (without the data augmentation), to get the test data and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_path = 'dataset/handwritten/digits/mnist/data/'\n",
    "mnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/'\n",
    "mnist_model_path = 'model/handwritten/digits/modelmnist/data/'\n",
    "\n",
    "api.checkout('model', 'handwritten__digits__modelmnist__1', dataset=True, labels=True)\n",
    "\n",
    "# Getting test data\n",
    "X_test, y_test = loadlocal_mnist(images_path= mnist_dataset_path + 't10k-images.idx3-ubyte', \n",
    "                                 labels_path= mnist_labels_path + 't10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the test data in hand, let's upload the model and evaluate it for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(mnist_model_path + 'rf_mnist.sav', 'rb'))\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy score for version 1: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take the model from the version 2 (model trained with data from the data augmentation) and evaluate it for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.checkout('model', 'handwritten__digits__modelmnist__2')\n",
    "loaded_model = pickle.load(open(mnist_model_path + 'rf_mnist.sav', 'rb'))\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy score for version 2: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a quick and practical way it was possible to obtain the models generated in the experiments and to evaluate them again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this execution we have two versions of each entity. If someone else wants to replicate this experiment, they can check out the model with the related dataset and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            DATASET            |                LABELS               |                MODEL               | ACCURACY |\n",
    "|:-----------------------------:|:-----------------------------------:|:----------------------------------:|:--------:|\n",
    "| handwritten__digits__mnist__1 | handwritten__digits__labelsmnist__1 | handwritten__digits__modelmnist__1 |  0.9705  |\n",
    "| handwritten__digits__mnist__2 | handwritten__digits__labelsmnist__2 | handwritten__digits__modelmnist__2 |+~0.009   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}